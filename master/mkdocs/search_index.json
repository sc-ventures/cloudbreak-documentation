{
    "docs": [
        {
            "location": "/index.html", 
            "text": "Introduction\n\n\nWelcome to the \nCloudbreak 2.8.0 TP\n documentation! \n\n\nCloudbreak simplifies the provisioning, management, and monitoring of on-demand HDP and HDF clusters in virtual and cloud environments. It leverages cloud infrastructure to create host instances, and uses Apache Ambari via Ambari blueprints to provision and manage Hortonworks clusters. \n\n\nCloudbreak allows you to create clusters using the Cloudbreak web UI, Cloudbreak CLI, and Cloudbreak REST API. Clusters can be launched on public cloud infrastructure platforms \nMicrosoft Azure\n, \nAmazon Web Services (AWS)\n, and \nGoogle Cloud Platform (GCP)\n, and on the private cloud infrastructure platform \nOpenStack\n.\n\n\n   \n\n\nPrimary use cases\n\n\nCloudbreak allows you to create, manage, and monitor your HDP and HDF clusters on your chosen cloud platform:\n\n\n\n\nDynamically deploy, configure, and manage clusters on public and private clouds (AWS, Azure, Google Cloud, OpenStack).   \n\n\nUse automated scaling to seamlessly manage elasticity requirements as cluster workloads change.    \n\n\nSecure your cluster by enabling Kerberos.  \n\n\n\n\nArchitecture\n\n\nRefer to \nArchitecture\n.\n\n\nCore concepts\n\n\nRefer to \nCore concepts\n. \n\n\nDeployment options\n\n\nIn general, Cloudbreak offers two deployment options: a quickstart option and a production deployment option. Refer to \nDeployment options\n.  \n\n\nDefault cluster configurations\n\n\nCloudbreak includes default cluster configurations (in the form of blueprints) and supports using your own custom cluster configurations (in the form of custom blueprints).\n\n\nThe following default cluster configurations are available:\n\n\nPlatform version: \nHDP 2.6\n\n\n\n\n\n\n\n\nCluster type\n\n\nMain services\n\n\nDescription\n\n\nList of all services included\n\n\n\n\n\n\n\n\n\n\nData Science\n\n\n Spark 2,\nZeppelin\n\n\nUseful for data science with Spark 2 and Zeppelin.\n\n\nHDFS, YARN, MapReduce2, Tez, Hive, Pig, Sqoop, ZooKeeper, Ambari Metrics, Spark 2, Zeppelin\n\n\n\n\n\n\nEDW - Analytics\n\n\n Hive 2 LLAP\n,\nZeppelin\n\n\nUseful for EDW analytics using Hive LLAP.\n\n\nHDFS, YARN, MapReduce2, Tez, Hive 2 LLAP, Druid, Pig, ZooKeeper, Ambari Metrics, Spark 2\n\n\n\n\n\n\nEDW - ETL\n\n\n Hive,\n Spark 2\n\n\nUseful for ETL data processing with Hive and Spark 2.\n\n\nHDFS, YARN, MapReduce2, Tez, Hive, Pig, ZooKeeper, Ambari Metrics, Spark 2\n\n\n\n\n\n\n\n\nPlatform version: \nHDF 3.1\n\n\n\n\n\n\n\n\nCluster type\n\n\nMain services\n\n\nDescription\n\n\nList of all services included\n\n\n\n\n\n\n\n\n\n\nFlow Management\n\n\n NiFi\n\n\nUseful for flow management with NiFi.\n\n\nNiFi, NiFi Registry, ZooKeeper, Ambari Metrics\n\n\n\n\n\n\nMessaging Management\n\n\n Kafka\n\n\nUseful for messaging management with Kafka.\n\n\nKafka, ZooKeeper, Ambari Metrics\n\n\n\n\n\n\n\n\nGet started\n\n\nTo quickly get started with Cloudbreak, use the quickstart deployment option, which allows you to launch Cloudbreak from a template:\n\n\n\n\nGet started on AWS\n  \n\n\nGet started on Azure\n \n\n\nGet started on GCP\n  \n\n\n\n\n\n\nThis option is not available for \nOpenStack\n; you must launch Cloudbreak manually, as described in \nLaunch on OpenStack\n.    \n\n\n\n\nIn general, the steps include meeting the prerequisites, launching Cloudbreak from a template, and creating the Cloudbreak credential. After performing these steps, you can create a cluster based on one of the default blueprints.\n\n\n\n    \nNote\n\n    \nThe Cloudbreak software runs in your cloud environment. You are responsible for cloud infrastructure related charges while running Cloudbreak and the clusters being managed by Cloudbreak.", 
            "title": "Introduction"
        }, 
        {
            "location": "/index.html#introduction", 
            "text": "Welcome to the  Cloudbreak 2.8.0 TP  documentation!   Cloudbreak simplifies the provisioning, management, and monitoring of on-demand HDP and HDF clusters in virtual and cloud environments. It leverages cloud infrastructure to create host instances, and uses Apache Ambari via Ambari blueprints to provision and manage Hortonworks clusters.   Cloudbreak allows you to create clusters using the Cloudbreak web UI, Cloudbreak CLI, and Cloudbreak REST API. Clusters can be launched on public cloud infrastructure platforms  Microsoft Azure ,  Amazon Web Services (AWS) , and  Google Cloud Platform (GCP) , and on the private cloud infrastructure platform  OpenStack .", 
            "title": "Introduction"
        }, 
        {
            "location": "/index.html#primary-use-cases", 
            "text": "Cloudbreak allows you to create, manage, and monitor your HDP and HDF clusters on your chosen cloud platform:   Dynamically deploy, configure, and manage clusters on public and private clouds (AWS, Azure, Google Cloud, OpenStack).     Use automated scaling to seamlessly manage elasticity requirements as cluster workloads change.      Secure your cluster by enabling Kerberos.", 
            "title": "Primary use cases"
        }, 
        {
            "location": "/index.html#architecture", 
            "text": "Refer to  Architecture .", 
            "title": "Architecture"
        }, 
        {
            "location": "/index.html#core-concepts", 
            "text": "Refer to  Core concepts .", 
            "title": "Core concepts"
        }, 
        {
            "location": "/index.html#deployment-options", 
            "text": "In general, Cloudbreak offers two deployment options: a quickstart option and a production deployment option. Refer to  Deployment options .", 
            "title": "Deployment options"
        }, 
        {
            "location": "/index.html#default-cluster-configurations", 
            "text": "Cloudbreak includes default cluster configurations (in the form of blueprints) and supports using your own custom cluster configurations (in the form of custom blueprints).  The following default cluster configurations are available:  Platform version:  HDP 2.6     Cluster type  Main services  Description  List of all services included      Data Science   Spark 2, Zeppelin  Useful for data science with Spark 2 and Zeppelin.  HDFS, YARN, MapReduce2, Tez, Hive, Pig, Sqoop, ZooKeeper, Ambari Metrics, Spark 2, Zeppelin    EDW - Analytics   Hive 2 LLAP , Zeppelin  Useful for EDW analytics using Hive LLAP.  HDFS, YARN, MapReduce2, Tez, Hive 2 LLAP, Druid, Pig, ZooKeeper, Ambari Metrics, Spark 2    EDW - ETL   Hive,  Spark 2  Useful for ETL data processing with Hive and Spark 2.  HDFS, YARN, MapReduce2, Tez, Hive, Pig, ZooKeeper, Ambari Metrics, Spark 2     Platform version:  HDF 3.1     Cluster type  Main services  Description  List of all services included      Flow Management   NiFi  Useful for flow management with NiFi.  NiFi, NiFi Registry, ZooKeeper, Ambari Metrics    Messaging Management   Kafka  Useful for messaging management with Kafka.  Kafka, ZooKeeper, Ambari Metrics", 
            "title": "Default cluster configurations"
        }, 
        {
            "location": "/index.html#get-started", 
            "text": "To quickly get started with Cloudbreak, use the quickstart deployment option, which allows you to launch Cloudbreak from a template:   Get started on AWS     Get started on Azure    Get started on GCP       This option is not available for  OpenStack ; you must launch Cloudbreak manually, as described in  Launch on OpenStack .       In general, the steps include meeting the prerequisites, launching Cloudbreak from a template, and creating the Cloudbreak credential. After performing these steps, you can create a cluster based on one of the default blueprints.  \n     Note \n     The Cloudbreak software runs in your cloud environment. You are responsible for cloud infrastructure related charges while running Cloudbreak and the clusters being managed by Cloudbreak.", 
            "title": "Get started"
        }, 
        {
            "location": "/architecture/index.html", 
            "text": "Architecture\n\n\nCloudbreak deployer\n installs Cloudbreak components on a VM. Once these components are deployed, you can use \nCloudbreak application\n or Cloudbreak CLI to create, manage, and monitor clusters. \n\n\nCloudbreak deployer architecture\n\n\nCloudbreak deployer\n installs Cloudbreak components on a VM. It includes the following components:\n\n\n\n\n\n\n\n\nComponent\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak Application\n\n\nCloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari.\n\n\n\n\n\n\nUluwatu\n\n\nThis is Cloudbreak web UI, which can be used to create, manage, and monitor clusters.\n\n\n\n\n\n\nCloudbreak CLI\n\n\nThis is Cloudbreak's command line tool, which can be used to create, manage, and monitor clusters.\n\n\n\n\n\n\nIdentity\n\n\nThis is Cloudbreak's OAuth identity server implementation, which utilizes UAA.\n\n\n\n\n\n\nSultans\n\n\nThis is Cloudbreak's user management system.\n\n\n\n\n\n\nPeriscope\n\n\nThis is Cloudbreak's autoscaling application, which is responsible for automatically increasing or decreasing the capacity of the cluster when your pre-defined conditions are met.\n\n\n\n\n\n\n\n\nCloudbreak application architecture\n\n\nThe Cloudbreak application is a web application which simplifies cluster provisioning in the cloud. Based on your input, Cloudbreak provisions all required cloud infrastructure and then provisions a cluster on your behalf within your cloud provider account.   \n\n\n \n\n\nCloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari:\n\n\n\n\n\n\nCloudbreak uses \ncloud provider APIs\n to communicate with the cloud providers. \n\n\n\n\n\n\nCloudbreak uses the \nCloudbreak credential\n to authenticate with your cloud provider account and provision cloud resources required for the clusters. \n\n\n\n\n\n\nCloudbreak uses Apache Ambari and \nAmbari blueprints\n to provision, manage, and monitor clusters. Ambari blueprints are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize a cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/index.html#architecture", 
            "text": "Cloudbreak deployer  installs Cloudbreak components on a VM. Once these components are deployed, you can use  Cloudbreak application  or Cloudbreak CLI to create, manage, and monitor clusters.", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/index.html#cloudbreak-deployer-architecture", 
            "text": "Cloudbreak deployer  installs Cloudbreak components on a VM. It includes the following components:     Component  Description      Cloudbreak Application  Cloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari.    Uluwatu  This is Cloudbreak web UI, which can be used to create, manage, and monitor clusters.    Cloudbreak CLI  This is Cloudbreak's command line tool, which can be used to create, manage, and monitor clusters.    Identity  This is Cloudbreak's OAuth identity server implementation, which utilizes UAA.    Sultans  This is Cloudbreak's user management system.    Periscope  This is Cloudbreak's autoscaling application, which is responsible for automatically increasing or decreasing the capacity of the cluster when your pre-defined conditions are met.", 
            "title": "Cloudbreak deployer architecture"
        }, 
        {
            "location": "/architecture/index.html#cloudbreak-application-architecture", 
            "text": "The Cloudbreak application is a web application which simplifies cluster provisioning in the cloud. Based on your input, Cloudbreak provisions all required cloud infrastructure and then provisions a cluster on your behalf within your cloud provider account.        Cloudbreak application is built on the foundation of cloud provider APIs and Apache Ambari:    Cloudbreak uses  cloud provider APIs  to communicate with the cloud providers.     Cloudbreak uses the  Cloudbreak credential  to authenticate with your cloud provider account and provision cloud resources required for the clusters.     Cloudbreak uses Apache Ambari and  Ambari blueprints  to provision, manage, and monitor clusters. Ambari blueprints are a declarative definition of a cluster. With a blueprint, you can specify stack, component layout, and configurations to materialize a cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.", 
            "title": "Cloudbreak application architecture"
        }, 
        {
            "location": "/concepts/index.html", 
            "text": "Core concepts\n\n\nBefore using Cloudbreak, you should familiarize yourself with the following concepts.     \n\n\nAmbari blueprints\n\n\nAmbari blueprints are a declarative definition of a cluster. A blueprint allows you to specify stack, component layout, and configurations to materialize a cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.  \n\n\nAmbari blueprints are specified in JSON format. After you provide the blueprint to Cloudbreak, the host groups in the JSON are mapped to a set of instances when starting the cluster, and the specified services and components are installed on the corresponding nodes.\n\n\nCloudbreak includes a few default blueprints and allows you to upload your own blueprints.\n\n\n \n\n\nRelated links\n\n\nUsing custom blueprints\n\n\nApache documentation\n (External)  \n\n\nCloudbreak credential\n\n\nAfter launching Cloudbreak, you must create a Cloudbreak credential for each cloud provider on which you would like to provision clusters. Only after you have completed that step you can start creating clusters. \n\n\nCloudbreak credential allows Cloudbreak to authenticate with the cloud provider and create resources on your behalf. The authentication process varies depending on the cloud provider, but is typically done via assigning a specific IAM role to Cloudbreak which allows Cloudbreak to perform certain actions within your cloud provider account. \n\n\n \n\n\nRelated links\n\n\nIdentity management\n  \n\n\nData lake\n\n\nA data lake provides a way for you to centrally apply and enforce authentication, authorization, and audit policies across multiple ephemeral workload clusters. \"Attaching\" your workload cluster to the data lake instance allows the attached cluster workloads to access data and run in the security context provided by the data lake. \n\n\nRelated links\n\n\nSetting up a data lake\n\n\nDynamic blueprints\n\n\nProduction cluster configurations typically include certain configuration parameters, such as those related to external database (for Hive, Ranger, etc) and LDAP/AD, forcing users to create 1+ versions of the same blueprint to handle different component configurations for these external systems.     \n\n\nDynamic blueprints offer the ability to manage external sources (such as RDBMS and LDAP/AD) outside of your blueprint, because they merely use the blueprint as a template and Cloudbreak injects the actual configurations into your blueprint. This simplifies the reuse of cluster configurations for external sources (RDBMS and LDAP/AD) and simplifies the blueprints themselves.  \n\n\nCloudbreak allows you to create special \"dynamic\" blueprints which include templating: the values of the variables specified in the blueprint are dynamically replaced in the cluster creation phase, picking up the parameter values that you provided in the Cloudbreak UI or CLI.\nCloudbreak supports \nmustache\n kind of templating with {{{variable}}} syntax.\n\n\nRelated links\n\n\nCreating dynamic blueprints\n\n\nExternal sources\n\n\nCloudbreak allows you to define external sources that are created independently of a cluster -- and therefore their lifespan is not limited by the lifespan of any cluster -- and that can be reused with multiple clusters:\n\n\n \n\n\nThe external sources that can be registered in Cloudbreak include: \n\n\n\n\nAuthentication configurations (LDAP/AD) \n\n\nDatabase configurations   \n\n\nImage catalogs  \n\n\nProxy configurations   \n\n\n\n\nOnce you register an external source, you may reuse it for multiple clusters. \n\n\nRelated links\n\n\nUsing an external database for cluster services\n \n\n\nUsing an external authentication source for clusters\n \n\n\nRegister a proxy\n\n\nUsing custom images\n      \n\n\nRecipes\n\n\nCloudbreak allows you to upload custom scripts, called \"recipes\". A recipe is a script that runs on all nodes of a selected node group at a specific time. You can use recipes for tasks such as installing additional software or performing advanced cluster configuration. For example, you can use a recipe to put a JAR file on the Hadoop classpath.\n\n\nAvailable recipe execution times are:  \n\n\n\n\nBefore Ambari server start    \n\n\nAfter Ambari server start    \n\n\nAfter cluster installation    \n\n\nBefore cluster termination   \n\n\n\n\nYou can upload your recipes to Cloudbreak via the UI or CLI. Then, when creating a cluster, you can optionally attach one or more \"recipes\" and they will be executed on a specific host group at a specified time. \n\n\nRelated links\n\n\nUsing custom scripts (recipes)", 
            "title": "Core concepts"
        }, 
        {
            "location": "/concepts/index.html#core-concepts", 
            "text": "Before using Cloudbreak, you should familiarize yourself with the following concepts.", 
            "title": "Core concepts"
        }, 
        {
            "location": "/concepts/index.html#ambari-blueprints", 
            "text": "Ambari blueprints are a declarative definition of a cluster. A blueprint allows you to specify stack, component layout, and configurations to materialize a cluster instance via Ambari REST API, without having to use the Ambari cluster install wizard.    Ambari blueprints are specified in JSON format. After you provide the blueprint to Cloudbreak, the host groups in the JSON are mapped to a set of instances when starting the cluster, and the specified services and components are installed on the corresponding nodes.  Cloudbreak includes a few default blueprints and allows you to upload your own blueprints.     Related links  Using custom blueprints  Apache documentation  (External)", 
            "title": "Ambari blueprints"
        }, 
        {
            "location": "/concepts/index.html#cloudbreak-credential", 
            "text": "After launching Cloudbreak, you must create a Cloudbreak credential for each cloud provider on which you would like to provision clusters. Only after you have completed that step you can start creating clusters.   Cloudbreak credential allows Cloudbreak to authenticate with the cloud provider and create resources on your behalf. The authentication process varies depending on the cloud provider, but is typically done via assigning a specific IAM role to Cloudbreak which allows Cloudbreak to perform certain actions within your cloud provider account.      Related links  Identity management", 
            "title": "Cloudbreak credential"
        }, 
        {
            "location": "/concepts/index.html#data-lake", 
            "text": "A data lake provides a way for you to centrally apply and enforce authentication, authorization, and audit policies across multiple ephemeral workload clusters. \"Attaching\" your workload cluster to the data lake instance allows the attached cluster workloads to access data and run in the security context provided by the data lake.   Related links  Setting up a data lake", 
            "title": "Data lake"
        }, 
        {
            "location": "/concepts/index.html#dynamic-blueprints", 
            "text": "Production cluster configurations typically include certain configuration parameters, such as those related to external database (for Hive, Ranger, etc) and LDAP/AD, forcing users to create 1+ versions of the same blueprint to handle different component configurations for these external systems.       Dynamic blueprints offer the ability to manage external sources (such as RDBMS and LDAP/AD) outside of your blueprint, because they merely use the blueprint as a template and Cloudbreak injects the actual configurations into your blueprint. This simplifies the reuse of cluster configurations for external sources (RDBMS and LDAP/AD) and simplifies the blueprints themselves.    Cloudbreak allows you to create special \"dynamic\" blueprints which include templating: the values of the variables specified in the blueprint are dynamically replaced in the cluster creation phase, picking up the parameter values that you provided in the Cloudbreak UI or CLI.\nCloudbreak supports  mustache  kind of templating with {{{variable}}} syntax.  Related links  Creating dynamic blueprints", 
            "title": "Dynamic blueprints"
        }, 
        {
            "location": "/concepts/index.html#external-sources", 
            "text": "Cloudbreak allows you to define external sources that are created independently of a cluster -- and therefore their lifespan is not limited by the lifespan of any cluster -- and that can be reused with multiple clusters:     The external sources that can be registered in Cloudbreak include:    Authentication configurations (LDAP/AD)   Database configurations     Image catalogs    Proxy configurations      Once you register an external source, you may reuse it for multiple clusters.   Related links  Using an external database for cluster services    Using an external authentication source for clusters    Register a proxy  Using custom images", 
            "title": "External sources"
        }, 
        {
            "location": "/concepts/index.html#recipes", 
            "text": "Cloudbreak allows you to upload custom scripts, called \"recipes\". A recipe is a script that runs on all nodes of a selected node group at a specific time. You can use recipes for tasks such as installing additional software or performing advanced cluster configuration. For example, you can use a recipe to put a JAR file on the Hadoop classpath.  Available recipe execution times are:     Before Ambari server start      After Ambari server start      After cluster installation      Before cluster termination      You can upload your recipes to Cloudbreak via the UI or CLI. Then, when creating a cluster, you can optionally attach one or more \"recipes\" and they will be executed on a specific host group at a specified time.   Related links  Using custom scripts (recipes)", 
            "title": "Recipes"
        }, 
        {
            "location": "/deployment-options/index.html", 
            "text": "Deployment options\n\n\nThe following section describes general options for deploying Cloudbreak and Cloudbreak-managed clusters. \n\n\nCloudbreak deployment options\n\n\nIn general, Cloudbreak offers a quickstart option, as well as a production deployment option:\n\n\n  \n\n\n\n\nThe \nquickstart option\n allows you to get started with Cloudbreak quickly, but offers limited flexibility. Use this option for getting started with Cloudbreak. This option is not suitable a production.        \n\n\nThe \nproduction option\n is less automated, but offers more configurability. This option is recommended when your production environments. \n\n\n\n\nDeployment option cheatsheet\n\n\nThe following table summarizes the available Cloudbreak deployment options:  \n\n\n\n\nThe following operating systems are used when launching by using the quickstart option:\n\n\n\n\nQuickstart option for AWS\n\n\nThe quickstart option allows you to instantiate Cloudbreak by using the CloudFormation template. This is the basic deployment option and the easiest to get started with. \n\n\n \n\n\nThis option utilizes the following AWS services and provisions the following resources:\n\n\n\n\n\n\n\n\nResource\n\n\nDescription\n\n\nHow it is used by Cloudbreak\n\n\n\n\n\n\n\n\n\n\nAWS CloudFormation\n\n\nAWS CloudFormation is used to create and manage a collection of related AWS resources.\n\n\nCloudbreak is launched by using a CloudFormation template.\n\n\n\n\n\n\nAmazon EC2\n\n\nAmazon EC2 is used to launch a virtual machine for Cloudbreak. Security groups are used to control the inbound and outbound traffic to and from the Cloudbreak instance.\n\n\nCloudbreak automatically provisions a new VM that runs Amazon Linux, installs Docker, and launches Cloudbreak.\n\n\n\n\n\n\nAmazon VPC\n\n\nAmazon VPC is used to provision your own dedicated virtual network and launch resources into that network. As part of VPC infrastructure, an internet gateway and a route table are provisioned: An internet gateway is used to enable outbound access to the internet from the control plane and the clusters, and a route table is used to connect the subnet to the internet gateway.\n\n\nCloudbreak provisions a new VPC and subnet, and launches the Cloudbreak VM within it.\n\n\n\n\n\n\nAWS IAM\n\n\nAWS Identity \n Access Management (IAM) is used to control access to AWS services and resources.\n\n\nCloudbreak provisions the CloudbreakQuickstartRole IAM role that is used during the quickstart deployment.\n\n\n\n\n\n\nAWS Lambda\n\n\nThis is a utility service for running code in AWS.\n\n\nCloudbreak uses AWS Lambda is for running code when deploying Cloudbreak.\n\n\n\n\n\n\n\n\nRelated links\n \n\n\nQuickstart on AWS\n     \n\n\nQuickstart option for Azure\n\n\nThe quickstart option allows you to instantiate Cloudbreak by using Azure Resource Manager (ARM) template. \n\n\nOn Azure, resources are organized by using resource groups. When you launch Cloudbreak, you may either select to use an existing resource group or a new resource group is created. The following Azure resources are provisioned within the selected resource group:\n\n\n\n\nVirtual network\n (VNet) securely connects Azure resources to each other. You may either launch Cloudbreak into an existing VPC, or a new VPC is created and added to the resource group.  \n\n\nNetwork security group\n (NSG) defines inbound and outbound security rules, which control network traffic flow.  \n\n\nVirtual machine\n runs Cloudbreak. Based on the ARM template, Azure automatically provisions a new VM that runs CentOS 7, installs Docker, and launches Cloudbreak.   \n\n\nPublic IP address\n is assigned to your VM so that it can communicate with other Azure resources.  \n\n\nNetwork interface\n (NIC) attached to the VM provides the interconnection between the VM and the underlying software network.  \n\n\nBlob storage container\n is created to store Cloudbreak Deployer OS disk's data.  \n\n\n\n\nRelated links\n \n\n\nQuickstart on Azure\n  \n\n\nQuickstart option for GCP\n\n\nBased on the Cloud Deployment Manager template, GCP automatically provisions a new VM that runs CentOS 7, installs Docker, and launches Cloudbreak. \n\n\nThe following basic resources are provisioned on you GCP account:\n\n\n\n\nNetwork    \n\n\nSubnetwork (subnet)    \n\n\nRoute (routing table)  \n\n\nFirewall   \n\n\nVM instance  \n\n\nConfig  \n\n\nConfig waiter   \n\n\n\n\nThe config and config waiter are scripts used for deploying Cloudbreak. The startup config script is watched by the waiter script and updated when the deployment in up and running or if it failed to start. The startup waiter script keeps the deployment \"in-progress\" until the startup script that's running on the created machine update the cbd-deployment-startup-config value to \"success\" or \"failed\".\n\n\nRelated links\n \n\n\nQuickstart on GCP\n   \n\n\nProduction deployment option\n\n\nThe option to install Cloudbreak deployer manually on your own VM is available for all cloud providers. \n\n\nThis option:\n\n\n\n\nAllows you to provide your own VM with CentOS 7, RHEL 7, or Oracle Linux 7  \n\n\nAllows you to use your custom virtual network (On Azure this is also possible with the quickstart option)  \n\n\nRequires you to install Docker  \n\n\nRequires you to download the cbd tarball, extract it, and configure Cloudbreak deployer  \n\n\n\n\nRelated links\n\n\nLaunch on AWS\n \n\n\nLaunch on Azure\n \n\n\nLaunch on GCP\n \n\n\nLaunch on OpenStack\n  \n\n\nCluster deployment options\n\n\nOn a basic level, Cloudbreak offers three cluster deployment options:\n\n\n\n\nBasic cluster deployment with prescriptive options  \n\n\nAdvanced cluster deployment with customized options  \n\n\nEnterprise HDP cluster deployment with a data lake and attached workload clusters:  \n\n\n\n\n\n\nThe data lake deployment option is technical preview.    \n\n\n\n\n\n\nRelated links\n\n\nSetting up a data lake", 
            "title": "Deployment options"
        }, 
        {
            "location": "/deployment-options/index.html#deployment-options", 
            "text": "The following section describes general options for deploying Cloudbreak and Cloudbreak-managed clusters.", 
            "title": "Deployment options"
        }, 
        {
            "location": "/deployment-options/index.html#cloudbreak-deployment-options", 
            "text": "In general, Cloudbreak offers a quickstart option, as well as a production deployment option:       The  quickstart option  allows you to get started with Cloudbreak quickly, but offers limited flexibility. Use this option for getting started with Cloudbreak. This option is not suitable a production.          The  production option  is less automated, but offers more configurability. This option is recommended when your production environments.", 
            "title": "Cloudbreak deployment options"
        }, 
        {
            "location": "/deployment-options/index.html#deployment-option-cheatsheet", 
            "text": "The following table summarizes the available Cloudbreak deployment options:     The following operating systems are used when launching by using the quickstart option:", 
            "title": "Deployment option cheatsheet"
        }, 
        {
            "location": "/deployment-options/index.html#quickstart-option-for-aws", 
            "text": "The quickstart option allows you to instantiate Cloudbreak by using the CloudFormation template. This is the basic deployment option and the easiest to get started with.      This option utilizes the following AWS services and provisions the following resources:     Resource  Description  How it is used by Cloudbreak      AWS CloudFormation  AWS CloudFormation is used to create and manage a collection of related AWS resources.  Cloudbreak is launched by using a CloudFormation template.    Amazon EC2  Amazon EC2 is used to launch a virtual machine for Cloudbreak. Security groups are used to control the inbound and outbound traffic to and from the Cloudbreak instance.  Cloudbreak automatically provisions a new VM that runs Amazon Linux, installs Docker, and launches Cloudbreak.    Amazon VPC  Amazon VPC is used to provision your own dedicated virtual network and launch resources into that network. As part of VPC infrastructure, an internet gateway and a route table are provisioned: An internet gateway is used to enable outbound access to the internet from the control plane and the clusters, and a route table is used to connect the subnet to the internet gateway.  Cloudbreak provisions a new VPC and subnet, and launches the Cloudbreak VM within it.    AWS IAM  AWS Identity   Access Management (IAM) is used to control access to AWS services and resources.  Cloudbreak provisions the CloudbreakQuickstartRole IAM role that is used during the quickstart deployment.    AWS Lambda  This is a utility service for running code in AWS.  Cloudbreak uses AWS Lambda is for running code when deploying Cloudbreak.     Related links    Quickstart on AWS", 
            "title": "Quickstart option for AWS"
        }, 
        {
            "location": "/deployment-options/index.html#quickstart-option-for-azure", 
            "text": "The quickstart option allows you to instantiate Cloudbreak by using Azure Resource Manager (ARM) template.   On Azure, resources are organized by using resource groups. When you launch Cloudbreak, you may either select to use an existing resource group or a new resource group is created. The following Azure resources are provisioned within the selected resource group:   Virtual network  (VNet) securely connects Azure resources to each other. You may either launch Cloudbreak into an existing VPC, or a new VPC is created and added to the resource group.    Network security group  (NSG) defines inbound and outbound security rules, which control network traffic flow.    Virtual machine  runs Cloudbreak. Based on the ARM template, Azure automatically provisions a new VM that runs CentOS 7, installs Docker, and launches Cloudbreak.     Public IP address  is assigned to your VM so that it can communicate with other Azure resources.    Network interface  (NIC) attached to the VM provides the interconnection between the VM and the underlying software network.    Blob storage container  is created to store Cloudbreak Deployer OS disk's data.     Related links    Quickstart on Azure", 
            "title": "Quickstart option for Azure"
        }, 
        {
            "location": "/deployment-options/index.html#quickstart-option-for-gcp", 
            "text": "Based on the Cloud Deployment Manager template, GCP automatically provisions a new VM that runs CentOS 7, installs Docker, and launches Cloudbreak.   The following basic resources are provisioned on you GCP account:   Network      Subnetwork (subnet)      Route (routing table)    Firewall     VM instance    Config    Config waiter      The config and config waiter are scripts used for deploying Cloudbreak. The startup config script is watched by the waiter script and updated when the deployment in up and running or if it failed to start. The startup waiter script keeps the deployment \"in-progress\" until the startup script that's running on the created machine update the cbd-deployment-startup-config value to \"success\" or \"failed\".  Related links    Quickstart on GCP", 
            "title": "Quickstart option for GCP"
        }, 
        {
            "location": "/deployment-options/index.html#production-deployment-option", 
            "text": "The option to install Cloudbreak deployer manually on your own VM is available for all cloud providers.   This option:   Allows you to provide your own VM with CentOS 7, RHEL 7, or Oracle Linux 7    Allows you to use your custom virtual network (On Azure this is also possible with the quickstart option)    Requires you to install Docker    Requires you to download the cbd tarball, extract it, and configure Cloudbreak deployer     Related links  Launch on AWS    Launch on Azure    Launch on GCP    Launch on OpenStack", 
            "title": "Production deployment option"
        }, 
        {
            "location": "/deployment-options/index.html#cluster-deployment-options", 
            "text": "On a basic level, Cloudbreak offers three cluster deployment options:   Basic cluster deployment with prescriptive options    Advanced cluster deployment with customized options    Enterprise HDP cluster deployment with a data lake and attached workload clusters:      The data lake deployment option is technical preview.        Related links  Setting up a data lake", 
            "title": "Cluster deployment options"
        }, 
        {
            "location": "/security/index.html", 
            "text": "Security overview\n\n\nCloudbreak utilizes cloud provider security resources such as virtual networks, security groups, and identity and access management:\n\n\n\n\n\n\nNetwork isolation\n is achieved via user-configured virtual networks and subnets.  \n\n\n\n\n\n\nNetwork security\n is achieved via out-of-the-box security group settings.  \n\n\n\n\n\n\nControlled use of cloud resources\n using IAM roles (AWS, GCP) or Active Directory (in case of Azure). \n\n\n\n\n\n\nVirtual networks\n\n\nCloud providers use virtual networks which resemble traditional networks. Depending on the options that you selected during deployment, your Cloudbreak instance and clusters are launched into new or existing cloud provider networking infrastructure (virtual networks and subnets). For more information about virtual networks, refer to the cloud-provider documentation:\n\n\n\n\n\n\n\n\nCloud provider\n\n\nExternal documentation link\n\n\n\n\n\n\n\n\n\n\nAWS\n\n\nAmazon Virtual Private Cloud (Amazon VPC)\n\n\n\n\n\n\nAzure\n\n\nMicrosoft Azure Virtual Network\n\n\n\n\n\n\nGoogle Cloud Platform\n\n\nVirtual Private Cloud (VPC) network\n\n\n\n\n\n\nOpenStack\n\n\nNetwork\n\n\n\n\n\n\n\n\nNetwork security\n\n\nSecurity groups are set up to control network traffic to the instances in the system.\n\n\nCloudbreak uses public IP addresses when communicating with cluster nodes. On AWS, you can configure it to use private IPs instead. For instructions, refer to \nConfigure communication via private IPs on AWS\n.  \n\n\nRelated links\n\n\nConfigure communication via private IPs on AWS\n\n\nCloudbreak instance security group\n\n\nThe following table lists the minimum security group port configuration required for the Cloudbreak instance:\n\n\n\n\n\n\n\n\nInbound port\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n22\n\n\nSSH access to the Cloudbreak VM.\n\n\n\n\n\n\n80\n\n\nHTTP access to the Cloudbreak UI. This is automatically redirected to the HTTPS (443) port.\n\n\n\n\n\n\n443\n\n\nHTTPS access to the Cloudbreak UI.\n\n\n\n\n\n\n\n\nDefault cluster security groups\n\n\nThe following section describes the network requirements and options. By default, when creating a cluster, a new network, subnet, and security groups will be created automatically.\n\n\n\n    \nNote\n\n    \nThe default experience of creating network resources such as network, subnet and security group automatically is provided for convenience. We strongly recommend you review these options and for production cluster deployments leverage your existing network resources that you have defined and validated to meet your enterprise requirements. \n\n\n\n\n\n\n\n\n\n\n\nPort\n\n\nSource\n\n\nTarget\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak\n\n\nAmbari server\n\n\n9443\n\n\nThis port is used by Cloudbreak to maintain management control of the cluster.\nThe default security group opens 9443 from anywhere. You should limit this CIDR further to \nonly allow access from the Cloudbreak host\n. This can be done by default by \nrestricting inbound access\n from Cloudbreak to cluster.\n\n\n\n\n\n\n*\n\n\nAll cluster hosts\n\n\n22\n\n\nThis is an optional port for end user SSH access to the hosts.\nYou should review and limit or remove this CIDR access.\n\n\n\n\n\n\n*\n\n\nAmbari server\n\n\n8443\n\n\nThis port is used to access the gateway (if configured).\nYou should review and limit this CIDR access.\nIf you do not configure the gateway, this port does not need to be opened. If you want access to any cluster resources, you must open those ports explicitly on the security groups for their respective hosts.\n\n\n\n\n\n\n*\n\n\nAmbari server\n\n\n443\n\n\nThis port is used to access Ambari directly.\nIf you are configuring the gateway, you should access Ambari through the gateway. You do not need to open this port.\nIf you do not configure the gateway, to obtain access to Ambari, you can open this port on the security group for the respective host.\n\n\n\n\n\n\n\n\nRelated links\n \n\n\nRestrict inbound access\n  \n\n\nIdentity management\n\n\nTo securely control access to cloud resources, cloud providers use identity management services such as IAM roles (AWS and GCP) and Active Directory (Azure). \n\n\n\n\n\n\n\n\nCloud provider\n\n\nExternal documentation link\n\n\n\n\n\n\n\n\n\n\nAWS\n\n\nAWS Identity and Access Management (IAM)\n\n\n\n\n\n\nAzure\n\n\nAzure Active Directory ((Azure AD))\n\n\n\n\n\n\nGoogle\n\n\nGoogle Cloud Identity and Access Management (IAM)\n\n\n\n\n\n\nOpenStack\n\n\nKeystone\n\n\n\n\n\n\n\n\nCloudbreak utilizes cloud provider\u2019s identity management services via Cloudbreak credential. After launching Cloudbreak on your chosen cloud provider, you must create a Cloudbreak credential, which allows Cloudbreak to authenticate with your cloud provider identity management service. Only after you have completed this step, Cloudbreak can create resources on your behalf. \n\n\nAuthentication with AWS\n\n\nWhen launching Cloudbreak on AWS, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. While key-based authentication uses your AWS access key and secret key, role-based authentication uses IAM roles.\n\n\nIf you are using role-based authentication for Cloudbreak on AWS, you must create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).\n\n\nThe following table provides contextual information about the two roles required: \n\n\n\n\n\n\n\n\nRole\n\n\nPurpose\n\n\nOverview of steps\n\n\nConfiguration\n\n\n\n\n\n\n\n\n\n\nCloudbreakRole\n\n\nAllows Cloudbreak to assume other IAM roles - specifically the CredentialRole.\n\n\nCreate a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition and steps for creating the CloudbreakRole are provided below.\n\n\nWhen launching Cloudbreak, you will attach the \"CloudbreakRole\" IAM role to the VM.\nIf you are using hosted Cloudbreak, you do not need to perform this step.\n\n\n\n\n\n\nCredentialRole\n\n\nAllows Cloudbreak to create AWS resources required for clusters.\n\n\nCreate a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition and steps for creating the CredentialRole are provided below.\n When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d. See steps below.\n\n\nOnce you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak credential.\n\n\n\n\n\n\n\n\nRelated links\n\n\nAuthentication\n  \n\n\nAuthentication with Azure\n\n\nAfter launching Cloudbreak on Azure, you are required to create a Cloudbreak credential, which allows Cloudbreak to authenticate with your Azure Active Directory. \n\n\nYou have two options:\n\n\n\n\n\n\nInteractive: The app and service principal creation and role assignment are fully automated, so the only input that you need to provide to Cloudbreak is your Subscription ID and Directory ID. \n\n\n\n\n\n\nApp-based: The app and service principal creation and role assignment are not automated You must create an Azure Active Directory application registration and then provide its parameters to Cloudbreak, in addition to providing your Subscription ID and Directory ID. \n\n\n\n\n\n\nRelated links\n\n\nCreate Cloudbreak credential\n  \n\n\nAuthentication with GCP\n\n\nAfter launching Cloudbreak on GCP, you are required to register a service account in Cloudbreak via creating a Cloudbreak credential. Cloudbreak uses this account to authenticate with the GCP identity management service.\n\n\nRelated links\n\n\nService account\n  \n\n\nAuthentication with OpenStack\n\n\nAfter launching Cloudbreak on OpenStack, you are required to create a Cloudbreak credential, which allows Cloudbreak to authenticate with keystone. \n\n\nRelated links\n\n\nCreate Cloudbreak credential", 
            "title": "Security overview"
        }, 
        {
            "location": "/security/index.html#security-overview", 
            "text": "Cloudbreak utilizes cloud provider security resources such as virtual networks, security groups, and identity and access management:    Network isolation  is achieved via user-configured virtual networks and subnets.      Network security  is achieved via out-of-the-box security group settings.      Controlled use of cloud resources  using IAM roles (AWS, GCP) or Active Directory (in case of Azure).", 
            "title": "Security overview"
        }, 
        {
            "location": "/security/index.html#virtual-networks", 
            "text": "Cloud providers use virtual networks which resemble traditional networks. Depending on the options that you selected during deployment, your Cloudbreak instance and clusters are launched into new or existing cloud provider networking infrastructure (virtual networks and subnets). For more information about virtual networks, refer to the cloud-provider documentation:     Cloud provider  External documentation link      AWS  Amazon Virtual Private Cloud (Amazon VPC)    Azure  Microsoft Azure Virtual Network    Google Cloud Platform  Virtual Private Cloud (VPC) network    OpenStack  Network", 
            "title": "Virtual networks"
        }, 
        {
            "location": "/security/index.html#network-security", 
            "text": "Security groups are set up to control network traffic to the instances in the system.  Cloudbreak uses public IP addresses when communicating with cluster nodes. On AWS, you can configure it to use private IPs instead. For instructions, refer to  Configure communication via private IPs on AWS .    Related links  Configure communication via private IPs on AWS", 
            "title": "Network security"
        }, 
        {
            "location": "/security/index.html#cloudbreak-instance-security-group", 
            "text": "The following table lists the minimum security group port configuration required for the Cloudbreak instance:     Inbound port  Description      22  SSH access to the Cloudbreak VM.    80  HTTP access to the Cloudbreak UI. This is automatically redirected to the HTTPS (443) port.    443  HTTPS access to the Cloudbreak UI.", 
            "title": "Cloudbreak instance security group"
        }, 
        {
            "location": "/security/index.html#default-cluster-security-groups", 
            "text": "The following section describes the network requirements and options. By default, when creating a cluster, a new network, subnet, and security groups will be created automatically.  \n     Note \n     The default experience of creating network resources such as network, subnet and security group automatically is provided for convenience. We strongly recommend you review these options and for production cluster deployments leverage your existing network resources that you have defined and validated to meet your enterprise requirements.       Port  Source  Target  Description      Cloudbreak  Ambari server  9443  This port is used by Cloudbreak to maintain management control of the cluster. The default security group opens 9443 from anywhere. You should limit this CIDR further to  only allow access from the Cloudbreak host . This can be done by default by  restricting inbound access  from Cloudbreak to cluster.    *  All cluster hosts  22  This is an optional port for end user SSH access to the hosts. You should review and limit or remove this CIDR access.    *  Ambari server  8443  This port is used to access the gateway (if configured). You should review and limit this CIDR access. If you do not configure the gateway, this port does not need to be opened. If you want access to any cluster resources, you must open those ports explicitly on the security groups for their respective hosts.    *  Ambari server  443  This port is used to access Ambari directly. If you are configuring the gateway, you should access Ambari through the gateway. You do not need to open this port. If you do not configure the gateway, to obtain access to Ambari, you can open this port on the security group for the respective host.     Related links    Restrict inbound access", 
            "title": "Default cluster security groups"
        }, 
        {
            "location": "/security/index.html#identity-management", 
            "text": "To securely control access to cloud resources, cloud providers use identity management services such as IAM roles (AWS and GCP) and Active Directory (Azure).      Cloud provider  External documentation link      AWS  AWS Identity and Access Management (IAM)    Azure  Azure Active Directory ((Azure AD))    Google  Google Cloud Identity and Access Management (IAM)    OpenStack  Keystone     Cloudbreak utilizes cloud provider\u2019s identity management services via Cloudbreak credential. After launching Cloudbreak on your chosen cloud provider, you must create a Cloudbreak credential, which allows Cloudbreak to authenticate with your cloud provider identity management service. Only after you have completed this step, Cloudbreak can create resources on your behalf.", 
            "title": "Identity management"
        }, 
        {
            "location": "/security/index.html#authentication-with-aws", 
            "text": "When launching Cloudbreak on AWS, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. While key-based authentication uses your AWS access key and secret key, role-based authentication uses IAM roles.  If you are using role-based authentication for Cloudbreak on AWS, you must create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).  The following table provides contextual information about the two roles required:      Role  Purpose  Overview of steps  Configuration      CloudbreakRole  Allows Cloudbreak to assume other IAM roles - specifically the CredentialRole.  Create a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition and steps for creating the CloudbreakRole are provided below.  When launching Cloudbreak, you will attach the \"CloudbreakRole\" IAM role to the VM. If you are using hosted Cloudbreak, you do not need to perform this step.    CredentialRole  Allows Cloudbreak to create AWS resources required for clusters.  Create a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition and steps for creating the CredentialRole are provided below.  When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d. See steps below.  Once you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak credential.     Related links  Authentication", 
            "title": "Authentication with AWS"
        }, 
        {
            "location": "/security/index.html#authentication-with-azure", 
            "text": "After launching Cloudbreak on Azure, you are required to create a Cloudbreak credential, which allows Cloudbreak to authenticate with your Azure Active Directory.   You have two options:    Interactive: The app and service principal creation and role assignment are fully automated, so the only input that you need to provide to Cloudbreak is your Subscription ID and Directory ID.     App-based: The app and service principal creation and role assignment are not automated You must create an Azure Active Directory application registration and then provide its parameters to Cloudbreak, in addition to providing your Subscription ID and Directory ID.     Related links  Create Cloudbreak credential", 
            "title": "Authentication with Azure"
        }, 
        {
            "location": "/security/index.html#authentication-with-gcp", 
            "text": "After launching Cloudbreak on GCP, you are required to register a service account in Cloudbreak via creating a Cloudbreak credential. Cloudbreak uses this account to authenticate with the GCP identity management service.  Related links  Service account", 
            "title": "Authentication with GCP"
        }, 
        {
            "location": "/security/index.html#authentication-with-openstack", 
            "text": "After launching Cloudbreak on OpenStack, you are required to create a Cloudbreak credential, which allows Cloudbreak to authenticate with keystone.   Related links  Create Cloudbreak credential", 
            "title": "Authentication with OpenStack"
        }, 
        {
            "location": "/aws-quick/index.html", 
            "text": "Quickstart on AWS\n\n\nThis quickstart documentation will help you get started with Cloudbreak. \n\n\nPrerequisites\n\n\nIn order to launch Cloudbreak from the CloudFormation template you must:\n\n\n\n\n\n\nHave an existing an AWS account. If you don't have an account, you can create one at \nhttps://aws.amazon.com/\n.\n\n\n\n\n\n\nImport an existing SSH key pair or generate a new key pair in the AWS region which you are planning to use for launching Cloudbreak and clusters. If you don't have a key pair, refer to \nSSH key pair\n documentation to create or import a key pair.  \n\n\n\n\n\n\nRelated links\n  \n\n\nSSH key pair\n   \n\n\nLaunch Cloudbreak from the quickstart template\n\n\nLaunch Cloudbreak from a CloudFormation template by using the following steps. This is the quickstart deployment option. \n\n\nSteps\n \n\n\n\n\n\n\nClick on the link to launch the CloudFormation template that will create the AWS resources, including an EC2 Instance running Cloudbreak:\n\n\n\n \n\n   \nRegion\n\n   \nLink\n\n \n\n \n\n   \nus-east-1 (N. Virginia)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in US East\n\n \n\n  \n\n   \nus-west-1 (N. California)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in US West (N. California)\n\n \n\n \n\n   \nus-west-2 (Oregon)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in US West (Oregon)\n\n \n\n \n\n   \neu-central-1 (Frankfurt)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in EU Central\n\n \n\n \n\n   \neu-west-1 (Dublin)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in EU West \n\n \n\n \n\n   \nsa-east-1 (S\u00e3o Paulo)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in South America\n\n \n\n \n\n   \nap-northeast-1 (Tokyo)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in Asia Pacific (Tokyo)\n\n \n\n \n\n   \nap-southeast-1 (Singapore)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in Asia Pacific (Singapore)\n\n \n\n \n\n   \nap-southeast-2 (Sydney)\n\n   \n\n   \n\n   \n Launch the CloudFormation Template in Asia Pacific (Sydney)\n\n \n\n\n  \n\n\n\n\n\n\nThe \nCreate stack\n wizard is launched in the Amazon CloudFormation management console:  \n\n\n\n\n\n\nYou do not need to change any template parameters on the \nSelect Template\n page, but check the region name in the top right corner to confirm the region in which you want to launch Cloudbreak:  \n\n\n  \n\n\n\n\nIf needed, you may change the region if needed by using the dropdown in the top right corner.\n\n\n\n\n\n\n\n\nClick \nNext\n to display the \nSpecify Details\n page.\n\n\n\n\n\n\nOn the the \nSpecify Details\n page, enter the following information:\n\n\n \n\n\nSpecify Details\n section \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nStack name\n\n\nEnter name for your stack. It must be unique in your AWS environment.\n\n\n\n\n\n\n\n\nGeneral Configuration\n \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nController Instance Type\n\n\nEC2 instance type to use for the cloud controller.\n\n\n\n\n\n\nEmail Address\n\n\nUsername for the Admin login. Must be a valid email address.\n\n\n\n\n\n\nAdmin Password\n\n\nPassword for Admin login. Must be at least 8 characters containing letters, numbers, and symbols.\n\n\n\n\n\n\n\n\nSecurity Configuration\n \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSSH Key Name\n\n\nName of an existing EC2 key pair to enable SSH to access the instances. Key pairs are region-specific, so only the key pairs that you created for a selected region will appear in the dropdown.If you don't have a key pair, refer to \nSSH key pair\n.\n\n\n\n\n\n\nRemote Access\n\n\nAllow connections to the cloud controller ports from this address range. Must be a valid \nCIDR IP\n. For example: \n192.168.27.0/24 will allow access from 192.168.27.0 through 192.168.27.255.\n192.168.27.10/32 will allow access from 192.168.27.10.\n0.0.0.0/0 will allow access from all.\n Refer to \nNetwork security\n for more information on the inbound ports that are used with Cloudbreak.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nNext\n  to display the \nOptions\n page.    \n\n\n\n\n\n\nOn he \nOptions\n page, if you expand the \nAdvanced\n section, there is an option to \nRollback on failure\n. \n\n\n\n\nBy\ndefault, this option is set to \nYes\n, which means that if there are any event failures when creating\nthe stack, all the AWS resources created so far are deleted (i.e rolled back) to avoid unnecessary charges. \n\n\nIf you set this option to \nNo\n, if there are any event failures when creating\nthe stack, the\nresources are left intact (i.e. not rolled back). Select the \nNo\n option to aid in\ntroubleshooting. Note that in this case you are responsible for deleting the stack later.\n\n\n\n\n\n\n\n\nClick \nNext\n to display the \nReview\n page.\n\n\n\n\n\n\nOn the \nReview\n page, click the \nI acknowledge...\n checkbox.  \n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\nThe \nStack Name\n is shown in the table with a \nCREATE_IN_PROGRESS\n status. You can click on the \nStack Name\n and see the specific events that are in progress. The create process takes about 10 minutes and once ready, you will see \nCREATE_COMPLETE\n. \n\n\n\n\n\n\nCleaning up after a failed deployment\n\n\nFor steps on how to delete Cloudbreak after a failed deployment, refer to \nDelete Cloudbreak on AWS\n.\n\n\n\n\n\n\n\n\nRelated links\n\n\nDelete Cloudbreak on AWS\n\n\nNetwork security\n\n\nSSH key pair\n\n\nCIDR IP\n (External)  \n\n\nAccess Cloudbreak web UI\n\n\nFollow these steps to obtain Cloudbreak VM's public IP address and log in to the Cloudbreak web UI. \n\n\nSteps\n     \n\n\n\n\n\n\nOnce the stack creation is complete, Cloudbreak is ready to use. You can obtain the URL to Cloudbreak from the \nOutputs\n tab:\n\n\n\n\n\n\nIf the Outputs tab is blank, refresh the page.\n\n\n\n\n\n\n\n\nPaste the link in your browser's address bar.\n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nLog in to the Cloudbreak web UI using the credential that you configured in the CloudFormation template.\n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n  \n\n\n\n\n\n\nCreate Cloudbreak credential\n\n\nBefore you can start using Cloudbreak to create clusters, you must create a Cloudbreak credential. Cloudbreak credential allows Cloudbreak to authenticate with your AWS account and provision resources on your behalf.\n\n\nPrerequisites\n\n\nIn order to use the key-based Cloudbreak credential: \n\n\n\n\n\n\nYou must have an access key and secret key. For information on how to generate it, refer to \nUse key-based authentication\n.  \n\n\n\n\n\n\nYour AWS user must have the minimum permissions described in \nCreate CredentialRole\n as well as the permission to create an IAM role. \n\n\n\n\n\n\nIf you are unable to obtain these permissions for your AWS user, you must use \nrole-based authentication\n instead of key-based authentication. If you would like to review both options, refer to \nAuthentication\n. \n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Amazon Web Services\":\n\n\n  \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nKey Based\n.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nAccess Key\n\n\nPaste your access key.\n\n\n\n\n\n\nSecret Access Key\n\n\nPaste your secret key.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You've successfully launched Cloudbreak and create a Cloudbreak credential. Now you can start creating clusters. \n\n\n\n\n\n\nRelated links\n \n\n\nAuthentication\n\n\nUse key-based authentication\n  \n\n\nUse role-based authentication\n   \n\n\nCreate CredentialRole\n  \n\n\nCreate a cluster\n\n\nUse these steps to create a cluster. This section only covers minimal steps required for creating a cluster based on the basic settings (2-node cluster with default hardware and storage options).  \n\n\nSteps\n\n\n\n\n\n\nClick the \nCreate Cluster\n button and the \nCreate Cluster\n wizard is displayed.\n\n    By default, \nBasic\n view is displayed.\n\n\n  \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page:\n\n\n\n\nCluster Name\n: Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\nRegion\n: Select the region in which you would like to launch your cluster. \n\n\nCluster Type\n: Choose one of default cluster configurations.  \n\n\n\n\n\n\n\n\nClick \nNext\n three times to navigate to the \nSecurity Page\n. You do not need to enter anything on the \nHardware and Storage\n and \nNetwork\n because by default Cloudbreak suggests the instance types, storage, and network to use (a new network and subnet is created by default).   \n\n\n\n\n\n\nOne the \nSecurity\n page, provide the following:\n\n\n\n\nCluster User\n: This will be the user that you should use to log in to Ambari and other cluster UIs. By default, this is \nadmin\n.   \n\n\nCluster Password\n: Password for the cluster user.  \n\n\nSSH public key\n: Select the existing public SSH key or paste your key. The key will be placed on the cluster VMs so that you can use the matching private key to access the VMs via SSH. \n\n\n\n\n  \n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nNext steps\n\n\nTo learn how to access your cluster, refer to \nAccessing a cluster\n.\n\n\nTo learn how to use Cloudbreak to manage your cluster, refer to  \nManaging and monitoring clusters\n.", 
            "title": "Quickstart on AWS"
        }, 
        {
            "location": "/aws-quick/index.html#quickstart-on-aws", 
            "text": "This quickstart documentation will help you get started with Cloudbreak.", 
            "title": "Quickstart on AWS"
        }, 
        {
            "location": "/aws-quick/index.html#prerequisites", 
            "text": "In order to launch Cloudbreak from the CloudFormation template you must:    Have an existing an AWS account. If you don't have an account, you can create one at  https://aws.amazon.com/ .    Import an existing SSH key pair or generate a new key pair in the AWS region which you are planning to use for launching Cloudbreak and clusters. If you don't have a key pair, refer to  SSH key pair  documentation to create or import a key pair.      Related links     SSH key pair", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/aws-quick/index.html#launch-cloudbreak-from-the-quickstart-template", 
            "text": "Launch Cloudbreak from a CloudFormation template by using the following steps. This is the quickstart deployment option.   Steps      Click on the link to launch the CloudFormation template that will create the AWS resources, including an EC2 Instance running Cloudbreak:  \n  \n    Region \n    Link \n  \n  \n    us-east-1 (N. Virginia) \n    \n    \n     Launch the CloudFormation Template in US East \n  \n   \n    us-west-1 (N. California) \n    \n    \n     Launch the CloudFormation Template in US West (N. California) \n  \n  \n    us-west-2 (Oregon) \n    \n    \n     Launch the CloudFormation Template in US West (Oregon) \n  \n  \n    eu-central-1 (Frankfurt) \n    \n    \n     Launch the CloudFormation Template in EU Central \n  \n  \n    eu-west-1 (Dublin) \n    \n    \n     Launch the CloudFormation Template in EU West  \n  \n  \n    sa-east-1 (S\u00e3o Paulo) \n    \n    \n     Launch the CloudFormation Template in South America \n  \n  \n    ap-northeast-1 (Tokyo) \n    \n    \n     Launch the CloudFormation Template in Asia Pacific (Tokyo) \n  \n  \n    ap-southeast-1 (Singapore) \n    \n    \n     Launch the CloudFormation Template in Asia Pacific (Singapore) \n  \n  \n    ap-southeast-2 (Sydney) \n    \n    \n     Launch the CloudFormation Template in Asia Pacific (Sydney) \n         The  Create stack  wizard is launched in the Amazon CloudFormation management console:      You do not need to change any template parameters on the  Select Template  page, but check the region name in the top right corner to confirm the region in which you want to launch Cloudbreak:         If needed, you may change the region if needed by using the dropdown in the top right corner.     Click  Next  to display the  Specify Details  page.    On the the  Specify Details  page, enter the following information:     Specify Details  section      Parameter  Description      Stack name  Enter name for your stack. It must be unique in your AWS environment.     General Configuration       Parameter  Description      Controller Instance Type  EC2 instance type to use for the cloud controller.    Email Address  Username for the Admin login. Must be a valid email address.    Admin Password  Password for Admin login. Must be at least 8 characters containing letters, numbers, and symbols.     Security Configuration       Parameter  Description      SSH Key Name  Name of an existing EC2 key pair to enable SSH to access the instances. Key pairs are region-specific, so only the key pairs that you created for a selected region will appear in the dropdown.If you don't have a key pair, refer to  SSH key pair .    Remote Access  Allow connections to the cloud controller ports from this address range. Must be a valid  CIDR IP . For example:  192.168.27.0/24 will allow access from 192.168.27.0 through 192.168.27.255. 192.168.27.10/32 will allow access from 192.168.27.10. 0.0.0.0/0 will allow access from all.  Refer to  Network security  for more information on the inbound ports that are used with Cloudbreak.       Click  Next   to display the  Options  page.        On he  Options  page, if you expand the  Advanced  section, there is an option to  Rollback on failure .    By\ndefault, this option is set to  Yes , which means that if there are any event failures when creating\nthe stack, all the AWS resources created so far are deleted (i.e rolled back) to avoid unnecessary charges.   If you set this option to  No , if there are any event failures when creating\nthe stack, the\nresources are left intact (i.e. not rolled back). Select the  No  option to aid in\ntroubleshooting. Note that in this case you are responsible for deleting the stack later.     Click  Next  to display the  Review  page.    On the  Review  page, click the  I acknowledge...  checkbox.      Click  Create .   The  Stack Name  is shown in the table with a  CREATE_IN_PROGRESS  status. You can click on the  Stack Name  and see the specific events that are in progress. The create process takes about 10 minutes and once ready, you will see  CREATE_COMPLETE .     Cleaning up after a failed deployment  For steps on how to delete Cloudbreak after a failed deployment, refer to  Delete Cloudbreak on AWS .     Related links  Delete Cloudbreak on AWS  Network security  SSH key pair  CIDR IP  (External)", 
            "title": "Launch Cloudbreak from the quickstart template"
        }, 
        {
            "location": "/aws-quick/index.html#access-cloudbreak-web-ui", 
            "text": "Follow these steps to obtain Cloudbreak VM's public IP address and log in to the Cloudbreak web UI.   Steps          Once the stack creation is complete, Cloudbreak is ready to use. You can obtain the URL to Cloudbreak from the  Outputs  tab:    If the Outputs tab is blank, refresh the page.     Paste the link in your browser's address bar.    Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...       The login page is displayed:        Log in to the Cloudbreak web UI using the credential that you configured in the CloudFormation template.    Upon a successful login, you are redirected to the dashboard:", 
            "title": "Access Cloudbreak web UI"
        }, 
        {
            "location": "/aws-quick/index.html#create-cloudbreak-credential", 
            "text": "Before you can start using Cloudbreak to create clusters, you must create a Cloudbreak credential. Cloudbreak credential allows Cloudbreak to authenticate with your AWS account and provision resources on your behalf.  Prerequisites  In order to use the key-based Cloudbreak credential:     You must have an access key and secret key. For information on how to generate it, refer to  Use key-based authentication .      Your AWS user must have the minimum permissions described in  Create CredentialRole  as well as the permission to create an IAM role.     If you are unable to obtain these permissions for your AWS user, you must use  role-based authentication  instead of key-based authentication. If you would like to review both options, refer to  Authentication .   Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Amazon Web Services\":        Provide the following information:     Parameter  Description      Select Credential Type  Select  Key Based .    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Access Key  Paste your access key.    Secret Access Key  Paste your secret key.       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You've successfully launched Cloudbreak and create a Cloudbreak credential. Now you can start creating clusters.     Related links    Authentication  Use key-based authentication     Use role-based authentication      Create CredentialRole", 
            "title": "Create Cloudbreak credential"
        }, 
        {
            "location": "/aws-quick/index.html#create-a-cluster", 
            "text": "Use these steps to create a cluster. This section only covers minimal steps required for creating a cluster based on the basic settings (2-node cluster with default hardware and storage options).    Steps    Click the  Create Cluster  button and the  Create Cluster  wizard is displayed. \n    By default,  Basic  view is displayed.        On the  General Configuration  page:   Cluster Name : Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.  Region : Select the region in which you would like to launch your cluster.   Cluster Type : Choose one of default cluster configurations.       Click  Next  three times to navigate to the  Security Page . You do not need to enter anything on the  Hardware and Storage  and  Network  because by default Cloudbreak suggests the instance types, storage, and network to use (a new network and subnet is created by default).       One the  Security  page, provide the following:   Cluster User : This will be the user that you should use to log in to Ambari and other cluster UIs. By default, this is  admin .     Cluster Password : Password for the cluster user.    SSH public key : Select the existing public SSH key or paste your key. The key will be placed on the cluster VMs so that you can use the matching private key to access the VMs via SSH.          Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.", 
            "title": "Create a cluster"
        }, 
        {
            "location": "/aws-quick/index.html#next-steps", 
            "text": "To learn how to access your cluster, refer to  Accessing a cluster .  To learn how to use Cloudbreak to manage your cluster, refer to   Managing and monitoring clusters .", 
            "title": "Next steps"
        }, 
        {
            "location": "/azure-quick/index.html", 
            "text": "Quickstart on Azure\n\n\nThis quickstart documentation will help you get started with Cloudbreak. \n\n\nPrerequisites\n\n\nIn order to launch Cloudbreak from the ARM template you must:\n\n\n\n\n\n\nHave an existing an Azure account. If you don't have an account, you can create one at \nhttps://azure.microsoft.com\n.\n\n\n\n\n\n\nHave an SSH key pair. If needed, you can generate a new SSH key pair:\n\n\n\n\nOn MacOS X and Linux using \nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n  \n\n\nOn Windows using \nPuTTygen\n\n\n\n\n\n\n\n\nIn order to create a Cloudbreak credential, you Azure account must have the minimum permissions described in \nAzure roles\n. \n\n\n\n\n\n\nRelated links\n\n\nAzure roles\n\n\nPuTTygen\n (External)      \n\n\nLaunch Cloudbreak from the quickstart template\n\n\nLaunch Cloudbreak from an Azure Resource Manager (ARM) template by using the following steps. This is the quickstart deployment option. \n\n\nSteps\n\n\n\n\n\n\nLog in to your \nAzure Portal\n.\n\n\n\n\n\n\nClick here to get started with Cloudbreak installation using the Azure Resource Manager template:\n\n\n\n\n\n\n\n\nThe template for installing Cloudbreak will appear. On the \nBasics\n page, provide the following basic parameters:   \n\n\n\n\nAll parameters except \"SmartSense Id\" are required.\n\n\n\n\nBASICS\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSubscription\n\n\nSelect which existing subscription you want to use.\n\n\n\n\n\n\nResource group\n\n\nSelect an existing resource group or create a new one by selecting \nCreate new\n and entering a name for your new resource group. Cloudbreak resources will later be accessible in that chosen resource group.\n\n\n\n\n\n\nLocation\n\n\nSelect an Azure region in which you want to deploy Cloudbreak.\n\n\n\n\n\n\n\n\nSETTINGS\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nVm Size\n\n\nSelect virtual machine instance type to use for the Cloudbreak controller. The minimum instance type suitable for Cloudbreak is \nStandard_DS3\n. The minimum requirements are 16GB RAM, 40GB disk, 4 cores.\n\n\n\n\n\n\nAdmin Username\n\n\nCreate an admin login that you will use to log in to the Cloudbreak UI. Must be a valid email address. By default, admin@example.com is used but you should change it to your email address.\n\n\n\n\n\n\nAdmin User Password\n\n\nPassword for the admin login. Must be at least 8 characters containing letters, numbers, and symbols.\n\n\n\n\n\n\nUsername\n\n\nEnter an admin username for the virtual machine. You will use it to SSH to the VM. By default, \"cloudbreak\" is used.\n\n\n\n\n\n\nRemote Location\n\n\nEnter a valid \nCIDR IP\n or use one of the default tags. Default value is \nInternet\n which allows access from all IP addresses. Examples: \n10.0.0.0/24 will allow access from 10.0.0.0 through 10.0.0.255\n'Internet' will allow access from all. This is not a secure option but you can use it it you are just getting started and are not planning to have the instance on for a longer period. \n(Advanced) 'VirtualNetwork' will allow access from the address space of the Virtual Network.\n (Advanced) 'AzureLoadBalancer' will allow access from the address space of the load balancer.\nFor more information, refer to the \nPlan virtual networks\n in Azure documentation.\n\n\n\n\n\n\nSsh Key\n\n\nPaste your SSH public key.\nYou can use \npbcopy\n to quickly copy it. For example: \npbcopy \n /Users/homedir/.ssh/id_rsa.pub\n\n\n\n\n\n\nVnet New Or Existing\n\n\nBy default, Cloudbreak is launched in a new VNet called \ncbdeployerVnet\n and a new subnet called \ncbdeployerSubnet\n; if needed, you can customize the settings for the new VNet using available VNet and Subnet parameters.\n\n\n\n\n\n\nVnet Name\n\n\nProvide the name for a new Vnet. Default is \n`cbdeployerVnet\n.\n\n\n\n\n\n\nVnet Subnet Name\n\n\nProvide a name for a new subnet. Default is \ncbdeployerSubnet\n.\n\n\n\n\n\n\nVnet Address Prefix\n\n\nProvide a CIDR for the virtual network. Default is \n10.0.0.0/16\n.\n\n\n\n\n\n\nVnet Subnet Address Prefix\n\n\nProvide a CIDR for the subnet. Default is \n10.0.0.0/24\n.\n\n\n\n\n\n\nVnet RG Name\n\n\nThe name of the resource group in which the Vnet is located. If creating a new Vnet, enter the same resource group name as provided in the \nResource group\n field in the \nBASICS\n section.\n\n\n\n\n\n\n\n\n\n\n\n\nReview terms of use and check \"I agree to the terms and conditions stated above\".\n\n\n\n\n\n\nClick \nPurchase\n.\n\n\n\n\n\n\nYour deployment should be initiated.  \n\n\n\n\nIf you encounter errors, refer to \nTroubleshooting Cloudbreak on Azure\n. \n\n\n\n\n\n\nCleaning up after a failed deployment\n\n\nFor steps on how to delete Cloudbreak after a failed deployment, refer to \nDelete Cloudbreak on Azure\n.\n\n\n\n\n\n\n\n\nRelated links\n\n\nDelete Cloudbreak on Azure\n\n\nTroubleshooting Cloudbreak on Azure\n     \n\n\nCIDR IP\n \n\n\nPlan virtual networks\n (External)    \n\n\nAccess Cloudbreak web UI\n\n\nFollow these steps to obtain Cloudbreak VM's public IP address and log in to the Cloudbreak web UI. \n\n\nSteps\n       \n\n\n\n\n\n\nWhen your deployment succeeds, you will receive a notification in the top-right corner. You can click on the link provided to navigate to the resource group created earlier.\n\n\n\n\nThis only works right after deployment. At other times, you can find your resource group by selecting \nResource Groups\n from the service menu and then finding your resource group by name.\n\n\n\n\n\n\n\n\nOnce you've navigated to your resource group, click on \nDeployments\n and then click on \nMicrosoft.Template\n:\n\n\n\n\n\n\n\n\nFrom \nOutputs\n, you can copy the link by clicking on the copy icon:\n\n\n   \n\n\n\n\n\n\nPaste the link in your browser's address bar.\n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nNow you should be able to access Cloudbreak UI and log in with the \nAdmin email address\n and \nAdmin password\n that you created when launching Cloudbreak. \n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n  \n\n\n\n\n\n\nThe last task that you need to perform before you can use Cloudbreak is to create Cloudbreak credential.     \n\n\nRelated links\n\n\nCIDR IP\n (External) \n\n\nFilter network traffic with network security groups\n (External)  \n\n\nCreate Cloudbreak credential\n\n\nBefore you can start using Cloudbreak to create clusters, you must create a Cloudbreak credential. Cloudbreak credential allows Cloudbreak to authenticate with your Azure account and provision resources on your behalf.\n\n\nThere are two ways for Cloudbreak to authenticate with Azure: interactive and app-based. Since the interactive approach is simpler, the steps below describe how to configure an interactive Cloudbreak credential. \n\n\nFollow these steps to create an interactive Cloudbreak credential.\n\n\nPrerequisites\n\n\nYour account must have have an Owner role in order for the interactive credential creation to work.   \n\n\nIf your account does not have an Owner role, you must you the \napp-based credential\n option instead of the interactive option. To review the requirements for both options. refer to \nAzure roles\n. \n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane.\n\n\n\n\n\n\nClick \nCreate Credential\n.\n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Microsoft Azure\".\n\n\n\n\n\n\nSelect \nInteractive Login\n:\n\n\n     \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nSubscription Id\n\n\nCopy and paste the Subscription ID from your \nSubscriptions\n.\n\n\n\n\n\n\nTenant Id\n\n\nCopy and paste your Directory ID from your \nActive Directory\n \n \nProperties\n.\n\n\n\n\n\n\nAzure role type\n\n\nYou have the following options:\n\"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \"\nContributor\n\" role to create resources. This requires no further input.\n\"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources.\n\"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to \nAzure\n documentation. \nIf using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters: \nMicrosoft.Compute/*\n, \nMicrosoft.Network/*\n, \nMicrosoft.Storage/*\n, \nMicrosoft.Resources/*\n.\n\n\n\n\n\n\n\n\nTo obtain the \nSubscription Id\n:\n\n\n   \n\n\nTo obtain the \nTenant ID\n (actually \nDirectory Id\n):\n\n\n    \n\n\n\n\n\n\nAfter providing the parameters, click \nInteractive Login\n.\n\n\n\n\n\n\nCopy the code provided in the UI:\n\n\n     \n\n\n\n\n\n\nClick \nAzure login\n and a new \nDevice login\n page will open in a new browser tab:\n\n\n  \n\n\n\n\n\n\nNext, paste the code in field on the  \nDevice login\n page and click \nContinue\n.\n\n\n\n\n\n\nConfirm your account by selecting it:\n\n\n\n\n\n\n\n\nA confirmation page will appear, confirming that you have signed in to the Microsoft Azure Cross-platform Command Line Interface application on your device. You may now close this window.\n\n\n\n\nIf you encounter errors, refer to \nTroubleshooting Azure\n.\n\n\n\n\nCongratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to create clusters.\n\n\n\n\n\n\nRelated links\n\n\nCreate app-based credential\n \n\n\nAzure roles\n\n\nTroubleshooting Azure\n\n\nContributor\n (External)\n\n\nCustom roles in Azure\n (External)  \n\n\nCreate a cluster\n\n\nUse these steps to create a cluster. This section only covers minimal steps required for creating a cluster based on the basic settings (2-node cluster with default hardware and storage options).  \n\n\nSteps\n\n\n\n\n\n\nClick the \nCreate Cluster\n button and the \nCreate Cluster\n wizard is displayed.\n\n    By default, \nBasic\n view is displayed.\n\n\n  \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page:\n\n\n\n\nCluster Name\n: Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\nRegion\n: Select the region in which you would like to launch your cluster. \n\n\nCluster Type\n: Choose one of default cluster configurations.  \n\n\n\n\n\n\n\n\nClick \nNext\n three times to navigate to the \nSecurity Page\n. You do not need to enter anything on the \nHardware and Storage\n and \nNetwork\n because by default Cloudbreak suggests the instance types, storage, and network to use (a new network and subnet is created by default).   \n\n\n\n\n\n\nOne the \nSecurity\n page, provide the following:\n\n\n\n\nCluster User\n: This will be the user that you should use to log in to Ambari and other cluster UIs. By default, this is \nadmin\n.   \n\n\nCluster Password\n: Password for the cluster user.  \n\n\nSSH public key\n: Select the existing public SSH key or paste your key. The key will be placed on the cluster VMs so that you can use the matching private key to access the VMs via SSH. \n\n\n\n\n  \n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nNext steps\n\n\nTo learn how to access your cluster, refer to \nAccessing a cluster\n.\n\n\nTo learn how to use Cloudbreak to manage your cluster, refer to  \nManaging and monitoring clusters\n.", 
            "title": "Quickstart on Azure"
        }, 
        {
            "location": "/azure-quick/index.html#quickstart-on-azure", 
            "text": "This quickstart documentation will help you get started with Cloudbreak.", 
            "title": "Quickstart on Azure"
        }, 
        {
            "location": "/azure-quick/index.html#prerequisites", 
            "text": "In order to launch Cloudbreak from the ARM template you must:    Have an existing an Azure account. If you don't have an account, you can create one at  https://azure.microsoft.com .    Have an SSH key pair. If needed, you can generate a new SSH key pair:   On MacOS X and Linux using  ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"     On Windows using  PuTTygen     In order to create a Cloudbreak credential, you Azure account must have the minimum permissions described in  Azure roles .     Related links  Azure roles  PuTTygen  (External)", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/azure-quick/index.html#launch-cloudbreak-from-the-quickstart-template", 
            "text": "Launch Cloudbreak from an Azure Resource Manager (ARM) template by using the following steps. This is the quickstart deployment option.   Steps    Log in to your  Azure Portal .    Click here to get started with Cloudbreak installation using the Azure Resource Manager template:     The template for installing Cloudbreak will appear. On the  Basics  page, provide the following basic parameters:      All parameters except \"SmartSense Id\" are required.   BASICS     Parameter  Description      Subscription  Select which existing subscription you want to use.    Resource group  Select an existing resource group or create a new one by selecting  Create new  and entering a name for your new resource group. Cloudbreak resources will later be accessible in that chosen resource group.    Location  Select an Azure region in which you want to deploy Cloudbreak.     SETTINGS     Parameter  Description      Vm Size  Select virtual machine instance type to use for the Cloudbreak controller. The minimum instance type suitable for Cloudbreak is  Standard_DS3 . The minimum requirements are 16GB RAM, 40GB disk, 4 cores.    Admin Username  Create an admin login that you will use to log in to the Cloudbreak UI. Must be a valid email address. By default, admin@example.com is used but you should change it to your email address.    Admin User Password  Password for the admin login. Must be at least 8 characters containing letters, numbers, and symbols.    Username  Enter an admin username for the virtual machine. You will use it to SSH to the VM. By default, \"cloudbreak\" is used.    Remote Location  Enter a valid  CIDR IP  or use one of the default tags. Default value is  Internet  which allows access from all IP addresses. Examples:  10.0.0.0/24 will allow access from 10.0.0.0 through 10.0.0.255 'Internet' will allow access from all. This is not a secure option but you can use it it you are just getting started and are not planning to have the instance on for a longer period.  (Advanced) 'VirtualNetwork' will allow access from the address space of the Virtual Network.  (Advanced) 'AzureLoadBalancer' will allow access from the address space of the load balancer. For more information, refer to the  Plan virtual networks  in Azure documentation.    Ssh Key  Paste your SSH public key. You can use  pbcopy  to quickly copy it. For example:  pbcopy   /Users/homedir/.ssh/id_rsa.pub    Vnet New Or Existing  By default, Cloudbreak is launched in a new VNet called  cbdeployerVnet  and a new subnet called  cbdeployerSubnet ; if needed, you can customize the settings for the new VNet using available VNet and Subnet parameters.    Vnet Name  Provide the name for a new Vnet. Default is  `cbdeployerVnet .    Vnet Subnet Name  Provide a name for a new subnet. Default is  cbdeployerSubnet .    Vnet Address Prefix  Provide a CIDR for the virtual network. Default is  10.0.0.0/16 .    Vnet Subnet Address Prefix  Provide a CIDR for the subnet. Default is  10.0.0.0/24 .    Vnet RG Name  The name of the resource group in which the Vnet is located. If creating a new Vnet, enter the same resource group name as provided in the  Resource group  field in the  BASICS  section.       Review terms of use and check \"I agree to the terms and conditions stated above\".    Click  Purchase .    Your deployment should be initiated.     If you encounter errors, refer to  Troubleshooting Cloudbreak on Azure .     Cleaning up after a failed deployment  For steps on how to delete Cloudbreak after a failed deployment, refer to  Delete Cloudbreak on Azure .     Related links  Delete Cloudbreak on Azure  Troubleshooting Cloudbreak on Azure        CIDR IP    Plan virtual networks  (External)", 
            "title": "Launch Cloudbreak from the quickstart template"
        }, 
        {
            "location": "/azure-quick/index.html#access-cloudbreak-web-ui", 
            "text": "Follow these steps to obtain Cloudbreak VM's public IP address and log in to the Cloudbreak web UI.   Steps            When your deployment succeeds, you will receive a notification in the top-right corner. You can click on the link provided to navigate to the resource group created earlier.   This only works right after deployment. At other times, you can find your resource group by selecting  Resource Groups  from the service menu and then finding your resource group by name.     Once you've navigated to your resource group, click on  Deployments  and then click on  Microsoft.Template :     From  Outputs , you can copy the link by clicking on the copy icon:         Paste the link in your browser's address bar.    Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...       The login page is displayed:        Now you should be able to access Cloudbreak UI and log in with the  Admin email address  and  Admin password  that you created when launching Cloudbreak.     Upon a successful login, you are redirected to the dashboard:        The last task that you need to perform before you can use Cloudbreak is to create Cloudbreak credential.       Related links  CIDR IP  (External)   Filter network traffic with network security groups  (External)", 
            "title": "Access Cloudbreak web UI"
        }, 
        {
            "location": "/azure-quick/index.html#create-cloudbreak-credential", 
            "text": "Before you can start using Cloudbreak to create clusters, you must create a Cloudbreak credential. Cloudbreak credential allows Cloudbreak to authenticate with your Azure account and provision resources on your behalf.  There are two ways for Cloudbreak to authenticate with Azure: interactive and app-based. Since the interactive approach is simpler, the steps below describe how to configure an interactive Cloudbreak credential.   Follow these steps to create an interactive Cloudbreak credential.  Prerequisites  Your account must have have an Owner role in order for the interactive credential creation to work.     If your account does not have an Owner role, you must you the  app-based credential  option instead of the interactive option. To review the requirements for both options. refer to  Azure roles .   Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.    Click  Create Credential .    Under  Cloud provider , select \"Microsoft Azure\".    Select  Interactive Login :           Provide the following information:     Parameter  Description      Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Subscription Id  Copy and paste the Subscription ID from your  Subscriptions .    Tenant Id  Copy and paste your Directory ID from your  Active Directory     Properties .    Azure role type  You have the following options: \"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \" Contributor \" role to create resources. This requires no further input. \"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources. \"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to  Azure  documentation.  If using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters:  Microsoft.Compute/* ,  Microsoft.Network/* ,  Microsoft.Storage/* ,  Microsoft.Resources/* .     To obtain the  Subscription Id :       To obtain the  Tenant ID  (actually  Directory Id ):          After providing the parameters, click  Interactive Login .    Copy the code provided in the UI:           Click  Azure login  and a new  Device login  page will open in a new browser tab:        Next, paste the code in field on the   Device login  page and click  Continue .    Confirm your account by selecting it:     A confirmation page will appear, confirming that you have signed in to the Microsoft Azure Cross-platform Command Line Interface application on your device. You may now close this window.   If you encounter errors, refer to  Troubleshooting Azure .   Congratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to create clusters.    Related links  Create app-based credential    Azure roles  Troubleshooting Azure  Contributor  (External)  Custom roles in Azure  (External)", 
            "title": "Create Cloudbreak credential"
        }, 
        {
            "location": "/azure-quick/index.html#create-a-cluster", 
            "text": "Use these steps to create a cluster. This section only covers minimal steps required for creating a cluster based on the basic settings (2-node cluster with default hardware and storage options).    Steps    Click the  Create Cluster  button and the  Create Cluster  wizard is displayed. \n    By default,  Basic  view is displayed.        On the  General Configuration  page:   Cluster Name : Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.  Region : Select the region in which you would like to launch your cluster.   Cluster Type : Choose one of default cluster configurations.       Click  Next  three times to navigate to the  Security Page . You do not need to enter anything on the  Hardware and Storage  and  Network  because by default Cloudbreak suggests the instance types, storage, and network to use (a new network and subnet is created by default).       One the  Security  page, provide the following:   Cluster User : This will be the user that you should use to log in to Ambari and other cluster UIs. By default, this is  admin .     Cluster Password : Password for the cluster user.    SSH public key : Select the existing public SSH key or paste your key. The key will be placed on the cluster VMs so that you can use the matching private key to access the VMs via SSH.          Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.", 
            "title": "Create a cluster"
        }, 
        {
            "location": "/azure-quick/index.html#next-steps", 
            "text": "To learn how to access your cluster, refer to  Accessing a cluster .  To learn how to use Cloudbreak to manage your cluster, refer to   Managing and monitoring clusters .", 
            "title": "Next steps"
        }, 
        {
            "location": "/gcp-quick/index.html", 
            "text": "Quickstart on GCP\n\n\nThis quickstart documentation will help you get started with Cloudbreak. \n\n\nPrerequisites\n\n\nPrior to launching Cloudbreak, you must meet these prerequisites.\n\n\nCloud SDK\n\n\nIn order to use the Cloud Deployment Manager, you must install the Google Cloud SDK on your machine. The SDK contains the gcloud CLI tool, which is used to deploy Cloudbreak. \n\n\nFor instructions, refer to \nInstalling Google Cloud SDK\n in the Google Cloud documentation. Make sure to perform all of the steps and validate that the \ngcloud\n command works on your computer. Only after validating, proceed to the next step.  \n\n\nRelated links\n\n\nInstalling Google Cloud SDK\n (External)  \n\n\nGCP APIs\n\n\nYou must enable the \nCompute Engine API\n and the \nCloud Runtime Configuration API\n services.\n\n\nSteps\n \n\n\n\n\n\n\nIn GCP web console, from the services menu, select \nAPIs \n Services\n: \n\n\n\n\n\n\n\n\nClick on \nEnable APIs and services\n:\n\n\n\n\n\n\n\n\nOn this page:\n\n\n\n\nIn the filter, type \"Compute Engine API\".  \n\n\nClick on the corresponding tile to navigate to the API details   \n\n\n\n\n  \n\n\n\n\n\n\nClick on the \nEnable\n button. Once the API has been enabled you should see:\n\n\n\n\n\n\n\n\nPerform the same for the \"Cloud Runtime Configuration API\". \n\n\n\n\n\n\nService account\n\n\nCreate a service account that has the following roles:\n\n\nComputer Engine roles: \n\n\n\n\nCompute Image User     \n\n\nCompute Instance Admin \n\n\nCompute Network Admin  \n\n\nCompute Security Admin  \n\n\n\n\nStorage roles:\n\n\n\n\nStorage \n Storage Admin \n\n\n\n\nOther roles: \n\n\n\n\nCloud RuntimeConfig Admin (You can find it under \"Other\")\n\n\n\n\nIf you already have a service account and a JSON key but you need to update the permissions for the account, you can do it from \nIAM \n admin\n \n \nIAM\n. If you need to create a service account, follow these steps. \n\n\nSteps\n\n\n\n\n\n\nTo create a service account In GCP web console, from the services menu, select \nIAM \n admin\n \n \nService account\n:\n\n\n  \n\n\n\n\n\n\nClick on \nCreate service account\n:\n\n\n  \n\n\n\n\n\n\nProvide the following:\n\n\n\n\nEnter the \nService account name\n.   \n\n\nThis will determine your service account email. Make a note of this service account email. You will need to provide it when creating a Cloudbreak credential.       \n\n\n\n\n\n\nUnder \nRole\n, select the roles described above.  \n\n\nUnder \nKey type\n, select \nJSON\n.    \n\n\n\n\n  \n\n\n\n\n\n\nClick \nCreate\n.   \n\n\n\n\n\n\nThe JSON key will be downloaded on your machine. You will need it later to create a Cloudbreak credential.  \n\n\n\n\n\n\nLaunch from the quickstart template\n\n\nLaunch Cloudbreak from an Cloud Deployment Manager template by using the following steps. This is the quickstart deployment option. \n\n\nSteps\n       \n\n\n\n\n\n\nLog in to your GitHub account.  \n\n\n\n\n\n\nRun the following command to download the following Hortonworks repo onto your computer and check out the release branch:\n\n\ngit clone https://github.com/hortonworks/cbd-quickstart\ncd cbd-quickstart\ngit checkout 2.7.0\n \n\n\n\n\n\n\nOn your computer, browse to the \ncbd-quickstart/gcp\n.         \n\n\n\n\n\n\nOpen the vm_template_config.yaml file in a text editor. \n\n\n\n\n\n\nEdit the file by updating the property values: \n\n\n\n\nDo not edit any other parameters in the vm_template_config.yaml file. \n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nDefault\n\n\n\n\n\n\n\n\n\n\nregion\n\n\nThe GCP region in which you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions \nsupported by GCP\n.\n\n\nus-central-1\n\n\n\n\n\n\nzone\n\n\nThe GCP region's zone in which you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions \nsupported by GCP\n.\n\n\nus-central1-a\n\n\n\n\n\n\ninstance_type\n\n\nSelect the VM instance type. For available instance types, reefer to \nMachine Types\n in GCP documentation.\n\n\nn1-standard-4\n\n\n\n\n\n\nssh_pub_key\n\n\nPaste your SSH public key.\n\n\n...\n\n\n\n\n\n\nos_user\n\n\nEnter the name of the user that you would like to use to SSH to the VM.\n\n\ncloudbreak\n\n\n\n\n\n\nuser_email\n\n\nEnter the email address that you would like to use to log in to Cloudbreak.\n\n\nadmin@cloudbreak.com\n\n\n\n\n\n\nuser_password\n\n\nEnter the password that you would like to use to log in to Cloudbreak.\n\n\ncloudbreak\n\n\n\n\n\n\nservice_account_email\n\n\nThe email for the service account created in prerequisites.\n\n\n...\n\n\n\n\n\n\n\n\n\n\n\n\nSave the changes on your local machine. \n\n\n\n\n\n\nRun the following command to create a new deployment:\n\n\ngcloud deployment-manager deployments create cbd-deployment --config=/path/to/your/file/vm_template_config.yaml\n\n\nFor example:\n\n\ngcloud deployment-manager deployments create cbd-deployment --config=/Users/youruser/Documents/cbd-quickstart/gcp/vm_template_config.yaml\n\n\n\n\n\n\nOnce your deployment has finished, you will see the following:\n\n\ngcloud deployment-manager deployments create cbd-deployment --config=/Users/youruser/Documents/cbd-quickstart/gcp/vm_template_config.yaml\nWaiting for create operation-1527749967574-56d7b021f73f1-773609ee-060d4332...done.\nCreate operation operation-1527749967574-56d7b021f73f1-773609ee-060d4332 completed successfully.\nNAME                            TYPE                          STATE      ERRORS  INTENT\ncbd-deployment-default-route-1  compute.v1.route              COMPLETED  []\ncbd-deployment-network          compute.v1.network            COMPLETED  []\ncbd-deployment-startup-config   runtimeconfig.v1beta1.config  COMPLETED  []\ncbd-deployment-startup-waiter   runtimeconfig.v1beta1.waiter  COMPLETED  []\ncbd-deployment-subnet           compute.v1.subnetwork         COMPLETED  []\ncbd-deployment-vm               compute.v1.instance           COMPLETED  []\nfirewall-cbd-deployment         compute.v1.firewall           COMPLETED  []\nOUTPUTS       VALUE\ndeploymentIp  35.224.36.96\n\n\n\n\nCleaning up after a failed deployment\n\n\nFor steps on how to delete Cloudbreak after a failed deployment, refer to \nDelete Cloudbreak on GCP\n.\n\n\n\n\n\n\n\n\nThe last output should be the the deploymentIp. Copy the IP address and paste it in the browser so that you can log in to the Cloudbreak web UI.                   \n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nLog in to the Cloudbreak web UI using the credentials that you configured in the vm_template_config.yaml file.  \n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n \n\n\n\n\n\n\nRelated Links\n\n\nMachine types\n (External)  \n\n\nRegions and zones\n (External)     \n\n\nCreate Cloudbreak credential\n\n\nCloudbreak works by connecting your GCP account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a \nCloudbreak credential\n.  \n\n\nPrerequisites\n\n\nAs described in the \nService account\n section in the prerequisites, in order to launch clusters on GCP via Cloudbreak, you must have a service account that Cloudbreak can use to create resources.   \n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Google Cloud Platform\":\n\n\n  \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKey type\n\n\nSelect JSON or P12. Since activating service accounts with P12 private keys has been deprecated in the Cloud SDK, we recommend using JSON.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nProject Id\n\n\n(Only required for P12 key type) Enter the project ID. You can obtain it from your GCP account by clicking on the name of your project at the top of the page and copying the \nID\n.\n\n\n\n\n\n\nService Account Email Address\n\n\n(Only required for P12 key type) \"Service account ID\" value for your service account created in prerequisites. You can find it on GCP at \nIAM \n Admin\n \n \nService accounts\n.\n\n\n\n\n\n\nService Account Private Key\n\n\nUpload the key that you created in the prerequisites when creating a service account.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to create clusters. \n\n\n\n\n\n\nRelated links\n\n\nCloudbreak credential\n\n\nService account\n \n\n\nCreate a cluster\n\n\nUse these steps to create a cluster. This section only covers minimal steps required for creating a cluster based on the basic settings (2-node cluster with default hardware and storage options).  \n\n\nSteps\n\n\n\n\n\n\nClick the \nCreate Cluster\n button and the \nCreate Cluster\n wizard is displayed.\n\n    By default, \nBasic\n view is displayed.\n\n\n  \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page:\n\n\n\n\nCluster Name\n: Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\nRegion\n: Select the region in which you would like to launch your cluster. \n\n\nCluster Type\n: Choose one of default cluster configurations.  \n\n\n\n\n\n\n\n\nClick \nNext\n three times to navigate to the \nSecurity Page\n. You do not need to enter anything on the \nHardware and Storage\n and \nNetwork\n because by default Cloudbreak suggests the instance types, storage, and network to use (a new network and subnet is created by default).   \n\n\n\n\n\n\nOne the \nSecurity\n page, provide the following:\n\n\n\n\nCluster User\n: This will be the user that you should use to log in to Ambari and other cluster UIs. By default, this is \nadmin\n.   \n\n\nCluster Password\n: Password for the cluster user.  \n\n\nSSH public key\n: Select the existing public SSH key or paste your key. The key will be placed on the cluster VMs so that you can use the matching private key to access the VMs via SSH. \n\n\n\n\n  \n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nNext steps\n\n\nTo learn how to access your cluster, refer to \nAccessing a cluster\n.\n\n\nTo learn how to use Cloudbreak to manage your cluster, refer to  \nManaging and monitoring clusters\n.", 
            "title": "Quickstart on GCP"
        }, 
        {
            "location": "/gcp-quick/index.html#quickstart-on-gcp", 
            "text": "This quickstart documentation will help you get started with Cloudbreak.", 
            "title": "Quickstart on GCP"
        }, 
        {
            "location": "/gcp-quick/index.html#prerequisites", 
            "text": "Prior to launching Cloudbreak, you must meet these prerequisites.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/gcp-quick/index.html#cloud-sdk", 
            "text": "In order to use the Cloud Deployment Manager, you must install the Google Cloud SDK on your machine. The SDK contains the gcloud CLI tool, which is used to deploy Cloudbreak.   For instructions, refer to  Installing Google Cloud SDK  in the Google Cloud documentation. Make sure to perform all of the steps and validate that the  gcloud  command works on your computer. Only after validating, proceed to the next step.    Related links  Installing Google Cloud SDK  (External)", 
            "title": "Cloud SDK"
        }, 
        {
            "location": "/gcp-quick/index.html#gcp-apis", 
            "text": "You must enable the  Compute Engine API  and the  Cloud Runtime Configuration API  services.  Steps      In GCP web console, from the services menu, select  APIs   Services :      Click on  Enable APIs and services :     On this page:   In the filter, type \"Compute Engine API\".    Click on the corresponding tile to navigate to the API details            Click on the  Enable  button. Once the API has been enabled you should see:     Perform the same for the \"Cloud Runtime Configuration API\".", 
            "title": "GCP APIs"
        }, 
        {
            "location": "/gcp-quick/index.html#service-account", 
            "text": "Create a service account that has the following roles:  Computer Engine roles:    Compute Image User       Compute Instance Admin   Compute Network Admin    Compute Security Admin     Storage roles:   Storage   Storage Admin    Other roles:    Cloud RuntimeConfig Admin (You can find it under \"Other\")   If you already have a service account and a JSON key but you need to update the permissions for the account, you can do it from  IAM   admin     IAM . If you need to create a service account, follow these steps.   Steps    To create a service account In GCP web console, from the services menu, select  IAM   admin     Service account :        Click on  Create service account :        Provide the following:   Enter the  Service account name .     This will determine your service account email. Make a note of this service account email. You will need to provide it when creating a Cloudbreak credential.           Under  Role , select the roles described above.    Under  Key type , select  JSON .             Click  Create .       The JSON key will be downloaded on your machine. You will need it later to create a Cloudbreak credential.", 
            "title": "Service account"
        }, 
        {
            "location": "/gcp-quick/index.html#launch-from-the-quickstart-template", 
            "text": "Launch Cloudbreak from an Cloud Deployment Manager template by using the following steps. This is the quickstart deployment option.   Steps            Log in to your GitHub account.      Run the following command to download the following Hortonworks repo onto your computer and check out the release branch:  git clone https://github.com/hortonworks/cbd-quickstart\ncd cbd-quickstart\ngit checkout 2.7.0      On your computer, browse to the  cbd-quickstart/gcp .             Open the vm_template_config.yaml file in a text editor.     Edit the file by updating the property values:    Do not edit any other parameters in the vm_template_config.yaml file.       Parameter  Description  Default      region  The GCP region in which you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions  supported by GCP .  us-central-1    zone  The GCP region's zone in which you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions  supported by GCP .  us-central1-a    instance_type  Select the VM instance type. For available instance types, reefer to  Machine Types  in GCP documentation.  n1-standard-4    ssh_pub_key  Paste your SSH public key.  ...    os_user  Enter the name of the user that you would like to use to SSH to the VM.  cloudbreak    user_email  Enter the email address that you would like to use to log in to Cloudbreak.  admin@cloudbreak.com    user_password  Enter the password that you would like to use to log in to Cloudbreak.  cloudbreak    service_account_email  The email for the service account created in prerequisites.  ...       Save the changes on your local machine.     Run the following command to create a new deployment:  gcloud deployment-manager deployments create cbd-deployment --config=/path/to/your/file/vm_template_config.yaml  For example:  gcloud deployment-manager deployments create cbd-deployment --config=/Users/youruser/Documents/cbd-quickstart/gcp/vm_template_config.yaml    Once your deployment has finished, you will see the following:  gcloud deployment-manager deployments create cbd-deployment --config=/Users/youruser/Documents/cbd-quickstart/gcp/vm_template_config.yaml\nWaiting for create operation-1527749967574-56d7b021f73f1-773609ee-060d4332...done.\nCreate operation operation-1527749967574-56d7b021f73f1-773609ee-060d4332 completed successfully.\nNAME                            TYPE                          STATE      ERRORS  INTENT\ncbd-deployment-default-route-1  compute.v1.route              COMPLETED  []\ncbd-deployment-network          compute.v1.network            COMPLETED  []\ncbd-deployment-startup-config   runtimeconfig.v1beta1.config  COMPLETED  []\ncbd-deployment-startup-waiter   runtimeconfig.v1beta1.waiter  COMPLETED  []\ncbd-deployment-subnet           compute.v1.subnetwork         COMPLETED  []\ncbd-deployment-vm               compute.v1.instance           COMPLETED  []\nfirewall-cbd-deployment         compute.v1.firewall           COMPLETED  []\nOUTPUTS       VALUE\ndeploymentIp  35.224.36.96   Cleaning up after a failed deployment  For steps on how to delete Cloudbreak after a failed deployment, refer to  Delete Cloudbreak on GCP .     The last output should be the the deploymentIp. Copy the IP address and paste it in the browser so that you can log in to the Cloudbreak web UI.                       Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...       The login page is displayed:        Log in to the Cloudbreak web UI using the credentials that you configured in the vm_template_config.yaml file.      Upon a successful login, you are redirected to the dashboard:       Related Links  Machine types  (External)    Regions and zones  (External)", 
            "title": "Launch from the quickstart template"
        }, 
        {
            "location": "/gcp-quick/index.html#create-cloudbreak-credential", 
            "text": "Cloudbreak works by connecting your GCP account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a  Cloudbreak credential .    Prerequisites  As described in the  Service account  section in the prerequisites, in order to launch clusters on GCP via Cloudbreak, you must have a service account that Cloudbreak can use to create resources.     Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Google Cloud Platform\":        Provide the following information:     Parameter  Description      Key type  Select JSON or P12. Since activating service accounts with P12 private keys has been deprecated in the Cloud SDK, we recommend using JSON.    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Project Id  (Only required for P12 key type) Enter the project ID. You can obtain it from your GCP account by clicking on the name of your project at the top of the page and copying the  ID .    Service Account Email Address  (Only required for P12 key type) \"Service account ID\" value for your service account created in prerequisites. You can find it on GCP at  IAM   Admin     Service accounts .    Service Account Private Key  Upload the key that you created in the prerequisites when creating a service account.       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to create clusters.     Related links  Cloudbreak credential  Service account", 
            "title": "Create Cloudbreak credential"
        }, 
        {
            "location": "/gcp-quick/index.html#create-a-cluster", 
            "text": "Use these steps to create a cluster. This section only covers minimal steps required for creating a cluster based on the basic settings (2-node cluster with default hardware and storage options).    Steps    Click the  Create Cluster  button and the  Create Cluster  wizard is displayed. \n    By default,  Basic  view is displayed.        On the  General Configuration  page:   Cluster Name : Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.  Region : Select the region in which you would like to launch your cluster.   Cluster Type : Choose one of default cluster configurations.       Click  Next  three times to navigate to the  Security Page . You do not need to enter anything on the  Hardware and Storage  and  Network  because by default Cloudbreak suggests the instance types, storage, and network to use (a new network and subnet is created by default).       One the  Security  page, provide the following:   Cluster User : This will be the user that you should use to log in to Ambari and other cluster UIs. By default, this is  admin .     Cluster Password : Password for the cluster user.    SSH public key : Select the existing public SSH key or paste your key. The key will be placed on the cluster VMs so that you can use the matching private key to access the VMs via SSH.          Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.", 
            "title": "Create a cluster"
        }, 
        {
            "location": "/gcp-quick/index.html#next-steps", 
            "text": "To learn how to access your cluster, refer to  Accessing a cluster .  To learn how to use Cloudbreak to manage your cluster, refer to   Managing and monitoring clusters .", 
            "title": "Next steps"
        }, 
        {
            "location": "/aws-pre/index.html", 
            "text": "Prerequisites on AWS\n\n\nBefore launching Cloudbreak on AWS, you must meet the following prerequisites.\n\n\nAWS account\n\n\nIn order to launch Cloudbreak on AWS, you must log in to your AWS account. If you don't have an account, you can create one at \nhttps://aws.amazon.com/\n.\n\n\nAWS region\n\n\nDecide in which AWS region you would like to launch Cloudbreak. The following AWS regions are supported:\n\n\n\n\n\n\n\n\nRegion name\n\n\nRegion\n\n\n\n\n\n\n\n\n\n\nEU (Ireland)\n\n\neu-west-1\n\n\n\n\n\n\nEU (Frankfurt)\n\n\neu-central-1\n\n\n\n\n\n\nUS East (N. Virginia)\n\n\nus-east-1\n\n\n\n\n\n\nUS West (N. California)\n\n\nus-west-1\n\n\n\n\n\n\nUS West (Oregon)\n\n\nus-west-2\n\n\n\n\n\n\nSouth America (S\u00e3o Paulo)\n\n\nsa-east-1\n\n\n\n\n\n\nAsia Pacific (Tokyo)\n\n\nap-northeast-1\n\n\n\n\n\n\nAsia Pacific (Singapore)\n\n\nap-southeast-1\n\n\n\n\n\n\nAsia Pacific (Sydney)\n\n\nap-southeast-2\n\n\n\n\n\n\n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.\n\n\nRelated links\n\n\nAWS regions and endpoints\n (External)   \n\n\nSSH key pair\n\n\nImport an existing key pair or generate a new key pair in the AWS region which you are planning to use for launching Cloudbreak and clusters. You can do this using the following steps.\n\n\nSteps\n\n\n\n\nNavigate to the Amazon EC2 console at https://console.aws.amazon.com/ec2/.  \n\n\nCheck the region listed in the top right corner to make sure that you are in the correct region.  \n\n\nIn the left pane, find \nNETWORK AND SECURITY\n and click \nKey Pairs\n.   \n\n\nDo one of the following:\n\n\nClick \nCreate Key Pair\n to create a new key pair. Your private key file will be automatically downloaded onto your computer. Make sure to save it in a secure location. You will need it to SSH to the cluster nodes. You may want to change access settings for the file using \nchmod 400 my-key-pair.pem\n.  \n\n\nClick \nImport Key Pair\n to upload an existing public key and then select it and click \nImport\n. Make sure that you have access to its corresponding private key.    \n\n\n\n\n\n\n\n\nYou need this SSH key pair to SSH to the Cloudbreak instance and start Cloudbreak.\n\n\nRelated links\n\n\nCreating a key pair using Amazon EC2\n (External)\n\n\nVirtual network\n\n\nYou must have a virtual network configured on your cloud provider.\n\n\nSecurity group\n\n\nPorts 22 (SSH), 80 (HTTPS), and 443 (HTTPS) must be open on the security group\n\n\nAuthentication\n\n\nBefore you can start using Cloudbreak for provisioning clusters, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. There are two ways to do this:\n\n\n\n\n\n\nKey-based\n: This is a simpler option which does not require additional configuration at this point. It requires that you provide your AWS access key and secret key pair in the Cloudbreak web UI later. All you need to do now is check your AWS account and ensure that you can access this key pair.\n\n\n\n\n\n\nRole-based\n: This requires that you or your AWS admin create an IAM role to allow Cloudbreak to assume AWS roles (the \"AssumeRole\" policy).\n\n\n\n\n\n\n(Option 1) Use key-based authentication\n\n\nIf you are using key-based authentication for Cloudbreak on AWS, you must be able to provide your AWS access key and secret key pair. Cloudbreak will use these keys to launch the resources. You must provide the access and secret keys later in the Cloudbreak web UI later when creating a credential.\n\n\nIf you choose this option, all you need to do at this point is check your AWS account and make sure that you can access this key pair. You can generate new access and secret keys from the \nIAM Console\n \n \nUsers\n. Next, select a user and click on the \nSecurity credentials\n tab:\n\n\n\n\nThe minimum set of permissions required by Cloudbreak are the same as for the \nCredentialRole\n. \n\n\nIf you choose this option, you can proceed to launch Cloudbreak.\n\n\n(Option 2) Configure role-based authentication\n\n\nIf you are using role-based authentication for Cloudbreak on AWS, you must create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).\n\n\nThe following table provides contextual information about the two roles required: \n\n\n\n\n\n\n\n\nRole\n\n\nPurpose\n\n\nOverview of steps\n\n\nConfiguration\n\n\n\n\n\n\n\n\n\n\nCloudbreakRole\n\n\nAllows Cloudbreak to assume other IAM roles - specifically the CredentialRole.\n\n\nCreate a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition and steps for creating the CloudbreakRole are provided below.\n\n\nWhen launching Cloudbreak, you will attach the \"CloudbreakRole\" IAM role to the VM.\nIf you are using hosted Cloudbreak, you do not need to perform this step.\n\n\n\n\n\n\nCredentialRole\n\n\nAllows Cloudbreak to create AWS resources required for clusters.\n\n\nCreate a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition and steps for creating the CredentialRole are provided below.\n When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d. See steps below.\n\n\nOnce you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak credential.\n\n\n\n\n\n\n\n\n\n\nThese role and policy names are just examples. You may use different names when creating your resources.  \n\n\nCloudbreakRole: Alternatively, instead of attaching the \"CloudbreakRole\" role during the VM launch, you can assign the \"CloudbreakRole\" to an IAM user and then add the access and secret key of that user to your 'Profile'.\n\n\nCredentialRole: Alternatively you can generate the \"CredentialRole\" role later once your Cloudbreak VM is running by SSHing to the Cloudbreak VM and running the \ncbd aws generate-role\n command. This command creates a role with the name \"cbreak-deployer\" (equivalent to the \"CredentialRole\"). To customize the name of the role, add \nexport AWS_ROLE_NAME=my-cloudbreak-role-name\n (where \"my-cloudbreak-role-name\" is your custom role name) as a new line to your Profile. If you choose this option, you must make sure that the \"CloudbreakRole\" or the IAM user have a permission not only to assume a role but also to create a role.  \n\n\n\n\nYou can create these roles in the \nIAM console\n, on the \nRoles\n page via the \nCreate Role\n option. Detailed steps are provided below.\n\n\nRelated links\n\n\nUsing instance profiles\n (External)\n\n\nUsing an IAM role to grant permissions to applications\n (External) \n\n\nCreate CloudbreakRole\n\n\nUse these steps to create CloudbreakRole.\n\n\nUse the following \"AssumeRole\" policy definition:\n\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": {\n    \"Sid\": \"Stmt1400068149000\",\n    \"Effect\": \"Allow\",\n    \"Action\": [\"sts:AssumeRole\"],\n    \"Resource\": \"*\"\n  }\n}\n\n\n\n\nSteps\n\n\n\n\n\n\nNavigate to the \nIAM console\n \n \nRoles\n and click \nCreate Role\n.\n\n\n\n\n\n\n\n\nIn the \"Create Role\" wizard, select \nAWS service\n role type and then select any service.\n\n\n\n\n\n\n\n\nWhen done, click \nNext: Permissions\n to navigate to the next page in the wizard.\n\n\n\n\n\n\nClick \nCreate policy\n.\n\n\n\n\n\n\n\n\nClick \nSelect\n next to \"Create Your Own Policy\".\n\n\n  \n\n\n\n\n\n\nIn the \nPolicy Name\n field, enter \"AssumeRole\" and in the \nPolicy Document\n paste the policy definition. You can either copy it from the section preceding these steps or download and copy it from \nhere\n.\n\n\n  \n\n\n\n\n\n\nWhen done, click \nCreate Policy\n.\n\n\n\n\n\n\nClick \nRefresh\n. Next, find the \"AssumeRole\" policy that you just created and select it by checking the box.\n\n\n\n\n\n\n\n\nWhen done, click \nNext: Review\n.\n\n\n\n\n\n\nIn the \nRoles name\n field, enter role name, for example \"CloudbreakRole\".\n\n\n\n\n\n\n\n\nWhen done, click \nCreate role\n to finish the role creation process.\n\n\n\n\n\n\nRelated links\n\n\nCloudbreakRole\n  \n\n\nCreate CredentialRole\n\n\nUse these steps to create CredentialRole.\n\n\nUse the following \"cb-policy\" policy definition:\n\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"cloudformation:CreateStack\",\n        \"cloudformation:DeleteStack\",\n        \"cloudformation:DescribeStackEvents\",\n        \"cloudformation:DescribeStackResource\",\n        \"cloudformation:DescribeStacks\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:AllocateAddress\",\n        \"ec2:AssociateAddress\",\n        \"ec2:AssociateRouteTable\",\n        \"ec2:AuthorizeSecurityGroupIngress\",\n        \"ec2:DescribeRegions\",\n        \"ec2:DescribeAvailabilityZones\",\n        \"ec2:CreateRoute\",\n        \"ec2:CreateRouteTable\",\n        \"ec2:CreateSecurityGroup\",\n        \"ec2:CreateSubnet\",\n        \"ec2:CreateTags\",\n        \"ec2:CreateVpc\",\n        \"ec2:ModifyVpcAttribute\",\n        \"ec2:DeleteSubnet\",\n        \"ec2:CreateInternetGateway\",\n        \"ec2:CreateKeyPair\",\n        \"ec2:DeleteKeyPair\",\n        \"ec2:DisassociateAddress\",\n        \"ec2:DisassociateRouteTable\",\n        \"ec2:ModifySubnetAttribute\",\n        \"ec2:ReleaseAddress\",\n        \"ec2:DescribeAddresses\",\n        \"ec2:DescribeImages\",\n        \"ec2:DescribeInstanceStatus\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeInternetGateways\",\n        \"ec2:DescribeKeyPairs\",\n        \"ec2:DescribeRouteTables\",\n        \"ec2:DescribeSecurityGroups\",\n        \"ec2:DescribeSubnets\",\n        \"ec2:DescribeVpcs\",\n        \"ec2:DescribeSpotInstanceRequests\",\n        \"ec2:DescribeVpcAttribute\",\n        \"ec2:ImportKeyPair\",\n        \"ec2:AttachInternetGateway\",\n        \"ec2:DeleteVpc\",\n        \"ec2:DeleteSecurityGroup\",\n        \"ec2:DeleteRouteTable\",\n        \"ec2:DeleteInternetGateway\",\n        \"ec2:DeleteRouteTable\",\n        \"ec2:DeleteRoute\",\n        \"ec2:DetachInternetGateway\",\n        \"ec2:RunInstances\",\n        \"ec2:StartInstances\",\n        \"ec2:StopInstances\",\n        \"ec2:TerminateInstances\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:ListRolePolicies\",\n        \"iam:GetRolePolicy\",\n        \"iam:ListAttachedRolePolicies\",\n        \"iam:ListInstanceProfiles\",\n        \"iam:PutRolePolicy\",\n        \"iam:PassRole\",\n        \"iam:GetRole\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"autoscaling:CreateAutoScalingGroup\",\n        \"autoscaling:CreateLaunchConfiguration\",\n        \"autoscaling:DeleteAutoScalingGroup\",\n        \"autoscaling:DeleteLaunchConfiguration\",\n        \"autoscaling:DescribeAutoScalingGroups\",\n        \"autoscaling:DescribeLaunchConfigurations\",\n        \"autoscaling:DescribeScalingActivities\",\n        \"autoscaling:DetachInstances\",\n        \"autoscaling:ResumeProcesses\",\n        \"autoscaling:SuspendProcesses\",\n        \"autoscaling:UpdateAutoScalingGroup\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}\n\n\n\nSteps\n\n\n\n\n\n\nNavigate to the \nIAM console\n \n \nRoles\n and click \nCreate Role\n.\n\n\n\n\n\n\n\n\nIn the \"Create Role\" wizard, select \nAnother AWS account\n role type. Next, provide the following:\n\n\n\n\nIn the \nAccount ID\n field, enter your AWS account ID.\n\n\nUnder \nOptions\n, check \nRequire external ID\n.\n\n\nIn the \nExternal ID\n, enter \"provision-ambari\".\n\n\n\n\n\n\n\n\n\n\nWhen done, click \nNext: Permissions\n to navigate to the next page in the wizard.\n\n\n\n\n\n\nClick \nCreate policy\n.\n\n\n\n\n\n\n\n\nClick \nSelect\n next to \"Create Your Own Policy\".\n\n\n\n\n\n\n\n\nIn the \nPolicy Name\n field, enter \"cb-policy\" and in the \nPolicy Document\n paste the policy definition.  You can either copy it from the section preceding these steps or download and copy it from \nhere\n.\n\n\n  \n\n\n\n\n\n\nWhen done, click \nCreate Policy\n.\n\n\n\n\n\n\nClick \nRefresh\n. Next, find the \"cb-policy\" that you just created and select it by checking the box.\n\n\n\n\n\n\n\n\nWhen done, click \nNext: Review\n.\n\n\n\n\n\n\nIn the \nRoles name\n field, enter role name, for example \"CredentialRole\".\n\n\n\n\n\n\n\n\nWhen done, click \nCreate role\n to finish the role creation process.\n\n\n\n\n\n\nOnce you are done, you can proceed to launch Cloudbreak.  \n\n\nRelated links\n\n\nCredentialRole\n  \n\n\n\n\nNext: Launch Cloudbreak", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/aws-pre/index.html#prerequisites-on-aws", 
            "text": "Before launching Cloudbreak on AWS, you must meet the following prerequisites.", 
            "title": "Prerequisites on AWS"
        }, 
        {
            "location": "/aws-pre/index.html#aws-account", 
            "text": "In order to launch Cloudbreak on AWS, you must log in to your AWS account. If you don't have an account, you can create one at  https://aws.amazon.com/ .", 
            "title": "AWS account"
        }, 
        {
            "location": "/aws-pre/index.html#aws-region", 
            "text": "Decide in which AWS region you would like to launch Cloudbreak. The following AWS regions are supported:     Region name  Region      EU (Ireland)  eu-west-1    EU (Frankfurt)  eu-central-1    US East (N. Virginia)  us-east-1    US West (N. California)  us-west-1    US West (Oregon)  us-west-2    South America (S\u00e3o Paulo)  sa-east-1    Asia Pacific (Tokyo)  ap-northeast-1    Asia Pacific (Singapore)  ap-southeast-1    Asia Pacific (Sydney)  ap-southeast-2     Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.  Related links  AWS regions and endpoints  (External)", 
            "title": "AWS region"
        }, 
        {
            "location": "/aws-pre/index.html#ssh-key-pair", 
            "text": "Import an existing key pair or generate a new key pair in the AWS region which you are planning to use for launching Cloudbreak and clusters. You can do this using the following steps.  Steps   Navigate to the Amazon EC2 console at https://console.aws.amazon.com/ec2/.    Check the region listed in the top right corner to make sure that you are in the correct region.    In the left pane, find  NETWORK AND SECURITY  and click  Key Pairs .     Do one of the following:  Click  Create Key Pair  to create a new key pair. Your private key file will be automatically downloaded onto your computer. Make sure to save it in a secure location. You will need it to SSH to the cluster nodes. You may want to change access settings for the file using  chmod 400 my-key-pair.pem .    Click  Import Key Pair  to upload an existing public key and then select it and click  Import . Make sure that you have access to its corresponding private key.         You need this SSH key pair to SSH to the Cloudbreak instance and start Cloudbreak.  Related links  Creating a key pair using Amazon EC2  (External)", 
            "title": "SSH key pair"
        }, 
        {
            "location": "/aws-pre/index.html#virtual-network", 
            "text": "You must have a virtual network configured on your cloud provider.", 
            "title": "Virtual network"
        }, 
        {
            "location": "/aws-pre/index.html#security-group", 
            "text": "Ports 22 (SSH), 80 (HTTPS), and 443 (HTTPS) must be open on the security group", 
            "title": "Security group"
        }, 
        {
            "location": "/aws-pre/index.html#authentication", 
            "text": "Before you can start using Cloudbreak for provisioning clusters, you must select a way for Cloudbreak to authenticate with your AWS account and create resources on your behalf. There are two ways to do this:    Key-based : This is a simpler option which does not require additional configuration at this point. It requires that you provide your AWS access key and secret key pair in the Cloudbreak web UI later. All you need to do now is check your AWS account and ensure that you can access this key pair.    Role-based : This requires that you or your AWS admin create an IAM role to allow Cloudbreak to assume AWS roles (the \"AssumeRole\" policy).", 
            "title": "Authentication"
        }, 
        {
            "location": "/aws-pre/index.html#option-1-use-key-based-authentication", 
            "text": "If you are using key-based authentication for Cloudbreak on AWS, you must be able to provide your AWS access key and secret key pair. Cloudbreak will use these keys to launch the resources. You must provide the access and secret keys later in the Cloudbreak web UI later when creating a credential.  If you choose this option, all you need to do at this point is check your AWS account and make sure that you can access this key pair. You can generate new access and secret keys from the  IAM Console     Users . Next, select a user and click on the  Security credentials  tab:   The minimum set of permissions required by Cloudbreak are the same as for the  CredentialRole .   If you choose this option, you can proceed to launch Cloudbreak.", 
            "title": "(Option 1) Use key-based authentication"
        }, 
        {
            "location": "/aws-pre/index.html#option-2-configure-role-based-authentication", 
            "text": "If you are using role-based authentication for Cloudbreak on AWS, you must create two IAM roles: one to grant Cloudbreak access to allow Cloudbreak to assume AWS roles (using the \"AssumeRole\" policy) and the second one to provide Cloudbreak with the capabilities required for cluster creation (using the \"cb-policy\" policy).  The following table provides contextual information about the two roles required:      Role  Purpose  Overview of steps  Configuration      CloudbreakRole  Allows Cloudbreak to assume other IAM roles - specifically the CredentialRole.  Create a role called \"CloudbreakRole\" and attach the \"AssumeRole\" policy. The \"AssumeRole\" policy definition and steps for creating the CloudbreakRole are provided below.  When launching Cloudbreak, you will attach the \"CloudbreakRole\" IAM role to the VM. If you are using hosted Cloudbreak, you do not need to perform this step.    CredentialRole  Allows Cloudbreak to create AWS resources required for clusters.  Create a new IAM role called \"CredentialRole\" and attach the \"cb-policy\" policy to it. The \"cb-policy\" policy definition and steps for creating the CredentialRole are provided below.  When creating this role using the AWS Console, make sure that that it is a role for cross-account access and that the trust-relation is set up as follows: 'Account ID' is your own 12-digit AWS account ID and 'External ID' is \u201cprovision-ambari\u201d. See steps below.  Once you log in to the Cloudbreak UI and are ready to create clusters, you will use this role to create the Cloudbreak credential.      These role and policy names are just examples. You may use different names when creating your resources.    CloudbreakRole: Alternatively, instead of attaching the \"CloudbreakRole\" role during the VM launch, you can assign the \"CloudbreakRole\" to an IAM user and then add the access and secret key of that user to your 'Profile'.  CredentialRole: Alternatively you can generate the \"CredentialRole\" role later once your Cloudbreak VM is running by SSHing to the Cloudbreak VM and running the  cbd aws generate-role  command. This command creates a role with the name \"cbreak-deployer\" (equivalent to the \"CredentialRole\"). To customize the name of the role, add  export AWS_ROLE_NAME=my-cloudbreak-role-name  (where \"my-cloudbreak-role-name\" is your custom role name) as a new line to your Profile. If you choose this option, you must make sure that the \"CloudbreakRole\" or the IAM user have a permission not only to assume a role but also to create a role.     You can create these roles in the  IAM console , on the  Roles  page via the  Create Role  option. Detailed steps are provided below.  Related links  Using instance profiles  (External)  Using an IAM role to grant permissions to applications  (External)", 
            "title": "(Option 2) Configure role-based authentication"
        }, 
        {
            "location": "/aws-pre/index.html#create-cloudbreakrole", 
            "text": "Use these steps to create CloudbreakRole.  Use the following \"AssumeRole\" policy definition:  {\n  \"Version\": \"2012-10-17\",\n  \"Statement\": {\n    \"Sid\": \"Stmt1400068149000\",\n    \"Effect\": \"Allow\",\n    \"Action\": [\"sts:AssumeRole\"],\n    \"Resource\": \"*\"\n  }\n}  Steps    Navigate to the  IAM console     Roles  and click  Create Role .     In the \"Create Role\" wizard, select  AWS service  role type and then select any service.     When done, click  Next: Permissions  to navigate to the next page in the wizard.    Click  Create policy .     Click  Select  next to \"Create Your Own Policy\".        In the  Policy Name  field, enter \"AssumeRole\" and in the  Policy Document  paste the policy definition. You can either copy it from the section preceding these steps or download and copy it from  here .        When done, click  Create Policy .    Click  Refresh . Next, find the \"AssumeRole\" policy that you just created and select it by checking the box.     When done, click  Next: Review .    In the  Roles name  field, enter role name, for example \"CloudbreakRole\".     When done, click  Create role  to finish the role creation process.    Related links  CloudbreakRole", 
            "title": "Create CloudbreakRole"
        }, 
        {
            "location": "/aws-pre/index.html#create-credentialrole", 
            "text": "Use these steps to create CredentialRole.  Use the following \"cb-policy\" policy definition:  {\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"cloudformation:CreateStack\",\n        \"cloudformation:DeleteStack\",\n        \"cloudformation:DescribeStackEvents\",\n        \"cloudformation:DescribeStackResource\",\n        \"cloudformation:DescribeStacks\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:AllocateAddress\",\n        \"ec2:AssociateAddress\",\n        \"ec2:AssociateRouteTable\",\n        \"ec2:AuthorizeSecurityGroupIngress\",\n        \"ec2:DescribeRegions\",\n        \"ec2:DescribeAvailabilityZones\",\n        \"ec2:CreateRoute\",\n        \"ec2:CreateRouteTable\",\n        \"ec2:CreateSecurityGroup\",\n        \"ec2:CreateSubnet\",\n        \"ec2:CreateTags\",\n        \"ec2:CreateVpc\",\n        \"ec2:ModifyVpcAttribute\",\n        \"ec2:DeleteSubnet\",\n        \"ec2:CreateInternetGateway\",\n        \"ec2:CreateKeyPair\",\n        \"ec2:DeleteKeyPair\",\n        \"ec2:DisassociateAddress\",\n        \"ec2:DisassociateRouteTable\",\n        \"ec2:ModifySubnetAttribute\",\n        \"ec2:ReleaseAddress\",\n        \"ec2:DescribeAddresses\",\n        \"ec2:DescribeImages\",\n        \"ec2:DescribeInstanceStatus\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeInternetGateways\",\n        \"ec2:DescribeKeyPairs\",\n        \"ec2:DescribeRouteTables\",\n        \"ec2:DescribeSecurityGroups\",\n        \"ec2:DescribeSubnets\",\n        \"ec2:DescribeVpcs\",\n        \"ec2:DescribeSpotInstanceRequests\",\n        \"ec2:DescribeVpcAttribute\",\n        \"ec2:ImportKeyPair\",\n        \"ec2:AttachInternetGateway\",\n        \"ec2:DeleteVpc\",\n        \"ec2:DeleteSecurityGroup\",\n        \"ec2:DeleteRouteTable\",\n        \"ec2:DeleteInternetGateway\",\n        \"ec2:DeleteRouteTable\",\n        \"ec2:DeleteRoute\",\n        \"ec2:DetachInternetGateway\",\n        \"ec2:RunInstances\",\n        \"ec2:StartInstances\",\n        \"ec2:StopInstances\",\n        \"ec2:TerminateInstances\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"iam:ListRolePolicies\",\n        \"iam:GetRolePolicy\",\n        \"iam:ListAttachedRolePolicies\",\n        \"iam:ListInstanceProfiles\",\n        \"iam:PutRolePolicy\",\n        \"iam:PassRole\",\n        \"iam:GetRole\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"autoscaling:CreateAutoScalingGroup\",\n        \"autoscaling:CreateLaunchConfiguration\",\n        \"autoscaling:DeleteAutoScalingGroup\",\n        \"autoscaling:DeleteLaunchConfiguration\",\n        \"autoscaling:DescribeAutoScalingGroups\",\n        \"autoscaling:DescribeLaunchConfigurations\",\n        \"autoscaling:DescribeScalingActivities\",\n        \"autoscaling:DetachInstances\",\n        \"autoscaling:ResumeProcesses\",\n        \"autoscaling:SuspendProcesses\",\n        \"autoscaling:UpdateAutoScalingGroup\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}  Steps    Navigate to the  IAM console     Roles  and click  Create Role .     In the \"Create Role\" wizard, select  Another AWS account  role type. Next, provide the following:   In the  Account ID  field, enter your AWS account ID.  Under  Options , check  Require external ID .  In the  External ID , enter \"provision-ambari\".      When done, click  Next: Permissions  to navigate to the next page in the wizard.    Click  Create policy .     Click  Select  next to \"Create Your Own Policy\".     In the  Policy Name  field, enter \"cb-policy\" and in the  Policy Document  paste the policy definition.  You can either copy it from the section preceding these steps or download and copy it from  here .        When done, click  Create Policy .    Click  Refresh . Next, find the \"cb-policy\" that you just created and select it by checking the box.     When done, click  Next: Review .    In the  Roles name  field, enter role name, for example \"CredentialRole\".     When done, click  Create role  to finish the role creation process.    Once you are done, you can proceed to launch Cloudbreak.    Related links  CredentialRole      Next: Launch Cloudbreak", 
            "title": "Create CredentialRole"
        }, 
        {
            "location": "/aws-launch/index.html", 
            "text": "Launching Cloudbreak on AWS\n\n\nThese steps describe how to launch Cloudbreak on AWS for production. \nBefore launching Cloudbreak on AWS, review and meet the AWS-specific \nprerequisites\n. Next, follow the steps below.  \n\n\nVM requirements\n\n\nTo launch the Cloudbreak deployer and install the Cloudbreak application, you must have an existing VM. \n\n\nSystem requirements\n\n\nYour system must meet the following requirements:\n\n\n\n\nMinimum VM requirements: 16GB RAM, 40GB disk, 4 cores\n\n\nSupported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)\n\n\n\n\n\n\nYou can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.\n\n\n\n\nRoot access\n\n\nEvery command must be executed as root. In order to get root privileges execute:\n\n\nsudo -i\n\n\n\nSystem updates\n\n\nEnsure that your system is up-to-date by executing:\n\n\nyum -y update\n\n\n\nReboot it if necessary.\n\n\nInstall iptables\n\n\nPerform these steps to install and configure iptables.\n\n\nSteps\n \n\n\n\n\n\n\nInstall iptables-services:\n\n\nyum -y install net-tools ntp wget lsof unzip tar iptables-services\nsystemctl enable ntpd \n systemctl start ntpd\nsystemctl disable firewalld \n systemctl stop firewalld\n\n\n\n\nWithout iptables-services installed the \niptables save\n command will not be available.\n\n\n\n\n\n\n\n\nConfigure permissive iptables on your machine:\n\n\niptables --flush INPUT \n \\\niptables --flush FORWARD \n \\\nservice iptables save\n\n\n\n\n\n\nDisable SELINUX\n\n\nPerform these steps to disable SELINUX.\n\n\nSteps\n \n\n\n\n\n\n\nDisable SELINUX:\n\n\nsetenforce 0\nsed -i 's/SELINUX=enforcing/SELINUX=disabled/g' \n/etc/selinux/config\n\n\n\n\n\n\nRun the following command to ensure that SELinux is not turned on afterwards: \n\n\ngetenforce\n\n\n\n\n\n\nThe command should return \"Disabled\".     \n\n\n\n\n\n\nInstall Docker\n\n\nPerform these steps to install Docker. The minimum Docker version is 1.13.1. If you are using an older image that comes with an older Docker version, upgrade Docker to 1.13.1 or newer. \n\n\nSteps\n    \n\n\n\n\n\n\nInstall Docker service:\n\n\nyum install -y docker\nsystemctl start docker\nsystemctl enable docker\n\n\n\n\n\n\nCheck the Docker Logging Driver configuration:\n\n\ndocker info | grep \"Logging Driver\"\n\n\n\n\n\n\nIf it is set to \nLogging Driver: journald\n, you must  set it to \"json-file\" instead. To do that:\n\n\n\n\n\n\nOpen the \ndocker\n file for editing:\n\n\nvi /etc/sysconfig/docker\n  \n\n\n\n\n\n\nEdit the following part of the file so that it looks like below (showing \nlog-driver=json-file\n):\n\n\n# Modify these options if you want to change the way the docker daemon runs\nOPTIONS='--selinux-enabled --log-driver=json-file --signature-verification=false'\n     \n\n\n\n\n\n\nRestart Docker:\n\n\nsystemctl restart docker\nsystemctl status docker\n\n\n\n\n\n\n\n\n\n\nInstall Cloudbreak on a VM\n\n\nInstall Cloudbreak using the following steps.\n\n\nSteps\n\n\n\n\n\n\nInstall the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:\n\n\nyum -y install unzip tar\ncurl -Ls public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_2.7.0_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version\n\n\nOnce the Cloudbreak deployer is installed, you can set up the Cloudbreak application.\n\n\n\n\n\n\nCreate a Cloudbreak deployment directory and navigate to it:\n\n\nmkdir cloudbreak-deployment\ncd cloudbreak-deployment\n\n\n\n\n\n\nIn the directory, create a file called \nProfile\n with the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL\nexport PUBLIC_IP=MY_VM_IP\n\n\nFor example:\n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com\nexport PUBLIC_IP=172.26.231.100\n\n\nYou will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.\n\n\nYou should set the CLOUDBREAK_SMTP_SENDER_USERNAME variable to the username you use to authenticate to your SMTP server. You should set the CLOUDBREAK_SMTP_SENDER_PASSWORD variable to the password you use to authenticate to your SMTP server.\n\n\n\n\n\n\nGenerate configurations by executing:\n\n\nrm *.yml\ncbd generate\n   \n\n\nThe cbd start command includes the cbd generate command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.  \n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.   \n\n\n\n\n\n\n\n\nStart the Cloudbreak application by using the following commands:\n\n\ncbd pull-parallel\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Cloudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\n\n\nIf you encounter errors during \ncbd start\n, refer to \nToubleshooting\n.  \n\n\n\n\n\n\n\n\nNext, check Cloudbreak application logs:\n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nRelated links\n\n\nToubleshooting\n   \n\n\nAccess Cloudbreak web UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n \n\n\n\n\n\n\nYou can log into the Cloudbreak application at \nhttps://IP_Address\n. For example \nhttps://34.212.141.253\n. You may use \ncbd start\n to obtain the login information. Alternatively, you can obtain the VM's IP address from your cloud provider console. \n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nLog in to the Cloudbreak web UI using the credentials that you configured in your \nProfile\n file:\n\n\n\n\nThe username is the \nUAA_DEFAULT_USER_EMAIL\n     \n\n\nThe password is the \nUAA_DEFAULT_USER_PW\n \n\n\n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n  \n\n\n\n\n\n\nConfigure external Cloudbreak database\n\n\nBy default, Cloudbreak uses an embedded PostgreSQL database to persist data related to Cloudbreak configuration, setup, and so on. For a production Cloudbreak deployment, you must \nconfigure an external database\n.\n\n\nRelated links\n\n\nConfigure an external database\n  \n\n\nCreate Cloudbreak credential\n\n\nBefore you can start creating clusters, you must first create a \nCloudbreak credential\n. Without this credential, you will not be able to create clusters via Cloudbreak. \n\n\nAs part of the prerequisites, you had two options to allow Cloudbreak to authenticate with AWS and create resources on your behalf: key-based or role-based \nauthentication\n. Depending on your choice, you must configure a key-based or role-based credential: \n\n\n\n\nCreate key-based credential\n  \n\n\nCreate role-based credential\n\n\n\n\nCreate key-based credential\n\n\nFollow these steps to create a key-based Cloudbreak credential. \n\n\nPrerequisites\n\n\nIn order to use the key-based Cloudbreak credential: \n\n\n\n\n\n\nYou must have an access key and secret key. For information on how to generate it, refer to \nUse key-based authentication\n.  \n\n\n\n\n\n\nYour AWS user must have the minimum permissions described in \nCreate CredentialRole\n as well as the permission to create an IAM role. \n\n\n\n\n\n\nIf you are unable to obtain these permissions for your AWS user, you must use \nrole-based authentication\n instead of key-based authentication. If you would like to review both options, refer to \nAuthentication\n. \n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Amazon Web Services\":\n\n\n  \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nKey Based\n.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nAccess Key\n\n\nPaste your access key.\n\n\n\n\n\n\nSecret Access Key\n\n\nPaste your secret key.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You've successfully launched Cloudbreak and create a Cloudbreak credential. Now you can start creating clusters. \n\n\n\n\n\n\nRelated links\n \n\n\nAuthentication\n\n\nUse key-based authentication\n  \n\n\nUse role-based authentication\n   \n\n\nCreate CredentialRole\n   \n\n\nCreate role-based credential\n\n\nFollow these steps to create a role-based Cloudbreak credential. \n\n\nPrerequisites\n\n\nTo perform these steps, you must know the \nIAM Role ARN\n corresponding to the \"\nCredentialRole\n\" (configured as part of the prerequisites).   \n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Amazon Web Services\":\n\n\n  \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nRole Based\n (default value).\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nIAM Role ARN\n\n\nPaste the IAM Role ARN corresponding to the \"CredentialRole\" that you created earlier. For example \narn:aws:iam::315627065446:role/CredentialRole\n is a valid IAM Role ARN.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to create clusters. \n\n\n\n\n\n\nRelated links\n\n\nCreate CredentialRole\n        \n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on AWS"
        }, 
        {
            "location": "/aws-launch/index.html#launching-cloudbreak-on-aws", 
            "text": "These steps describe how to launch Cloudbreak on AWS for production. \nBefore launching Cloudbreak on AWS, review and meet the AWS-specific  prerequisites . Next, follow the steps below.", 
            "title": "Launching Cloudbreak on AWS"
        }, 
        {
            "location": "/aws-launch/index.html#vm-requirements", 
            "text": "To launch the Cloudbreak deployer and install the Cloudbreak application, you must have an existing VM.", 
            "title": "VM requirements"
        }, 
        {
            "location": "/aws-launch/index.html#system-requirements", 
            "text": "Your system must meet the following requirements:   Minimum VM requirements: 16GB RAM, 40GB disk, 4 cores  Supported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)    You can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.", 
            "title": "System requirements"
        }, 
        {
            "location": "/aws-launch/index.html#root-access", 
            "text": "Every command must be executed as root. In order to get root privileges execute:  sudo -i", 
            "title": "Root access"
        }, 
        {
            "location": "/aws-launch/index.html#system-updates", 
            "text": "Ensure that your system is up-to-date by executing:  yum -y update  Reboot it if necessary.", 
            "title": "System updates"
        }, 
        {
            "location": "/aws-launch/index.html#install-iptables", 
            "text": "Perform these steps to install and configure iptables.  Steps      Install iptables-services:  yum -y install net-tools ntp wget lsof unzip tar iptables-services\nsystemctl enable ntpd   systemctl start ntpd\nsystemctl disable firewalld   systemctl stop firewalld   Without iptables-services installed the  iptables save  command will not be available.     Configure permissive iptables on your machine:  iptables --flush INPUT   \\\niptables --flush FORWARD   \\\nservice iptables save", 
            "title": "Install iptables"
        }, 
        {
            "location": "/aws-launch/index.html#disable-selinux", 
            "text": "Perform these steps to disable SELINUX.  Steps      Disable SELINUX:  setenforce 0\nsed -i 's/SELINUX=enforcing/SELINUX=disabled/g' \n/etc/selinux/config    Run the following command to ensure that SELinux is not turned on afterwards:   getenforce    The command should return \"Disabled\".", 
            "title": "Disable SELINUX"
        }, 
        {
            "location": "/aws-launch/index.html#install-docker", 
            "text": "Perform these steps to install Docker. The minimum Docker version is 1.13.1. If you are using an older image that comes with an older Docker version, upgrade Docker to 1.13.1 or newer.   Steps         Install Docker service:  yum install -y docker\nsystemctl start docker\nsystemctl enable docker    Check the Docker Logging Driver configuration:  docker info | grep \"Logging Driver\"    If it is set to  Logging Driver: journald , you must  set it to \"json-file\" instead. To do that:    Open the  docker  file for editing:  vi /etc/sysconfig/docker       Edit the following part of the file so that it looks like below (showing  log-driver=json-file ):  # Modify these options if you want to change the way the docker daemon runs\nOPTIONS='--selinux-enabled --log-driver=json-file --signature-verification=false'          Restart Docker:  systemctl restart docker\nsystemctl status docker", 
            "title": "Install Docker"
        }, 
        {
            "location": "/aws-launch/index.html#install-cloudbreak-on-a-vm", 
            "text": "Install Cloudbreak using the following steps.  Steps    Install the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:  yum -y install unzip tar\ncurl -Ls public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_2.7.0_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version  Once the Cloudbreak deployer is installed, you can set up the Cloudbreak application.    Create a Cloudbreak deployment directory and navigate to it:  mkdir cloudbreak-deployment\ncd cloudbreak-deployment    In the directory, create a file called  Profile  with the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL\nexport PUBLIC_IP=MY_VM_IP  For example:  export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com\nexport PUBLIC_IP=172.26.231.100  You will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.  You should set the CLOUDBREAK_SMTP_SENDER_USERNAME variable to the username you use to authenticate to your SMTP server. You should set the CLOUDBREAK_SMTP_SENDER_PASSWORD variable to the password you use to authenticate to your SMTP server.    Generate configurations by executing:  rm *.yml\ncbd generate      The cbd start command includes the cbd generate command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.    Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.        Start the Cloudbreak application by using the following commands:  cbd pull-parallel\ncbd start  This will start the Docker containers and initialize the application. The first time you start the Cloudbreak app, the process will take longer than usual due to the download of all the necessary docker images.   If you encounter errors during  cbd start , refer to  Toubleshooting .       Next, check Cloudbreak application logs:  cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.    Related links  Toubleshooting", 
            "title": "Install Cloudbreak on a VM"
        }, 
        {
            "location": "/aws-launch/index.html#access-cloudbreak-web-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps      You can log into the Cloudbreak application at  https://IP_Address . For example  https://34.212.141.253 . You may use  cbd start  to obtain the login information. Alternatively, you can obtain the VM's IP address from your cloud provider console.     Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...       The login page is displayed:        Log in to the Cloudbreak web UI using the credentials that you configured in your  Profile  file:   The username is the  UAA_DEFAULT_USER_EMAIL        The password is the  UAA_DEFAULT_USER_PW       Upon a successful login, you are redirected to the dashboard:", 
            "title": "Access Cloudbreak web UI"
        }, 
        {
            "location": "/aws-launch/index.html#configure-external-cloudbreak-database", 
            "text": "By default, Cloudbreak uses an embedded PostgreSQL database to persist data related to Cloudbreak configuration, setup, and so on. For a production Cloudbreak deployment, you must  configure an external database .  Related links  Configure an external database", 
            "title": "Configure external Cloudbreak database"
        }, 
        {
            "location": "/aws-launch/index.html#create-cloudbreak-credential", 
            "text": "Before you can start creating clusters, you must first create a  Cloudbreak credential . Without this credential, you will not be able to create clusters via Cloudbreak.   As part of the prerequisites, you had two options to allow Cloudbreak to authenticate with AWS and create resources on your behalf: key-based or role-based  authentication . Depending on your choice, you must configure a key-based or role-based credential:    Create key-based credential     Create role-based credential", 
            "title": "Create Cloudbreak credential"
        }, 
        {
            "location": "/aws-launch/index.html#create-key-based-credential", 
            "text": "Follow these steps to create a key-based Cloudbreak credential.   Prerequisites  In order to use the key-based Cloudbreak credential:     You must have an access key and secret key. For information on how to generate it, refer to  Use key-based authentication .      Your AWS user must have the minimum permissions described in  Create CredentialRole  as well as the permission to create an IAM role.     If you are unable to obtain these permissions for your AWS user, you must use  role-based authentication  instead of key-based authentication. If you would like to review both options, refer to  Authentication .   Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Amazon Web Services\":        Provide the following information:     Parameter  Description      Select Credential Type  Select  Key Based .    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Access Key  Paste your access key.    Secret Access Key  Paste your secret key.       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You've successfully launched Cloudbreak and create a Cloudbreak credential. Now you can start creating clusters.     Related links    Authentication  Use key-based authentication     Use role-based authentication      Create CredentialRole", 
            "title": "Create key-based credential"
        }, 
        {
            "location": "/aws-launch/index.html#create-role-based-credential", 
            "text": "Follow these steps to create a role-based Cloudbreak credential.   Prerequisites  To perform these steps, you must know the  IAM Role ARN  corresponding to the \" CredentialRole \" (configured as part of the prerequisites).     Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Amazon Web Services\":        Provide the following information:     Parameter  Description      Select Credential Type  Select  Role Based  (default value).    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    IAM Role ARN  Paste the IAM Role ARN corresponding to the \"CredentialRole\" that you created earlier. For example  arn:aws:iam::315627065446:role/CredentialRole  is a valid IAM Role ARN.       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to create clusters.     Related links  Create CredentialRole            Next: Create a Cluster", 
            "title": "Create role-based credential"
        }, 
        {
            "location": "/aws-create/index.html", 
            "text": "Creating a cluster on AWS\n\n\nUse these steps to create a cluster.\n\n\n\n\nTroubleshooting cluster creation\n\n\nIf you experience problems during cluster creation, refer to \nTroubleshooting cluster creation\n.\n\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick the \nCreate Cluster\n button and the \nCreate Cluster\n wizard is displayed.\n\n    By default, \nBasic\n view is displayed. To view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced cluster options\n.\n\n\n \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, specify the following general parameters for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nChoose a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nRegion\n\n\nSelect the AWS region in which you would like to launch your cluster. For information on available AWS regions, refer to \nAWS regions and endpoints\n in AWS documentation.\n\n\n\n\n\n\nPlatform Version\n\n\nChoose the HDP or HDF version to use for this cluster. Blueprints available for this platform version will be populated under \"Cluster Type\" below. If you selected the \nHDF\n platform, refer to \nCreating HDF clusters\n for HDF cluster configuration tips.\n\n\n\n\n\n\nCluster Type\n\n\nChoose one of the default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to \nUsing custom blueprints\n.\n\n\n\n\n\n\nFlex Subscription\n\n\nThis option will appear if you have configured your deployment for a \nflex support subscription\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, for each host group provide the following information to define your cluster nodes and attached storage. \n\n\nTo edit this section, click on the \n. When done editing, click on the \n to save the changes.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server by clicking the \n button. The \"Instance Count\" for that host group must be set to \"1\". If you are using one of the default blueprints, this is set by default.\n\n\n\n\n\n\nInstance Type\n\n\nSelect an instance type. For information about instance types on AWS refer to \nAmazon EC2 instance types\n in AWS documentation.\n\n\n\n\n\n\nInstance Count\n\n\nEnter the number of instances of a given type. Default is 1.\n\n\n\n\n\n\nStorage Type\n\n\nSelect the volume type. The options are:\nMagnetic (default)\nGeneral Purpose (SSD)\nThroughput Optimized HDD\nFor more information about these options refer to \nAWS documentation\n.\n\n\n\n\n\n\nAttached Volumes Per Instance\n\n\nEnter the number of volumes attached per instance. Default is 1.\n\n\n\n\n\n\nVolume Size\n\n\nEnter the size in GBs for each volume. Default is 100.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nGateway Configuration\n page, you can access gateway configuration options. \n\n\nWhen creating a cluster, Cloudbreak installs and configures a gateway (powered by Apache Knox) to protect access to the cluster resources. By default, the gateway is enabled for Ambari; You can optionally enable it for other cluster services. \n\n\nFor more information, refer to \nConfiguring the Gateway\n documentation.\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following to specify the networking resources that will be used for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Network\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.\n\n\n\n\n\n\nSelect Subnet\n\n\nSelect the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.\n\n\n\n\n\n\nSubnet (CIDR)\n\n\nIf you selected to create a new subnet, you must define a valid \nCIDR\n for the subnet. Default is 10.0.0.0/16.\n\n\n\n\n\n\n\n\n\n\nCloudbreak uses public IP addresses when communicating with cluster nodes.\n\nOn AWS, you can configure it to use private IPs instead. For instructions, refer to \nConfigure communication via private IPs on AWS\n. \n\n\n\n\n\n\n\n\nDefine security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:\n\n\n\n\nExisting security groups are only available for an existing VPC. \n\n\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNew Security Group\n\n\n(Default) Creates a new security group with the rules that you defined:\nA set of \ndefault rules\n is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied. \nYou may open ports by defining the CIDR, entering port range, selecting protocol and clicking \n+\n.\nYou may delete default or previously added rules using the delete icon.\nIf you don't want to use security group, remove the default rules.\n\n\n\n\n\n\nExisting Security Groups\n\n\nAllows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.\n\n\n\n\n\n\n\n\nThe default experience of creating network resources such as network, subnet and security group automatically is provided for convenience. We strongly recommend you review these options and for production cluster deployments leverage your existing network resources that you have defined and validated to meet your enterprise requirements. For more information, refer to \nRestricting inbound access from Cloudbreak to cluster\n. \n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nPassword\n\n\nYou can log in to the Ambari UI using this password.\n\n\n\n\n\n\nConfirm Password\n\n\nConfirm the password.\n\n\n\n\n\n\nNew SSH public key\n\n\nCheck this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.\n\n\n\n\n\n\nExisting SSH public key\n\n\nSelect an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nRelated links\n \n\n\nAdvanced cluster options\n\n\nConfigure communication via private IPs on AWS\n\n\nConfiguring the Gateway\n \n\n\nDefault cluster security groups\n\n\nFlex support subscription\n \n\n\nRestricting inbound access from Cloudbreak to cluster\n\n\nTroubleshooting cluster creation\n  \n\n\nUsing custom blueprints\n\n\nAmazon EC2 instance types\n (External) \n\n\nAWS regions and endpoints\n (External)   \n\n\nCIDR\n (External)   \n\n\nAdvanced cluster options\n\n\nClick on \nAdvanced\n to view and enter additional configuration options\n\n\nAvailability zone\n\n\nChoose one of the availability zones within the selected region. \n\n\nEnable lifetime management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes). \n\n\nTags\n\n\nYou can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. \n\n\nBy default, the following tags are created:\n\n\n\n\n\n\n\n\nTag\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncb-version\n\n\nCloudbreak version\n\n\n\n\n\n\nOwner\n\n\nYour Cloudbreak admin email.\n\n\n\n\n\n\ncb-account-name\n\n\nYour automatically generated Cloudbreak account name stored in the identity server.\n\n\n\n\n\n\ncb-user-name\n\n\nYour Cloudbreak admin email.\n\n\n\n\n\n\n\n\nFor more information, refer to \nTagging resources\n.\n\n\nRelated links\n    \n\n\nTagging resources\n \n\n\nChoose image catalog\n\n\nBy default, \nChoose Image Catalog\n is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to \nUsing custom images\n.\n\n\nRelated links\n   \n\n\nUsing custom images\n  \n\n\nChoose image type\n\n\nCloudbreak supports the following types of images for launching clusters:\n\n\n\n\n\n\n\n\nImage type\n\n\nDescription\n\n\nDefault images provided\n\n\nSupport for custom images\n\n\n\n\n\n\n\n\n\n\nPrewarmed Image\n\n\nBy default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP/HDF. The Ambari and HDP/HDF version used by prewarmed images cannot be customized.\n\n\nYes\n\n\nNo\n\n\n\n\n\n\nBase Image\n\n\nBase images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP/HDF software.\n\n\nYes\n\n\nYes\n\n\n\n\n\n\n\n\nBy default, Cloudbreak uses the included default \nprewarmed images\n, which include the operating system, as well as\nAmbari and HDP/HDF packages installed. You can optionally select the \nbase image\n option if you would like to:\n\n\n\n\nUse an Ambari and HDP/HDF versions different than what the prewarmed image includes and/or  \n\n\nChoose a previously created custom base image\n\n\n\n\nChoose image\n\n\nIf under \nChoose image catalog\n, you selected a custom image catalog, under \nChoose Image\n you can select an image from that catalog. For complete instructions, refer to \nUsing custom images\n. \n\n\nIf you are trying to customize Ambari and HDP/HDF versions, you can ignore the \nChoose Image\n option; in this case default base image is used.\n\n\nAmbari repo specification\n\n\nIf you would like to use a custom Ambari version, provide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nVersion\n\n\nEnter Ambari version.\n\n\n2.6.1.3\n\n\n\n\n\n\nRepo Url\n\n\nProvide a URL to the Ambari version repo that you would like to use.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.3\n\n\n\n\n\n\nRepo Gpg Key Url\n\n\nProvide a URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins\n\n\n\n\n\n\n\n\nHDP/HDF repo specification\n\n\nIf you would like to use a custom HDP or HDF version, provide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nThis is populated by default based on the \"Platform Version\" parameter.\n\n\nHDP\n\n\n\n\n\n\nVersion\n\n\nThis is populated by default based on the \"Platform Version\" parameter.\n\n\n2.6\n\n\n\n\n\n\nOS\n\n\nOperating system.\n\n\ncentos7 (Azure, GCP, OpenStack) or amazonlinux (AWS)\n\n\n\n\n\n\nRepository Version\n\n\nEnter repository version.\n\n\n2.6.4.0-91\n\n\n\n\n\n\nVersion Definition File\n\n\nEnter the URL of the VDF file.\n\n\nhttp://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.6.4.0/HDP-2.6.4.0-91.xml\n\n\n\n\n\n\n(HDF only) MPack Url\n\n\n(HDF only) Provide MPack URL.\n\n\nhttp://public-repo-1.hortonworks.com/HDF/centos7/3.x/updates/3.1.1.0/tars/hdf_ambari_mp/hdf-ambari-mpack-3.1.1.0-35.tar.gz\n\n\n\n\n\n\nEnable Ambari Server to download and install GPL Licensed LZO packages?\n\n\n(Optional, only available if using Ambari 2.6.1.0 or newer) Use this option to enable LZO compression in your HDP/HDF cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to \nEnabling LZO\n.\n\n\n\n\n\n\n\n\n\n\nIf you choose to use a base image with custom Ambari and/or HDP/HDF version, Cloudbreak validates the information entered. When Cloudbreak detects that the information entered is incorrect, it displays a warning marked with the \n sign. You should review all the warnings before proceeding and make sure that the information that you entered is correct. If you choose to proceed in spite of the warnings, check \"Ignore repository warnings\".  \n\n\nRelated links\n    \n\n\nUsing custom images\n      \n\n\nRoot volume size\n\n\nUse this option to increase the root volume size. Default is 50 GB. This option is useful if your custom image requires more space than the default 50 GB. \n\n\nUse spot instances\n\n\nCheck this option to use EC2 spot instances as your cluster nodes. Next, enter your bid price. The price that is pre-loaded in the form is the current on-demand price for your chosen EC2 instance type.   \n\n\nNote that: \n\n\n\n\nWe recommend not using spot instances for any host group that includes Ambari server components.  \n\n\nIf you choose to use spot instances for a given host group when creating your cluster, any nodes that you add to that host group (during cluster creation or later) will be using spot instances. Any additional nodes will be requested at the same bid price that you entered when creating a cluster.  \n\n\nIf you decide not to use spot instances when creating your cluster, any nodes that you add to your host group (during cluster creation or later) will be using standard on-demand instances.     \n\n\nOnce someone outbids you, the spot instances are taken away, removing the nodes from the cluster. \n\n\nIf spot instances are not available right away, creating a cluster will take longer than usual. \n\n\n\n\nAfter creating a cluster, you can view your spot instance requests, including bid price, on the EC2 dashboard under \nINSTANCES\n \n \nSpot Requests\n. For more information about spot instances, refer to \nAWS documentation\n.  \n\n\nCloud storage\n\n\nIf you would like to access S3 from your cluster, you must configure access as described in \nAccessing data on S3\n. \n\n\nRelated links\n\n\nAccessing data on S3\n    \n\n\nRecipes\n\n\nThis option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to \nUsing custom scripts (recipes)\n. \n\n\nRelated links\n    \n\n\nUsing custom scripts (recipes)\n \n\n\nManagement packs\n\n\nThis option allows you to select previously uploaded management packs. For more information on management packs, refer to \nUsing management packs\n. \n\n\nRelated links\n    \n\n\nUsing management packs\n  \n\n\nExternal sources\n\n\nYou can register external sources with Cloudbreak, and then select and attach them during cluster create. To register external sources with Cloudbreak, refer to:\n\n\n\n\nUsing an external authentication source\n    \n\n\nUsing an external database\n  \n\n\nRegister a proxy\n  \n\n\n\n\nCustom properties\n\n\nThis option allows you to set custom properties based on the template defined in your custom blueprint. For more information, refer to \nSet custom properties\n. \n\n\nRelated links\n    \n\n\nSet custom properties\n   \n\n\nSingle sign-on (SSO)\n\n\nThis option allows you to configure the gateway to be the SSO identity provider. \n\n\n\n\nThis option is technical preview. \n\n\n\n\nFor more information, refer to \nConfiguring the Gateway\n documentation. \n\n\nAmbari server master key\n\n\nThe Ambari server master key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.  \n\n\nEnable Kerberos security\n\n\nSelect this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to \nEnabling Kerberos security\n. \n\n\nRelated links\n    \n\n\nEnabling Kerberos security\n \n\n\nAmazon EC2 instance store\n (External)\n\n\nAmazon EC2 spot instances\n (External)   \n\n\n\n\nNext: Configure Access to S3", 
            "title": "Create a cluster"
        }, 
        {
            "location": "/aws-create/index.html#creating-a-cluster-on-aws", 
            "text": "Use these steps to create a cluster.   Troubleshooting cluster creation  If you experience problems during cluster creation, refer to  Troubleshooting cluster creation .  Steps    Log in to the Cloudbreak UI.    Click the  Create Cluster  button and the  Create Cluster  wizard is displayed. \n    By default,  Basic  view is displayed. To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced cluster options .       On the  General Configuration  page, specify the following general parameters for your cluster:     Parameter  Description      Select Credential  Choose a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.    Region  Select the AWS region in which you would like to launch your cluster. For information on available AWS regions, refer to  AWS regions and endpoints  in AWS documentation.    Platform Version  Choose the HDP or HDF version to use for this cluster. Blueprints available for this platform version will be populated under \"Cluster Type\" below. If you selected the  HDF  platform, refer to  Creating HDF clusters  for HDF cluster configuration tips.    Cluster Type  Choose one of the default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to  Using custom blueprints .    Flex Subscription  This option will appear if you have configured your deployment for a  flex support subscription .       On the  Hardware and Storage  page, for each host group provide the following information to define your cluster nodes and attached storage.   To edit this section, click on the  . When done editing, click on the   to save the changes.     Parameter  Description      Ambari Server  You must select one node for Ambari Server by clicking the   button. The \"Instance Count\" for that host group must be set to \"1\". If you are using one of the default blueprints, this is set by default.    Instance Type  Select an instance type. For information about instance types on AWS refer to  Amazon EC2 instance types  in AWS documentation.    Instance Count  Enter the number of instances of a given type. Default is 1.    Storage Type  Select the volume type. The options are: Magnetic (default) General Purpose (SSD) Throughput Optimized HDD For more information about these options refer to  AWS documentation .    Attached Volumes Per Instance  Enter the number of volumes attached per instance. Default is 1.    Volume Size  Enter the size in GBs for each volume. Default is 100.       On the  Gateway Configuration  page, you can access gateway configuration options.   When creating a cluster, Cloudbreak installs and configures a gateway (powered by Apache Knox) to protect access to the cluster resources. By default, the gateway is enabled for Ambari; You can optionally enable it for other cluster services.   For more information, refer to  Configuring the Gateway  documentation.    On the  Network  page, provide the following to specify the networking resources that will be used for your cluster:     Parameter  Description      Select Network  Select the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.    Select Subnet  Select the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.    Subnet (CIDR)  If you selected to create a new subnet, you must define a valid  CIDR  for the subnet. Default is 10.0.0.0/16.      Cloudbreak uses public IP addresses when communicating with cluster nodes. \nOn AWS, you can configure it to use private IPs instead. For instructions, refer to  Configure communication via private IPs on AWS .      Define security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:   Existing security groups are only available for an existing VPC.       Option  Description      New Security Group  (Default) Creates a new security group with the rules that you defined: A set of  default rules  is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied.  You may open ports by defining the CIDR, entering port range, selecting protocol and clicking  + . You may delete default or previously added rules using the delete icon. If you don't want to use security group, remove the default rules.    Existing Security Groups  Allows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.     The default experience of creating network resources such as network, subnet and security group automatically is provided for convenience. We strongly recommend you review these options and for production cluster deployments leverage your existing network resources that you have defined and validated to meet your enterprise requirements. For more information, refer to  Restricting inbound access from Cloudbreak to cluster .     On the  Security  page, provide the following parameters:     Parameter  Description      Cluster User  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Password  You can log in to the Ambari UI using this password.    Confirm Password  Confirm the password.    New SSH public key  Check this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.    Existing SSH public key  Select an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.       Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.    Related links    Advanced cluster options  Configure communication via private IPs on AWS  Configuring the Gateway    Default cluster security groups  Flex support subscription    Restricting inbound access from Cloudbreak to cluster  Troubleshooting cluster creation     Using custom blueprints  Amazon EC2 instance types  (External)   AWS regions and endpoints  (External)     CIDR  (External)", 
            "title": "Creating a cluster on AWS"
        }, 
        {
            "location": "/aws-create/index.html#advanced-cluster-options", 
            "text": "Click on  Advanced  to view and enter additional configuration options", 
            "title": "Advanced cluster options"
        }, 
        {
            "location": "/aws-create/index.html#availability-zone", 
            "text": "Choose one of the availability zones within the selected region.", 
            "title": "Availability zone"
        }, 
        {
            "location": "/aws-create/index.html#enable-lifetime-management", 
            "text": "Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes).", 
            "title": "Enable lifetime management"
        }, 
        {
            "location": "/aws-create/index.html#tags", 
            "text": "You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.   By default, the following tags are created:     Tag  Description      cb-version  Cloudbreak version    Owner  Your Cloudbreak admin email.    cb-account-name  Your automatically generated Cloudbreak account name stored in the identity server.    cb-user-name  Your Cloudbreak admin email.     For more information, refer to  Tagging resources .  Related links       Tagging resources", 
            "title": "Tags"
        }, 
        {
            "location": "/aws-create/index.html#choose-image-catalog", 
            "text": "By default,  Choose Image Catalog  is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to  Using custom images .  Related links      Using custom images", 
            "title": "Choose image catalog"
        }, 
        {
            "location": "/aws-create/index.html#choose-image-type", 
            "text": "Cloudbreak supports the following types of images for launching clusters:     Image type  Description  Default images provided  Support for custom images      Prewarmed Image  By default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP/HDF. The Ambari and HDP/HDF version used by prewarmed images cannot be customized.  Yes  No    Base Image  Base images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP/HDF software.  Yes  Yes     By default, Cloudbreak uses the included default  prewarmed images , which include the operating system, as well as\nAmbari and HDP/HDF packages installed. You can optionally select the  base image  option if you would like to:   Use an Ambari and HDP/HDF versions different than what the prewarmed image includes and/or    Choose a previously created custom base image", 
            "title": "Choose image type"
        }, 
        {
            "location": "/aws-create/index.html#choose-image", 
            "text": "If under  Choose image catalog , you selected a custom image catalog, under  Choose Image  you can select an image from that catalog. For complete instructions, refer to  Using custom images .   If you are trying to customize Ambari and HDP/HDF versions, you can ignore the  Choose Image  option; in this case default base image is used.", 
            "title": "Choose image"
        }, 
        {
            "location": "/aws-create/index.html#ambari-repo-specification", 
            "text": "If you would like to use a custom Ambari version, provide the following information:      Parameter  Description  Example      Version  Enter Ambari version.  2.6.1.3    Repo Url  Provide a URL to the Ambari version repo that you would like to use.  http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.3    Repo Gpg Key Url  Provide a URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.  http://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins", 
            "title": "Ambari repo specification"
        }, 
        {
            "location": "/aws-create/index.html#hdphdf-repo-specification", 
            "text": "If you would like to use a custom HDP or HDF version, provide the following information:      Parameter  Description  Example      Stack  This is populated by default based on the \"Platform Version\" parameter.  HDP    Version  This is populated by default based on the \"Platform Version\" parameter.  2.6    OS  Operating system.  centos7 (Azure, GCP, OpenStack) or amazonlinux (AWS)    Repository Version  Enter repository version.  2.6.4.0-91    Version Definition File  Enter the URL of the VDF file.  http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.6.4.0/HDP-2.6.4.0-91.xml    (HDF only) MPack Url  (HDF only) Provide MPack URL.  http://public-repo-1.hortonworks.com/HDF/centos7/3.x/updates/3.1.1.0/tars/hdf_ambari_mp/hdf-ambari-mpack-3.1.1.0-35.tar.gz    Enable Ambari Server to download and install GPL Licensed LZO packages?  (Optional, only available if using Ambari 2.6.1.0 or newer) Use this option to enable LZO compression in your HDP/HDF cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to  Enabling LZO .      If you choose to use a base image with custom Ambari and/or HDP/HDF version, Cloudbreak validates the information entered. When Cloudbreak detects that the information entered is incorrect, it displays a warning marked with the   sign. You should review all the warnings before proceeding and make sure that the information that you entered is correct. If you choose to proceed in spite of the warnings, check \"Ignore repository warnings\".    Related links       Using custom images", 
            "title": "HDP/HDF repo specification"
        }, 
        {
            "location": "/aws-create/index.html#root-volume-size", 
            "text": "Use this option to increase the root volume size. Default is 50 GB. This option is useful if your custom image requires more space than the default 50 GB.", 
            "title": "Root volume size"
        }, 
        {
            "location": "/aws-create/index.html#use-spot-instances", 
            "text": "Check this option to use EC2 spot instances as your cluster nodes. Next, enter your bid price. The price that is pre-loaded in the form is the current on-demand price for your chosen EC2 instance type.     Note that:    We recommend not using spot instances for any host group that includes Ambari server components.    If you choose to use spot instances for a given host group when creating your cluster, any nodes that you add to that host group (during cluster creation or later) will be using spot instances. Any additional nodes will be requested at the same bid price that you entered when creating a cluster.    If you decide not to use spot instances when creating your cluster, any nodes that you add to your host group (during cluster creation or later) will be using standard on-demand instances.       Once someone outbids you, the spot instances are taken away, removing the nodes from the cluster.   If spot instances are not available right away, creating a cluster will take longer than usual.    After creating a cluster, you can view your spot instance requests, including bid price, on the EC2 dashboard under  INSTANCES     Spot Requests . For more information about spot instances, refer to  AWS documentation .", 
            "title": "Use spot instances"
        }, 
        {
            "location": "/aws-create/index.html#cloud-storage", 
            "text": "If you would like to access S3 from your cluster, you must configure access as described in  Accessing data on S3 .   Related links  Accessing data on S3", 
            "title": "Cloud storage"
        }, 
        {
            "location": "/aws-create/index.html#recipes", 
            "text": "This option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to  Using custom scripts (recipes) .   Related links       Using custom scripts (recipes)", 
            "title": "Recipes"
        }, 
        {
            "location": "/aws-create/index.html#management-packs", 
            "text": "This option allows you to select previously uploaded management packs. For more information on management packs, refer to  Using management packs .   Related links       Using management packs", 
            "title": "Management packs"
        }, 
        {
            "location": "/aws-create/index.html#external-sources", 
            "text": "You can register external sources with Cloudbreak, and then select and attach them during cluster create. To register external sources with Cloudbreak, refer to:   Using an external authentication source       Using an external database     Register a proxy", 
            "title": "External sources"
        }, 
        {
            "location": "/aws-create/index.html#custom-properties", 
            "text": "This option allows you to set custom properties based on the template defined in your custom blueprint. For more information, refer to  Set custom properties .   Related links       Set custom properties", 
            "title": "Custom properties"
        }, 
        {
            "location": "/aws-create/index.html#single-sign-on-sso", 
            "text": "This option allows you to configure the gateway to be the SSO identity provider.    This option is technical preview.    For more information, refer to  Configuring the Gateway  documentation.", 
            "title": "Single sign-on (SSO)"
        }, 
        {
            "location": "/aws-create/index.html#ambari-server-master-key", 
            "text": "The Ambari server master key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.", 
            "title": "Ambari server master key"
        }, 
        {
            "location": "/aws-create/index.html#enable-kerberos-security", 
            "text": "Select this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to  Enabling Kerberos security .   Related links       Enabling Kerberos security    Amazon EC2 instance store  (External)  Amazon EC2 spot instances  (External)      Next: Configure Access to S3", 
            "title": "Enable Kerberos security"
        }, 
        {
            "location": "/aws-data/index.html", 
            "text": "Configuring access to Amazon S3\n\n\nAmazon S3 is not supported as a default file system, but access to data in S3 is possible via the s3a connector. Use these steps to configure access from your cluster to Amazon S3. \n\n\nThese steps assume that you are using an HDP version that supports the s3a cloud storage connector (HDP 2.6.1 or newer).  \n\n\nCreating an IAM role for S3 access\n\n\nIn order to configure access from your cluster to Amazon S3, you must have an existing IAM role which determines what actions can be performed on which S3 buckets. If you already have an IAM role, skip to the next step. If you do not have an existing IAM role, use the following instructions to create one. \n\n\nSteps\n\n\n\n\n\n\nNavigate to the \nIAM console\n \n \nRoles\n and click \nCreate Role\n.\n\n\n \n\n\n\n\n\n\nIn the \"Create Role\" wizard, select \nAWS service\n role type and then select \nEC2\n service and \nEC2\n use case. \n\n\n \n\n\n\n\n\n\nWhen done, click \nNext: Permissions\n to navigate to the next page in the wizard.\n\n\n\n\n\n\nSelect an existing S3 access policy or click \nCreate policy\n to define a new policy. If you are just getting started, you can select a built-in policy called \"AmazonS3FullAccess\", which provides full access to S3 buckets that are part of your account:\n\n\n\n\n\n\n\n\nWhen done attaching the policy, click \nNext: Review\n.\n\n\n\n\n\n\nIn the \nRoles name\n field, enter a name for the role that you are creating:  \n\n\n \n\n\n\n\n\n\nClick \nCreate role\n to finish the role creation process.\n\n\n\n\n\n\nConfigure access to S3\n\n\nAmazon S3 is not supported as a default file system, but access to data in S3 from your cluster VMs can be automatically configured by attaching an \ninstance profile\n allowing access to S3. You can optionally create or attach an existing instance profile during cluster creation on the \nCloud Storage\n page.\n\n\nTo configure access to S3 with an instance profile, follow these steps. \n\n\nSteps\n\n\n\n\nYou or your AWS admin must create an IAM role with an S3 access policy which can be used by cluster instances to access one or more S3 buckets. Refer to \nCreating an IAM role for S3 access\n.    \n\n\nOn the \nCloud Storage\n page in the advanced cluster wizard view, select \nUse existing instance profile\n.  \n\n\nSelect an existing IAM role created in step 1:\n\n\n\n\nDuring the cluster creation process, Cloudbreak assigns the IAM role and its associated permissions to the EC2 instances that are part of the cluster so that applications running on these instances can use the role to access S3.   \n\n\nTesting access from HDP to S3\n\n\nAmazon S3 is not supported in HDP as a default file system, but access to data in Amazon S3 is possible via the s3a connector. \n\n\nTo tests access to S3 from HDP, SSH to a cluster node and run a few hadoop fs shell commands against your existing S3 bucket.\n\n\nAmazon S3 access path syntax is:\n\n\ns3a://bucket/dir/file\n\n\n\nFor example, to access a file called \"mytestfile\" in a directory called \"mytestdir\", which is stored in a bucket called \"mytestbucket\", the URL is:\n\n\ns3a://mytestbucket/mytestdir/mytestfile\n\n\n\nThe following FileSystem shell commands demonstrate access to a bucket named \"mytestbucket\": \n\n\nhadoop fs -ls s3a://mytestbucket/\n\nhadoop fs -mkdir s3a://mytestbucket/testDir\n\nhadoop fs -put testFile s3a://mytestbucket/testFile\n\nhadoop fs -cat s3a://mytestbucket/testFile\ntest file content\n\n\n\nFor more information about configuring the S3 connector for HDP and working with data stored on S3, refer to \nCloud Data Access\n documentation.\n\n\nRelated links\n\n\nCloud Data Access\n (Hortonworks)\n\n\nConfigure S3 storage locations\n\n\nAfter configuring access to S3 via instance profile, you can optionally use an S3 bucket as a base storage location; this storage location is mainly for the Hive Warehouse Directory (used for storing the table data for managed tables).   \n\n\nPrerequisites\n \n\n\n\n\nYou must have an existing bucket. For instructions on how to create a bucket on S3, refer to \nAWS documentation\n.   \n\n\nThe instance profile that you configured under \nConfigure access to S3\n must allow access to the bucket.   \n\n\n\n\nSteps\n\n\n\n\nWhen creating a cluster, on the \nCloud Storage\n page in the advanced cluster wizard view, select \nUse existing instance profile\n and select the instance profile to use, as described in \nConfigure access to S3\n.    \n\n\nUnder \nStorage Locations\n, enable \nConfigure Storage Locations\n by clicking the \n button.   \n\n\n\n\nProvide your existing bucket name under \nBase Storage Location\n. \n\n\n\n\n\n\nUnder \nPath for Hive Warehouse Directory property (hive.metastore.warehouse.dir)\n, Cloudbreak automatically suggests a location within the bucket. For example, if the bucket that you specified is \nmy-test-bucket\n then the suggested location will be \nmy-test-bucket/apps/hive/warehouse\n.  You may optionally update this path.   \n\n\nCloudbreak automatically creates this directory structure in your bucket.", 
            "title": "Configure access to S3"
        }, 
        {
            "location": "/aws-data/index.html#configuring-access-to-amazon-s3", 
            "text": "Amazon S3 is not supported as a default file system, but access to data in S3 is possible via the s3a connector. Use these steps to configure access from your cluster to Amazon S3.   These steps assume that you are using an HDP version that supports the s3a cloud storage connector (HDP 2.6.1 or newer).", 
            "title": "Configuring access to Amazon S3"
        }, 
        {
            "location": "/aws-data/index.html#creating-an-iam-role-for-s3-access", 
            "text": "In order to configure access from your cluster to Amazon S3, you must have an existing IAM role which determines what actions can be performed on which S3 buckets. If you already have an IAM role, skip to the next step. If you do not have an existing IAM role, use the following instructions to create one.   Steps    Navigate to the  IAM console     Roles  and click  Create Role .       In the \"Create Role\" wizard, select  AWS service  role type and then select  EC2  service and  EC2  use case.        When done, click  Next: Permissions  to navigate to the next page in the wizard.    Select an existing S3 access policy or click  Create policy  to define a new policy. If you are just getting started, you can select a built-in policy called \"AmazonS3FullAccess\", which provides full access to S3 buckets that are part of your account:     When done attaching the policy, click  Next: Review .    In the  Roles name  field, enter a name for the role that you are creating:         Click  Create role  to finish the role creation process.", 
            "title": "Creating an IAM role for S3 access"
        }, 
        {
            "location": "/aws-data/index.html#configure-access-to-s3", 
            "text": "Amazon S3 is not supported as a default file system, but access to data in S3 from your cluster VMs can be automatically configured by attaching an  instance profile  allowing access to S3. You can optionally create or attach an existing instance profile during cluster creation on the  Cloud Storage  page.  To configure access to S3 with an instance profile, follow these steps.   Steps   You or your AWS admin must create an IAM role with an S3 access policy which can be used by cluster instances to access one or more S3 buckets. Refer to  Creating an IAM role for S3 access .      On the  Cloud Storage  page in the advanced cluster wizard view, select  Use existing instance profile .    Select an existing IAM role created in step 1:   During the cluster creation process, Cloudbreak assigns the IAM role and its associated permissions to the EC2 instances that are part of the cluster so that applications running on these instances can use the role to access S3.", 
            "title": "Configure access to S3"
        }, 
        {
            "location": "/aws-data/index.html#testing-access-from-hdp-to-s3", 
            "text": "Amazon S3 is not supported in HDP as a default file system, but access to data in Amazon S3 is possible via the s3a connector.   To tests access to S3 from HDP, SSH to a cluster node and run a few hadoop fs shell commands against your existing S3 bucket.  Amazon S3 access path syntax is:  s3a://bucket/dir/file  For example, to access a file called \"mytestfile\" in a directory called \"mytestdir\", which is stored in a bucket called \"mytestbucket\", the URL is:  s3a://mytestbucket/mytestdir/mytestfile  The following FileSystem shell commands demonstrate access to a bucket named \"mytestbucket\":   hadoop fs -ls s3a://mytestbucket/\n\nhadoop fs -mkdir s3a://mytestbucket/testDir\n\nhadoop fs -put testFile s3a://mytestbucket/testFile\n\nhadoop fs -cat s3a://mytestbucket/testFile\ntest file content  For more information about configuring the S3 connector for HDP and working with data stored on S3, refer to  Cloud Data Access  documentation.  Related links  Cloud Data Access  (Hortonworks)", 
            "title": "Testing access from HDP to S3"
        }, 
        {
            "location": "/aws-data/index.html#configure-s3-storage-locations", 
            "text": "After configuring access to S3 via instance profile, you can optionally use an S3 bucket as a base storage location; this storage location is mainly for the Hive Warehouse Directory (used for storing the table data for managed tables).     Prerequisites     You must have an existing bucket. For instructions on how to create a bucket on S3, refer to  AWS documentation .     The instance profile that you configured under  Configure access to S3  must allow access to the bucket.      Steps   When creating a cluster, on the  Cloud Storage  page in the advanced cluster wizard view, select  Use existing instance profile  and select the instance profile to use, as described in  Configure access to S3 .      Under  Storage Locations , enable  Configure Storage Locations  by clicking the   button.      Provide your existing bucket name under  Base Storage Location .     Under  Path for Hive Warehouse Directory property (hive.metastore.warehouse.dir) , Cloudbreak automatically suggests a location within the bucket. For example, if the bucket that you specified is  my-test-bucket  then the suggested location will be  my-test-bucket/apps/hive/warehouse .  You may optionally update this path.     Cloudbreak automatically creates this directory structure in your bucket.", 
            "title": "Configure S3 storage locations"
        }, 
        {
            "location": "/azure-pre/index.html", 
            "text": "Prerequisites on Azure\n\n\nBefore launching Cloudbreak on Azure, you must meet the following prerequisites.\n\n\nAzure account\n\n\nIn order to launch Cloudbreak on the Azure, log in to your existing Microsoft Azure account. If you don't have an account, you can set it up at \nhttps://azure.microsoft.com\n.\n\n\nAzure region\n\n\nDecide in which Azure region you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions \nsupported by Microsoft Azure\n.\n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.\n\n\nRelated links\n\n\nAzure regions\n (External)\n\n\nSSH key pair\n\n\nWhen launching Cloudbreak, you will be required to provide your public SSH key. If needed, you can generate a new SSH key pair:\n\n\n\n\nOn MacOS X and Linux using \nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n  \n\n\nOn Windows using \nPuTTygen\n\n\n\n\nVirtual network\n\n\nYou must have a virtual network configured on your cloud provider.\n\n\nSecurity group\n\n\nPorts 22 (SSH), 80 (HTTPS), and 443 (HTTPS) must be open on the security group\n\n\nAzure roles\n\n\nIn order to provision clusters on Azure, Cloudbreak must be able to assume a sufficient Azure role (\"Owner\" or \"Contributor\") via Cloudbreak credential:\n\n\n\n\n\n\nYour account must have the \"\nOwner\n\" role (or a role with equivalent permissions) in the subscription in order to \ncreate a Cloudbreak credential\n using the interactive credential method.\n\n\n\n\n\n\nYour account must have the \"\nContributor\n\" role (or a role with equivalent permissions) in the subscription in order to \ncreate a Cloudbreak credential\n using the app-based credential method. The role must be assigned to the app that you register in Cloudbreak.\n\n\n\n\n\n\nTo check the roles in your subscription, log in to your Azure account and navigate to \nSubscriptions\n.\n\n\nRelated links\n\n\nBuilt-in roles: Owner\n (External)\n\n\nBuilt-in roles: Contributor\n (External)\n\n\n\n\nNext: Launch Cloudbreak", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/azure-pre/index.html#prerequisites-on-azure", 
            "text": "Before launching Cloudbreak on Azure, you must meet the following prerequisites.", 
            "title": "Prerequisites on Azure"
        }, 
        {
            "location": "/azure-pre/index.html#azure-account", 
            "text": "In order to launch Cloudbreak on the Azure, log in to your existing Microsoft Azure account. If you don't have an account, you can set it up at  https://azure.microsoft.com .", 
            "title": "Azure account"
        }, 
        {
            "location": "/azure-pre/index.html#azure-region", 
            "text": "Decide in which Azure region you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions  supported by Microsoft Azure .  Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.  Related links  Azure regions  (External)", 
            "title": "Azure region"
        }, 
        {
            "location": "/azure-pre/index.html#ssh-key-pair", 
            "text": "When launching Cloudbreak, you will be required to provide your public SSH key. If needed, you can generate a new SSH key pair:   On MacOS X and Linux using  ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"     On Windows using  PuTTygen", 
            "title": "SSH key pair"
        }, 
        {
            "location": "/azure-pre/index.html#virtual-network", 
            "text": "You must have a virtual network configured on your cloud provider.", 
            "title": "Virtual network"
        }, 
        {
            "location": "/azure-pre/index.html#security-group", 
            "text": "Ports 22 (SSH), 80 (HTTPS), and 443 (HTTPS) must be open on the security group", 
            "title": "Security group"
        }, 
        {
            "location": "/azure-pre/index.html#azure-roles", 
            "text": "In order to provision clusters on Azure, Cloudbreak must be able to assume a sufficient Azure role (\"Owner\" or \"Contributor\") via Cloudbreak credential:    Your account must have the \" Owner \" role (or a role with equivalent permissions) in the subscription in order to  create a Cloudbreak credential  using the interactive credential method.    Your account must have the \" Contributor \" role (or a role with equivalent permissions) in the subscription in order to  create a Cloudbreak credential  using the app-based credential method. The role must be assigned to the app that you register in Cloudbreak.    To check the roles in your subscription, log in to your Azure account and navigate to  Subscriptions .  Related links  Built-in roles: Owner  (External)  Built-in roles: Contributor  (External)   Next: Launch Cloudbreak", 
            "title": "Azure roles"
        }, 
        {
            "location": "/azure-launch/index.html", 
            "text": "Launching Cloudbreak on Azure\n\n\nThese steps describe how to launch Cloudbreak on Azure for production. \nBefore launching Cloudbreak on Azure, review and meet the \nprerequisites\n. Next, follow the steps below.  \n\n\nVM requirements\n\n\nTo launch the Cloudbreak deployer and install the Cloudbreak application, you must have an existing VM. \n\n\nSystem requirements\n\n\nYour system must meet the following requirements:\n\n\n\n\nMinimum VM requirements: 16GB RAM, 40GB disk, 4 cores\n\n\nSupported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)\n\n\n\n\n\n\nYou can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.\n\n\n\n\nRoot access\n\n\nEvery command must be executed as root. In order to get root privileges execute:\n\n\nsudo -i\n\n\n\nSystem updates\n\n\nEnsure that your system is up-to-date by executing:\n\n\nyum -y update\n\n\n\nReboot it if necessary.\n\n\nInstall iptables\n\n\nPerform these steps to install and configure iptables.\n\n\nSteps\n \n\n\n\n\n\n\nInstall iptables-services:\n\n\nyum -y install net-tools ntp wget lsof unzip tar iptables-services\nsystemctl enable ntpd \n systemctl start ntpd\nsystemctl disable firewalld \n systemctl stop firewalld\n\n\n\n\nWithout iptables-services installed the \niptables save\n command will not be available.\n\n\n\n\n\n\n\n\nConfigure permissive iptables on your machine:\n\n\niptables --flush INPUT \n \\\niptables --flush FORWARD \n \\\nservice iptables save\n\n\n\n\n\n\nDisable SELINUX\n\n\nPerform these steps to disable SELINUX.\n\n\nSteps\n \n\n\n\n\n\n\nDisable SELINUX:\n\n\nsetenforce 0\nsed -i 's/SELINUX=enforcing/SELINUX=disabled/g' \n/etc/selinux/config\n\n\n\n\n\n\nRun the following command to ensure that SELinux is not turned on afterwards: \n\n\ngetenforce\n\n\n\n\n\n\nThe command should return \"Disabled\".     \n\n\n\n\n\n\nInstall Docker\n\n\nPerform these steps to install Docker. The minimum Docker version is 1.13.1. If you are using an older image that comes with an older Docker version, upgrade Docker to 1.13.1 or newer. \n\n\nSteps\n    \n\n\n\n\n\n\nInstall Docker service:\n\n\nyum install -y docker\nsystemctl start docker\nsystemctl enable docker\n\n\n\n\n\n\nCheck the Docker Logging Driver configuration:\n\n\ndocker info | grep \"Logging Driver\"\n\n\n\n\n\n\nIf it is set to \nLogging Driver: journald\n, you must  set it to \"json-file\" instead. To do that:\n\n\n\n\n\n\nOpen the \ndocker\n file for editing:\n\n\nvi /etc/sysconfig/docker\n  \n\n\n\n\n\n\nEdit the following part of the file so that it looks like below (showing \nlog-driver=json-file\n):\n\n\n# Modify these options if you want to change the way the docker daemon runs\nOPTIONS='--selinux-enabled --log-driver=json-file --signature-verification=false'\n     \n\n\n\n\n\n\nRestart Docker:\n\n\nsystemctl restart docker\nsystemctl status docker\n\n\n\n\n\n\n\n\n\n\nInstall Cloudbreak on a VM\n\n\nInstall Cloudbreak using the following steps.\n\n\nSteps\n\n\n\n\n\n\nInstall the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:\n\n\nyum -y install unzip tar\ncurl -Ls public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_2.7.0_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version\n\n\nOnce the Cloudbreak deployer is installed, you can set up the Cloudbreak application.\n\n\n\n\n\n\nCreate a Cloudbreak deployment directory and navigate to it:\n\n\nmkdir cloudbreak-deployment\ncd cloudbreak-deployment\n\n\n\n\n\n\nIn the directory, create a file called \nProfile\n with the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL\nexport PUBLIC_IP=MY_VM_IP\n\n\nFor example:\n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com\nexport PUBLIC_IP=172.26.231.100\n\n\nYou will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.\n\n\nYou should set the CLOUDBREAK_SMTP_SENDER_USERNAME variable to the username you use to authenticate to your SMTP server. You should set the CLOUDBREAK_SMTP_SENDER_PASSWORD variable to the password you use to authenticate to your SMTP server.\n\n\n\n\n\n\nGenerate configurations by executing:\n\n\nrm *.yml\ncbd generate\n   \n\n\nThe cbd start command includes the cbd generate command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.  \n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.   \n\n\n\n\n\n\n\n\nStart the Cloudbreak application by using the following commands:\n\n\ncbd pull-parallel\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Cloudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\n\n\nIf you encounter errors during \ncbd start\n, refer to \nToubleshooting\n.  \n\n\n\n\n\n\n\n\nNext, check Cloudbreak application logs:\n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nRelated links\n\n\nToubleshooting\n   \n\n\nAccess Cloudbreak web UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n \n\n\n\n\n\n\nYou can log into the Cloudbreak application at \nhttps://IP_Address\n. For example \nhttps://34.212.141.253\n. You may use \ncbd start\n to obtain the login information. Alternatively, you can obtain the VM's IP address from your cloud provider console. \n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nLog in to the Cloudbreak web UI using the credentials that you configured in your \nProfile\n file:\n\n\n\n\nThe username is the \nUAA_DEFAULT_USER_EMAIL\n     \n\n\nThe password is the \nUAA_DEFAULT_USER_PW\n \n\n\n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n  \n\n\n\n\n\n\nConfigure external Cloudbreak database\n\n\nBy default, Cloudbreak uses an embedded PostgreSQL database to persist data related to Cloudbreak configuration, setup, and so on. For a production Cloudbreak deployment, you must \nconfigure an external database\n.\n\n\nRelated links\n\n\nConfigure an external database\n  \n\n\nCreate Cloudbreak credential\n\n\nBefore you can start creating clusters, you must first create a \nCloudbreak credential\n. Without this credential, you will not be able to create clusters via Cloudbreak. Cloudbreak works by connecting your Azure account through this credential, and then uses it to create resources on your behalf.\n\n\nThere are two methods for creating a Cloudbreak credential:\n\n\n\n\n\n\n\n\nMethod\n\n\nDescription\n\n\nPrerequisite\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nInteractive\n\n\nThe advantage of using this method is that the app and service principal creation and role assignment are fully automated, so the only input that you need to provide is the Subscription ID and Directory ID. During the interactive credential creation, you are required to log in to your Azure account.\n\n\n(1) Your account must have the \"Owner\" role (or its equivalent) in the subscription. (2) You must be able log in to your Azure account.\n\n\nTo configure an interactive credential, refer to \nCreate an interactive credential\n.\n\n\n\n\n\n\nApp-based\n\n\nThe advantage of the app-based credential creation is that it allows you to create a credential without logging in to the Azure account, as long as you have been given all the information. In addition to providing your Subscription ID and  Directory ID, you must provide information for your previously created Azure AD application (its ID and key which allows access to it).\n\n\n(1) Your account must have the \"Contributor\" role (or equivalent) in the subscription. (2) You or your Azure administrator must perform prerequisite steps of registering an Azure application and assigning the \"Contributor\" role to it. This step typically requires admin permissions so you may have to contact your Azure administrator.\n\n\nTo configure an app based credential, refer to \nCreate an app-based credential\n.\n\n\n\n\n\n\n\n\nCreate interactive credential\n\n\nFollow these steps to create an interactive Cloudbreak credential.\n\n\nPrerequisites\n\n\nYour account must have have an Owner role in order for the interactive credential creation to work.   \n\n\nIf your account does not have an Owner role, you must you the \napp-based credential\n option instead of the interactive option. To review the requirements for both options. refer to \nAzure roles\n. \n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane.\n\n\n\n\n\n\nClick \nCreate Credential\n.\n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Microsoft Azure\".\n\n\n\n\n\n\nSelect \nInteractive Login\n:\n\n\n     \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nSubscription Id\n\n\nCopy and paste the Subscription ID from your \nSubscriptions\n.\n\n\n\n\n\n\nTenant Id\n\n\nCopy and paste your Directory ID from your \nActive Directory\n \n \nProperties\n.\n\n\n\n\n\n\nAzure role type\n\n\nYou have the following options:\n\"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \"\nContributor\n\" role to create resources. This requires no further input.\n\"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources.\n\"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to \nAzure\n documentation. \nIf using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters: \nMicrosoft.Compute/*\n, \nMicrosoft.Network/*\n, \nMicrosoft.Storage/*\n, \nMicrosoft.Resources/*\n.\n\n\n\n\n\n\n\n\nTo obtain the \nSubscription Id\n:\n\n\n   \n\n\nTo obtain the \nTenant ID\n (actually \nDirectory Id\n):\n\n\n    \n\n\n\n\n\n\nAfter providing the parameters, click \nInteractive Login\n.\n\n\n\n\n\n\nCopy the code provided in the UI:\n\n\n     \n\n\n\n\n\n\nClick \nAzure login\n and a new \nDevice login\n page will open in a new browser tab:\n\n\n  \n\n\n\n\n\n\nNext, paste the code in field on the  \nDevice login\n page and click \nContinue\n.\n\n\n\n\n\n\nConfirm your account by selecting it:\n\n\n\n\n\n\n\n\nA confirmation page will appear, confirming that you have signed in to the Microsoft Azure Cross-platform Command Line Interface application on your device. You may now close this window.\n\n\n\n\nIf you encounter errors, refer to \nTroubleshooting Azure\n.\n\n\n\n\nCongratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to create clusters.\n\n\n\n\n\n\nRelated links\n\n\nCreate app-based credential\n \n\n\nAzure roles\n\n\nTroubleshooting Azure\n\n\nContributor\n (External)\n\n\nCustom roles in Azure\n (External)  \n\n\nCreate app-based credential\n\n\nFollow these steps to create an app based Cloudbreak credential.\n\n\nPrerequisites\n\n\n\n\n\n\nOn Azure Portal, navigate to the \nActive Directory\n \n \nApp Registrations\n and register a new application. For more information, refer to \nCreate an Azure AD application\n.\n\n\n\n\nAa an alternative to the steps listed below for creating an application registration, you use a utility called \nazure-cli-tools\n. The utility supports app creation and role assignment. It is available at \nhttps://github.com/sequenceiq/azure-cli-tools/blob/master/cli_tools\n.\n\n\n\n\n  \n\n\n\n\n\n\nNavigate to the \nSubscriptions\n, choose \nAccess control (IAM)\n. Click \nAdd\n and then assign the \"Contributor\" role to your newly created application by selecting \"Contributor\" under \nRole\n and typing your app name under \nSelect\n (You must type your app name in order to find it):\n\n\n\n\nThis step typically requires admin permissions so you may have to contact your Azure administrator.\n\n\n\n\n   \n\n\n\n\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane.\n\n\n\n\n\n\nClick \nCreate Credential\n.\n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Microsoft Azure\".\n\n\n\n\n\n\nSelect \nApp based Login\n:\n\n\n\n\n\n\n\n\nOn the \nConfigure credential\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential Type\n\n\nSelect \nApp based\n.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nSubscription Id\n\n\nCopy and paste the Subscription ID from your \nSubscriptions\n.\n\n\n\n\n\n\nTenant Id\n\n\nCopy and paste your Directory ID from your \nActive Directory\n \n \nProperties\n.\n\n\n\n\n\n\nApp Id\n\n\nCopy and paste the Application ID from your \nAzure Active Directory\n \n \nApp Registrations\n \n your app registration's \nSettings\n \n \nProperties\n.\n\n\n\n\n\n\nPassword\n\n\nThis is your application key. You can generate it from your \nAzure Active Directory\n app registration's \nSettings\n \n \nKeys\n.\n\n\n\n\n\n\n\n\nTo obtain the \nSubscription Id\n from Subscriptions:\n\n\n   \n\n\nTo obtain the \nApp ID\n (actually \nApplication ID\n) and an application key from Azure Active Directory:\n\n\n  \n\n\n      \n\n\nTo obtain the \nTenant ID\n (actually \nDirectory Id\n) from Azure Active Directory:\n\n\n  \n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\nIf you encounter errors, refer to \nTroubleshooting Cloudbreak on Azure\n.  \n\n\n\n\nCongratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to \ncreate clusters\n.\n\n\n\n\n\n\nRelated links\n\n\nCLI tools\n (Hortonworks)  \n\n\nUse portal to create an Azure Active Directory application\n (External)     \n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on Azure"
        }, 
        {
            "location": "/azure-launch/index.html#launching-cloudbreak-on-azure", 
            "text": "These steps describe how to launch Cloudbreak on Azure for production. \nBefore launching Cloudbreak on Azure, review and meet the  prerequisites . Next, follow the steps below.", 
            "title": "Launching Cloudbreak on Azure"
        }, 
        {
            "location": "/azure-launch/index.html#vm-requirements", 
            "text": "To launch the Cloudbreak deployer and install the Cloudbreak application, you must have an existing VM.", 
            "title": "VM requirements"
        }, 
        {
            "location": "/azure-launch/index.html#system-requirements", 
            "text": "Your system must meet the following requirements:   Minimum VM requirements: 16GB RAM, 40GB disk, 4 cores  Supported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)    You can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.", 
            "title": "System requirements"
        }, 
        {
            "location": "/azure-launch/index.html#root-access", 
            "text": "Every command must be executed as root. In order to get root privileges execute:  sudo -i", 
            "title": "Root access"
        }, 
        {
            "location": "/azure-launch/index.html#system-updates", 
            "text": "Ensure that your system is up-to-date by executing:  yum -y update  Reboot it if necessary.", 
            "title": "System updates"
        }, 
        {
            "location": "/azure-launch/index.html#install-iptables", 
            "text": "Perform these steps to install and configure iptables.  Steps      Install iptables-services:  yum -y install net-tools ntp wget lsof unzip tar iptables-services\nsystemctl enable ntpd   systemctl start ntpd\nsystemctl disable firewalld   systemctl stop firewalld   Without iptables-services installed the  iptables save  command will not be available.     Configure permissive iptables on your machine:  iptables --flush INPUT   \\\niptables --flush FORWARD   \\\nservice iptables save", 
            "title": "Install iptables"
        }, 
        {
            "location": "/azure-launch/index.html#disable-selinux", 
            "text": "Perform these steps to disable SELINUX.  Steps      Disable SELINUX:  setenforce 0\nsed -i 's/SELINUX=enforcing/SELINUX=disabled/g' \n/etc/selinux/config    Run the following command to ensure that SELinux is not turned on afterwards:   getenforce    The command should return \"Disabled\".", 
            "title": "Disable SELINUX"
        }, 
        {
            "location": "/azure-launch/index.html#install-docker", 
            "text": "Perform these steps to install Docker. The minimum Docker version is 1.13.1. If you are using an older image that comes with an older Docker version, upgrade Docker to 1.13.1 or newer.   Steps         Install Docker service:  yum install -y docker\nsystemctl start docker\nsystemctl enable docker    Check the Docker Logging Driver configuration:  docker info | grep \"Logging Driver\"    If it is set to  Logging Driver: journald , you must  set it to \"json-file\" instead. To do that:    Open the  docker  file for editing:  vi /etc/sysconfig/docker       Edit the following part of the file so that it looks like below (showing  log-driver=json-file ):  # Modify these options if you want to change the way the docker daemon runs\nOPTIONS='--selinux-enabled --log-driver=json-file --signature-verification=false'          Restart Docker:  systemctl restart docker\nsystemctl status docker", 
            "title": "Install Docker"
        }, 
        {
            "location": "/azure-launch/index.html#install-cloudbreak-on-a-vm", 
            "text": "Install Cloudbreak using the following steps.  Steps    Install the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:  yum -y install unzip tar\ncurl -Ls public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_2.7.0_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version  Once the Cloudbreak deployer is installed, you can set up the Cloudbreak application.    Create a Cloudbreak deployment directory and navigate to it:  mkdir cloudbreak-deployment\ncd cloudbreak-deployment    In the directory, create a file called  Profile  with the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL\nexport PUBLIC_IP=MY_VM_IP  For example:  export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com\nexport PUBLIC_IP=172.26.231.100  You will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.  You should set the CLOUDBREAK_SMTP_SENDER_USERNAME variable to the username you use to authenticate to your SMTP server. You should set the CLOUDBREAK_SMTP_SENDER_PASSWORD variable to the password you use to authenticate to your SMTP server.    Generate configurations by executing:  rm *.yml\ncbd generate      The cbd start command includes the cbd generate command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.    Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.        Start the Cloudbreak application by using the following commands:  cbd pull-parallel\ncbd start  This will start the Docker containers and initialize the application. The first time you start the Cloudbreak app, the process will take longer than usual due to the download of all the necessary docker images.   If you encounter errors during  cbd start , refer to  Toubleshooting .       Next, check Cloudbreak application logs:  cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.    Related links  Toubleshooting", 
            "title": "Install Cloudbreak on a VM"
        }, 
        {
            "location": "/azure-launch/index.html#access-cloudbreak-web-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps      You can log into the Cloudbreak application at  https://IP_Address . For example  https://34.212.141.253 . You may use  cbd start  to obtain the login information. Alternatively, you can obtain the VM's IP address from your cloud provider console.     Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...       The login page is displayed:        Log in to the Cloudbreak web UI using the credentials that you configured in your  Profile  file:   The username is the  UAA_DEFAULT_USER_EMAIL        The password is the  UAA_DEFAULT_USER_PW       Upon a successful login, you are redirected to the dashboard:", 
            "title": "Access Cloudbreak web UI"
        }, 
        {
            "location": "/azure-launch/index.html#configure-external-cloudbreak-database", 
            "text": "By default, Cloudbreak uses an embedded PostgreSQL database to persist data related to Cloudbreak configuration, setup, and so on. For a production Cloudbreak deployment, you must  configure an external database .  Related links  Configure an external database", 
            "title": "Configure external Cloudbreak database"
        }, 
        {
            "location": "/azure-launch/index.html#create-cloudbreak-credential", 
            "text": "Before you can start creating clusters, you must first create a  Cloudbreak credential . Without this credential, you will not be able to create clusters via Cloudbreak. Cloudbreak works by connecting your Azure account through this credential, and then uses it to create resources on your behalf.  There are two methods for creating a Cloudbreak credential:     Method  Description  Prerequisite  Steps      Interactive  The advantage of using this method is that the app and service principal creation and role assignment are fully automated, so the only input that you need to provide is the Subscription ID and Directory ID. During the interactive credential creation, you are required to log in to your Azure account.  (1) Your account must have the \"Owner\" role (or its equivalent) in the subscription. (2) You must be able log in to your Azure account.  To configure an interactive credential, refer to  Create an interactive credential .    App-based  The advantage of the app-based credential creation is that it allows you to create a credential without logging in to the Azure account, as long as you have been given all the information. In addition to providing your Subscription ID and  Directory ID, you must provide information for your previously created Azure AD application (its ID and key which allows access to it).  (1) Your account must have the \"Contributor\" role (or equivalent) in the subscription. (2) You or your Azure administrator must perform prerequisite steps of registering an Azure application and assigning the \"Contributor\" role to it. This step typically requires admin permissions so you may have to contact your Azure administrator.  To configure an app based credential, refer to  Create an app-based credential .", 
            "title": "Create Cloudbreak credential"
        }, 
        {
            "location": "/azure-launch/index.html#create-interactive-credential", 
            "text": "Follow these steps to create an interactive Cloudbreak credential.  Prerequisites  Your account must have have an Owner role in order for the interactive credential creation to work.     If your account does not have an Owner role, you must you the  app-based credential  option instead of the interactive option. To review the requirements for both options. refer to  Azure roles .   Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.    Click  Create Credential .    Under  Cloud provider , select \"Microsoft Azure\".    Select  Interactive Login :           Provide the following information:     Parameter  Description      Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Subscription Id  Copy and paste the Subscription ID from your  Subscriptions .    Tenant Id  Copy and paste your Directory ID from your  Active Directory     Properties .    Azure role type  You have the following options: \"Use existing Contributor role\" (default): If you select this option, Cloudbreak will use the \" Contributor \" role to create resources. This requires no further input. \"Reuse existing custom role\": If you select this option and enter the name of an existing role, Cloudbreak will use this role to create resources. \"Let Cloudbreak create a custom role\": If you select this option and enter a name for the new role, the role will be created. When choosing role name, make sure that there is no existing role with the name chosen. For information on creating custom roles, refer to  Azure  documentation.  If using a custom role, make sure that it includes the necessary Action set for Cloudbreak to be able to manage clusters:  Microsoft.Compute/* ,  Microsoft.Network/* ,  Microsoft.Storage/* ,  Microsoft.Resources/* .     To obtain the  Subscription Id :       To obtain the  Tenant ID  (actually  Directory Id ):          After providing the parameters, click  Interactive Login .    Copy the code provided in the UI:           Click  Azure login  and a new  Device login  page will open in a new browser tab:        Next, paste the code in field on the   Device login  page and click  Continue .    Confirm your account by selecting it:     A confirmation page will appear, confirming that you have signed in to the Microsoft Azure Cross-platform Command Line Interface application on your device. You may now close this window.   If you encounter errors, refer to  Troubleshooting Azure .   Congratulations! You've successfully launched and configured Cloudbreak. Now you can use Cloudbreak to create clusters.    Related links  Create app-based credential    Azure roles  Troubleshooting Azure  Contributor  (External)  Custom roles in Azure  (External)", 
            "title": "Create interactive credential"
        }, 
        {
            "location": "/azure-launch/index.html#create-app-based-credential", 
            "text": "Follow these steps to create an app based Cloudbreak credential.  Prerequisites    On Azure Portal, navigate to the  Active Directory     App Registrations  and register a new application. For more information, refer to  Create an Azure AD application .   Aa an alternative to the steps listed below for creating an application registration, you use a utility called  azure-cli-tools . The utility supports app creation and role assignment. It is available at  https://github.com/sequenceiq/azure-cli-tools/blob/master/cli_tools .         Navigate to the  Subscriptions , choose  Access control (IAM) . Click  Add  and then assign the \"Contributor\" role to your newly created application by selecting \"Contributor\" under  Role  and typing your app name under  Select  (You must type your app name in order to find it):   This step typically requires admin permissions so you may have to contact your Azure administrator.          Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.    Click  Create Credential .    Under  Cloud provider , select \"Microsoft Azure\".    Select  App based Login :     On the  Configure credential  page, provide the following parameters:     Parameter  Description      Select Credential Type  Select  App based .    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Subscription Id  Copy and paste the Subscription ID from your  Subscriptions .    Tenant Id  Copy and paste your Directory ID from your  Active Directory     Properties .    App Id  Copy and paste the Application ID from your  Azure Active Directory     App Registrations    your app registration's  Settings     Properties .    Password  This is your application key. You can generate it from your  Azure Active Directory  app registration's  Settings     Keys .     To obtain the  Subscription Id  from Subscriptions:       To obtain the  App ID  (actually  Application ID ) and an application key from Azure Active Directory:              To obtain the  Tenant ID  (actually  Directory Id ) from Azure Active Directory:        Click  Create .   If you encounter errors, refer to  Troubleshooting Cloudbreak on Azure .     Congratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to  create clusters .    Related links  CLI tools  (Hortonworks)    Use portal to create an Azure Active Directory application  (External)        Next: Create a Cluster", 
            "title": "Create app-based credential"
        }, 
        {
            "location": "/azure-create/index.html", 
            "text": "Creating a cluster on Azure\n\n\nUse these steps to create a cluster.\n\n\n\n\nTroubleshooting cluster creation\n\n\nIf you experience problems during cluster creation, refer to \nTroubleshooting cluster creation\n.\n\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick the \nCreate Cluster\n button and the \nCreate Cluster\n wizard is displayed.\n\n    By default, \nBasic\n view is displayed. To view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced cluster options\n.\n\n\n \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, specify the following general parameters for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nChoose a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nRegion\n\n\nSelect the Azure region in which you would like to launch your cluster. For information on available Azure regions, refer to \nAzure documentation\n.\n\n\n\n\n\n\nPlatform Version\n\n\nChoose the HDP or HDF version to use for this cluster. Blueprints available for this platform version will be populated under \"Cluster Type\" below. If you selected the \nHDF\n platform, refer to \nCreating HDF clusters\n for HDF cluster configuration tips.\n\n\n\n\n\n\nCluster Type\n\n\nChoose one of the default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to \nUsing custom blueprints\n.\n\n\n\n\n\n\nFlex Subscription\n\n\nThis option will appear if you have configured your deployment for a \nflex support subscription\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, for each host group provide the following information to define your cluster nodes and attached storage. \n\n\nTo edit this section, click on the \n. When done editing, click on the \n to save the changes.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server by clicking the \n button. The \"Instance Count\" for that host group must be set to \"1\". If you are using one of the default blueprints, this is set by default.\n\n\n\n\n\n\nInstance Type\n\n\nSelect an instance type. For information about instance types on Azure refer to \nAzure documentation\n.\n\n\n\n\n\n\nInstance Count\n\n\nEnter the number of instances of a given type. Default is 1.\n\n\n\n\n\n\nStorage Type\n\n\nSelect the volume type. The options are:\nLocally-redundant storage\nGeo-redundant storage\nPremium locally-redundant storage\n For more information about these options refer to \nAzure documentation\n.\n\n\n\n\n\n\nAttached Volumes Per Instance\n\n\nEnter the number of volumes attached per instance. Default is 1.\n\n\n\n\n\n\nVolume Size\n\n\nEnter the size in GBs for each volume. Default is 100.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nGateway Configuration\n page, you can access gateway configuration options. \n\n\nWhen creating a cluster, Cloudbreak installs and configures a gateway (powered by Apache Knox) to protect access to the cluster resources. By default, the gateway is enabled for Ambari; You can optionally enable it for other cluster services. \n\n\nFor more information, refer to \nConfiguring the Gateway\n documentation.\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following to specify the networking resources that will be used for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Network\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.\n\n\n\n\n\n\nSelect Subnet\n\n\nSelect the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.\n\n\n\n\n\n\nSubnet (CIDR)\n\n\nIf you selected to create a new subnet, you must define a valid \nCIDR\n for the subnet. Default is 10.0.0.0/16.\n\n\n\n\n\n\n\n\n\n\nCloudbreak uses public IP addresses when communicating with cluster nodes.  \n\n\n\n\n\n\n\n\nDefine security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNew Security Group\n\n\n(Default) Creates a new security group with the rules that you defined:\nA set of \ndefault rules\n is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied. \nYou may open ports by defining the CIDR, entering port range, selecting protocol and clicking \n+\n.\nYou may delete default or previously added rules using the delete icon.\nIf you don't want to use security group, remove the default rules.\n\n\n\n\n\n\nExisting Security Groups\n\n\nAllows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.\n\n\n\n\n\n\n\n\nThe default experience of creating network resources such as network, subnet and security group automatically is provided for convenience. We strongly recommend you review these options and for production cluster deployments leverage your existing network resources that you have defined and validated to meet your enterprise requirements. For more information, refer to \nRestricting inbound access from Cloudbreak to cluster\n. \n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nPassword\n\n\nYou can log in to the Ambari UI using this password.\n\n\n\n\n\n\nConfirm Password\n\n\nConfirm the password.\n\n\n\n\n\n\nNew SSH public key\n\n\nCheck this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.\n\n\n\n\n\n\nExisting SSH public key\n\n\nSelect an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nRelated links\n\n\nFlex support subscription\n\n\nUsing custom blueprints\n \n\n\nDefault cluster security groups\n\n\nTroubleshooting cluster creation\n \n\n\nAzure regions\n (External)   \n\n\nCIDR\n (External)\n\n\nGeneral purpose Linux VM sizes\n (External)  \n\n\nAdvanced cluster options\n\n\nClick on \nAdvanced\n to view and enter additional configuration options\n\n\nEnable lifetime management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes). \n\n\nTags\n\n\nYou can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. \n\n\nBy default, the following tags are created:\n\n\n\n\n\n\n\n\nTag\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncb-version\n\n\nCloudbreak version\n\n\n\n\n\n\nOwner\n\n\nYour Cloudbreak admin email.\n\n\n\n\n\n\ncb-account-name\n\n\nYour automatically generated Cloudbreak account name stored in the identity server.\n\n\n\n\n\n\ncb-user-name\n\n\nYour Cloudbreak admin email.\n\n\n\n\n\n\n\n\nFor more information, refer to \nTagging resources\n.\n\n\nRelated links\n    \n\n\nTagging resources\n \n\n\nChoose image catalog\n\n\nBy default, \nChoose Image Catalog\n is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to \nUsing custom images\n.\n\n\nRelated links\n   \n\n\nUsing custom images\n  \n\n\nChoose image type\n\n\nCloudbreak supports the following types of images for launching clusters:\n\n\n\n\n\n\n\n\nImage type\n\n\nDescription\n\n\nDefault images provided\n\n\nSupport for custom images\n\n\n\n\n\n\n\n\n\n\nPrewarmed Image\n\n\nBy default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP/HDF. The Ambari and HDP/HDF version used by prewarmed images cannot be customized.\n\n\nYes\n\n\nNo\n\n\n\n\n\n\nBase Image\n\n\nBase images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP/HDF software.\n\n\nYes\n\n\nYes\n\n\n\n\n\n\n\n\nBy default, Cloudbreak uses the included default \nprewarmed images\n, which include the operating system, as well as\nAmbari and HDP/HDF packages installed. You can optionally select the \nbase image\n option if you would like to:\n\n\n\n\nUse an Ambari and HDP/HDF versions different than what the prewarmed image includes and/or  \n\n\nChoose a previously created custom base image\n\n\n\n\nChoose image\n\n\nIf under \nChoose image catalog\n, you selected a custom image catalog, under \nChoose Image\n you can select an image from that catalog. For complete instructions, refer to \nUsing custom images\n. \n\n\nIf you are trying to customize Ambari and HDP/HDF versions, you can ignore the \nChoose Image\n option; in this case default base image is used.\n\n\nAmbari repo specification\n\n\nIf you would like to use a custom Ambari version, provide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nVersion\n\n\nEnter Ambari version.\n\n\n2.6.1.3\n\n\n\n\n\n\nRepo Url\n\n\nProvide a URL to the Ambari version repo that you would like to use.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.3\n\n\n\n\n\n\nRepo Gpg Key Url\n\n\nProvide a URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins\n\n\n\n\n\n\n\n\nHDP/HDF repo specification\n\n\nIf you would like to use a custom HDP or HDF version, provide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nThis is populated by default based on the \"Platform Version\" parameter.\n\n\nHDP\n\n\n\n\n\n\nVersion\n\n\nThis is populated by default based on the \"Platform Version\" parameter.\n\n\n2.6\n\n\n\n\n\n\nOS\n\n\nOperating system.\n\n\ncentos7 (Azure, GCP, OpenStack) or amazonlinux (AWS)\n\n\n\n\n\n\nRepository Version\n\n\nEnter repository version.\n\n\n2.6.4.0-91\n\n\n\n\n\n\nVersion Definition File\n\n\nEnter the URL of the VDF file.\n\n\nhttp://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.6.4.0/HDP-2.6.4.0-91.xml\n\n\n\n\n\n\n(HDF only) MPack Url\n\n\n(HDF only) Provide MPack URL.\n\n\nhttp://public-repo-1.hortonworks.com/HDF/centos7/3.x/updates/3.1.1.0/tars/hdf_ambari_mp/hdf-ambari-mpack-3.1.1.0-35.tar.gz\n\n\n\n\n\n\nEnable Ambari Server to download and install GPL Licensed LZO packages?\n\n\n(Optional, only available if using Ambari 2.6.1.0 or newer) Use this option to enable LZO compression in your HDP/HDF cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to \nEnabling LZO\n.\n\n\n\n\n\n\n\n\n\n\nIf you choose to use a base image with custom Ambari and/or HDP/HDF version, Cloudbreak validates the information entered. When Cloudbreak detects that the information entered is incorrect, it displays a warning marked with the \n sign. You should review all the warnings before proceeding and make sure that the information that you entered is correct. If you choose to proceed in spite of the warnings, check \"Ignore repository warnings\".  \n\n\nRelated links\n    \n\n\nUsing custom images\n      \n\n\nRoot volume size\n\n\nUse this option to increase the root volume size. Default is 30 GB. This option is useful if your custom image requires more space than the default 30 GB.\n\n\nIf you change the value of the root volume size, an osDisk with the given rootVolumeSize will be created for the instance automatically; However, you will have to manually resize the osDisk partition by using the steps provided in the \nAzure documentation\n.\n\n\nRelated links\n\n\nHow to: Resize Linux osDisk partition on Azure\n  \n\n\nAvailability sets\n\n\nTo support fault tolerance for VMs, Azure uses the concept of \navailability sets\n. This allows two or more VMs to be mapped to multiple fault domains, each of which defines a group of virtual machines that share a common power source and a network switch. When adding VMs to an availability set, Azure automatically assigns each VM a fault domain. The SLA includes guarantees that during OS Patching in Azure or during maintenance operations, at least one VM belonging to a given fault domain will be available.\n\n\nIn Cloudbreak, an availability set is automatically configured during cluster creation for each host group with \"Instance Count\" that is set to 1 or larger. The assignment of fault domains is automated by Azure, so there is no option for this in Cloudbreak UI. \n\n\nCloudbreak allows you to configure the availability set on the advanced \nHardware and Storage\n page of the create cluster wizard by providing the following options for each host group:  \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nDefault\n\n\n\n\n\n\n\n\n\n\nAvailability Set Name\n\n\nChoose a name for the availability set that will be created for the selected host group\n\n\nThe following convention is used: \"clustername-hostgroupname-as\"\n\n\n\n\n\n\nFault Domain Count\n\n\nThe number of fault domains.\n\n\n2 or 3, depending on the setting supported by Azure\n\n\n\n\n\n\nUpdate Domain Count\n\n\nThis number of update domains. This can be set to a number in range of 2-20.\n\n\n20\n\n\n\n\n\n\n\n\nAfter the deployment is finished, you can check the layout of the VMs inside an availability set on Azure Portal. You will find the \"Availability set\" resources corresponding to the host groups inside the deployment's resource group.\n\n\nCloud storage\n\n\nIf you would like to access ADLS or WASB from your cluster, you must configure access as described in \nConfiguring access to ADLS\n or \nConfiguring access to WASB\n. \n\n\nRelated links\n\n\nConfiguring access to ADLS\n\n\nConfiguring access to WASB\n  \n\n\nRecipes\n\n\nThis option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to \nUsing custom scripts (recipes)\n. \n\n\nRelated links\n    \n\n\nUsing custom scripts (recipes)\n \n\n\nManagement packs\n\n\nThis option allows you to select previously uploaded management packs. For more information on management packs, refer to \nUsing management packs\n. \n\n\nRelated links\n    \n\n\nUsing management packs\n  \n\n\nExternal sources\n\n\nYou can register external sources with Cloudbreak, and then select and attach them during cluster create. To register external sources with Cloudbreak, refer to:\n\n\n\n\nUsing an external authentication source\n    \n\n\nUsing an external database\n  \n\n\nRegister a proxy\n  \n\n\n\n\nCustom properties\n\n\nThis option allows you to set custom properties based on the template defined in your custom blueprint. For more information, refer to \nSet custom properties\n. \n\n\nRelated links\n    \n\n\nSet custom properties\n   \n\n\nSingle sign-on (SSO)\n\n\nThis option allows you to configure the gateway to be the SSO identity provider. \n\n\n\n\nThis option is technical preview. \n\n\n\n\nFor more information, refer to \nConfiguring the Gateway\n documentation.  \n\n\nDon't create public IP\n\n\nThis option is available if you are creating a cluster in an existing network and subnet. Select this option if you don't want to use public IPs for the network. \n\n\nDon't create new firewall rules\n\n\nThis option is available if you are creating a cluster in an existing network and subnet. Select this option if you don't want to create new firewall rules for the network. \n\n\nAmbari server master key\n\n\nThe Ambari server master key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.  \n\n\nEnable Azure disk encryption\n\n\nCheck this option if you would like to have your virtual machine disks encrypted using the Azure Disk Encryption capability provided by Azure. For more information, refer to \nAzure documentation\n.  \n\n\nEnable Kerberos security\n\n\nSelect this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to \nEnabling Kerberos security\n. \n\n\nRelated links\n    \n\n\nEnabling Kerberos security\n \n\n\nIntroduction to Microsoft Azure storage\n (External)  \n\n\n\n\nNext: Configure Access to ADLS", 
            "title": "Create a cluster"
        }, 
        {
            "location": "/azure-create/index.html#creating-a-cluster-on-azure", 
            "text": "Use these steps to create a cluster.   Troubleshooting cluster creation  If you experience problems during cluster creation, refer to  Troubleshooting cluster creation .  Steps    Log in to the Cloudbreak UI.    Click the  Create Cluster  button and the  Create Cluster  wizard is displayed. \n    By default,  Basic  view is displayed. To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced cluster options .       On the  General Configuration  page, specify the following general parameters for your cluster:     Parameter  Description      Select Credential  Choose a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.    Region  Select the Azure region in which you would like to launch your cluster. For information on available Azure regions, refer to  Azure documentation .    Platform Version  Choose the HDP or HDF version to use for this cluster. Blueprints available for this platform version will be populated under \"Cluster Type\" below. If you selected the  HDF  platform, refer to  Creating HDF clusters  for HDF cluster configuration tips.    Cluster Type  Choose one of the default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to  Using custom blueprints .    Flex Subscription  This option will appear if you have configured your deployment for a  flex support subscription .       On the  Hardware and Storage  page, for each host group provide the following information to define your cluster nodes and attached storage.   To edit this section, click on the  . When done editing, click on the   to save the changes.     Parameter  Description      Ambari Server  You must select one node for Ambari Server by clicking the   button. The \"Instance Count\" for that host group must be set to \"1\". If you are using one of the default blueprints, this is set by default.    Instance Type  Select an instance type. For information about instance types on Azure refer to  Azure documentation .    Instance Count  Enter the number of instances of a given type. Default is 1.    Storage Type  Select the volume type. The options are: Locally-redundant storage Geo-redundant storage Premium locally-redundant storage  For more information about these options refer to  Azure documentation .    Attached Volumes Per Instance  Enter the number of volumes attached per instance. Default is 1.    Volume Size  Enter the size in GBs for each volume. Default is 100.       On the  Gateway Configuration  page, you can access gateway configuration options.   When creating a cluster, Cloudbreak installs and configures a gateway (powered by Apache Knox) to protect access to the cluster resources. By default, the gateway is enabled for Ambari; You can optionally enable it for other cluster services.   For more information, refer to  Configuring the Gateway  documentation.    On the  Network  page, provide the following to specify the networking resources that will be used for your cluster:     Parameter  Description      Select Network  Select the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.    Select Subnet  Select the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.    Subnet (CIDR)  If you selected to create a new subnet, you must define a valid  CIDR  for the subnet. Default is 10.0.0.0/16.      Cloudbreak uses public IP addresses when communicating with cluster nodes.       Define security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:     Option  Description      New Security Group  (Default) Creates a new security group with the rules that you defined: A set of  default rules  is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied.  You may open ports by defining the CIDR, entering port range, selecting protocol and clicking  + . You may delete default or previously added rules using the delete icon. If you don't want to use security group, remove the default rules.    Existing Security Groups  Allows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.     The default experience of creating network resources such as network, subnet and security group automatically is provided for convenience. We strongly recommend you review these options and for production cluster deployments leverage your existing network resources that you have defined and validated to meet your enterprise requirements. For more information, refer to  Restricting inbound access from Cloudbreak to cluster .     On the  Security  page, provide the following parameters:     Parameter  Description      Cluster User  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Password  You can log in to the Ambari UI using this password.    Confirm Password  Confirm the password.    New SSH public key  Check this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.    Existing SSH public key  Select an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.       Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.    Related links  Flex support subscription  Using custom blueprints    Default cluster security groups  Troubleshooting cluster creation    Azure regions  (External)     CIDR  (External)  General purpose Linux VM sizes  (External)", 
            "title": "Creating a cluster on Azure"
        }, 
        {
            "location": "/azure-create/index.html#advanced-cluster-options", 
            "text": "Click on  Advanced  to view and enter additional configuration options", 
            "title": "Advanced cluster options"
        }, 
        {
            "location": "/azure-create/index.html#enable-lifetime-management", 
            "text": "Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes).", 
            "title": "Enable lifetime management"
        }, 
        {
            "location": "/azure-create/index.html#tags", 
            "text": "You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.   By default, the following tags are created:     Tag  Description      cb-version  Cloudbreak version    Owner  Your Cloudbreak admin email.    cb-account-name  Your automatically generated Cloudbreak account name stored in the identity server.    cb-user-name  Your Cloudbreak admin email.     For more information, refer to  Tagging resources .  Related links       Tagging resources", 
            "title": "Tags"
        }, 
        {
            "location": "/azure-create/index.html#choose-image-catalog", 
            "text": "By default,  Choose Image Catalog  is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to  Using custom images .  Related links      Using custom images", 
            "title": "Choose image catalog"
        }, 
        {
            "location": "/azure-create/index.html#choose-image-type", 
            "text": "Cloudbreak supports the following types of images for launching clusters:     Image type  Description  Default images provided  Support for custom images      Prewarmed Image  By default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP/HDF. The Ambari and HDP/HDF version used by prewarmed images cannot be customized.  Yes  No    Base Image  Base images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP/HDF software.  Yes  Yes     By default, Cloudbreak uses the included default  prewarmed images , which include the operating system, as well as\nAmbari and HDP/HDF packages installed. You can optionally select the  base image  option if you would like to:   Use an Ambari and HDP/HDF versions different than what the prewarmed image includes and/or    Choose a previously created custom base image", 
            "title": "Choose image type"
        }, 
        {
            "location": "/azure-create/index.html#choose-image", 
            "text": "If under  Choose image catalog , you selected a custom image catalog, under  Choose Image  you can select an image from that catalog. For complete instructions, refer to  Using custom images .   If you are trying to customize Ambari and HDP/HDF versions, you can ignore the  Choose Image  option; in this case default base image is used.", 
            "title": "Choose image"
        }, 
        {
            "location": "/azure-create/index.html#ambari-repo-specification", 
            "text": "If you would like to use a custom Ambari version, provide the following information:      Parameter  Description  Example      Version  Enter Ambari version.  2.6.1.3    Repo Url  Provide a URL to the Ambari version repo that you would like to use.  http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.3    Repo Gpg Key Url  Provide a URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.  http://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins", 
            "title": "Ambari repo specification"
        }, 
        {
            "location": "/azure-create/index.html#hdphdf-repo-specification", 
            "text": "If you would like to use a custom HDP or HDF version, provide the following information:      Parameter  Description  Example      Stack  This is populated by default based on the \"Platform Version\" parameter.  HDP    Version  This is populated by default based on the \"Platform Version\" parameter.  2.6    OS  Operating system.  centos7 (Azure, GCP, OpenStack) or amazonlinux (AWS)    Repository Version  Enter repository version.  2.6.4.0-91    Version Definition File  Enter the URL of the VDF file.  http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.6.4.0/HDP-2.6.4.0-91.xml    (HDF only) MPack Url  (HDF only) Provide MPack URL.  http://public-repo-1.hortonworks.com/HDF/centos7/3.x/updates/3.1.1.0/tars/hdf_ambari_mp/hdf-ambari-mpack-3.1.1.0-35.tar.gz    Enable Ambari Server to download and install GPL Licensed LZO packages?  (Optional, only available if using Ambari 2.6.1.0 or newer) Use this option to enable LZO compression in your HDP/HDF cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to  Enabling LZO .      If you choose to use a base image with custom Ambari and/or HDP/HDF version, Cloudbreak validates the information entered. When Cloudbreak detects that the information entered is incorrect, it displays a warning marked with the   sign. You should review all the warnings before proceeding and make sure that the information that you entered is correct. If you choose to proceed in spite of the warnings, check \"Ignore repository warnings\".    Related links       Using custom images", 
            "title": "HDP/HDF repo specification"
        }, 
        {
            "location": "/azure-create/index.html#root-volume-size", 
            "text": "Use this option to increase the root volume size. Default is 30 GB. This option is useful if your custom image requires more space than the default 30 GB.  If you change the value of the root volume size, an osDisk with the given rootVolumeSize will be created for the instance automatically; However, you will have to manually resize the osDisk partition by using the steps provided in the  Azure documentation .  Related links  How to: Resize Linux osDisk partition on Azure", 
            "title": "Root volume size"
        }, 
        {
            "location": "/azure-create/index.html#availability-sets", 
            "text": "To support fault tolerance for VMs, Azure uses the concept of  availability sets . This allows two or more VMs to be mapped to multiple fault domains, each of which defines a group of virtual machines that share a common power source and a network switch. When adding VMs to an availability set, Azure automatically assigns each VM a fault domain. The SLA includes guarantees that during OS Patching in Azure or during maintenance operations, at least one VM belonging to a given fault domain will be available.  In Cloudbreak, an availability set is automatically configured during cluster creation for each host group with \"Instance Count\" that is set to 1 or larger. The assignment of fault domains is automated by Azure, so there is no option for this in Cloudbreak UI.   Cloudbreak allows you to configure the availability set on the advanced  Hardware and Storage  page of the create cluster wizard by providing the following options for each host group:       Parameter  Description  Default      Availability Set Name  Choose a name for the availability set that will be created for the selected host group  The following convention is used: \"clustername-hostgroupname-as\"    Fault Domain Count  The number of fault domains.  2 or 3, depending on the setting supported by Azure    Update Domain Count  This number of update domains. This can be set to a number in range of 2-20.  20     After the deployment is finished, you can check the layout of the VMs inside an availability set on Azure Portal. You will find the \"Availability set\" resources corresponding to the host groups inside the deployment's resource group.", 
            "title": "Availability sets"
        }, 
        {
            "location": "/azure-create/index.html#cloud-storage", 
            "text": "If you would like to access ADLS or WASB from your cluster, you must configure access as described in  Configuring access to ADLS  or  Configuring access to WASB .   Related links  Configuring access to ADLS  Configuring access to WASB", 
            "title": "Cloud storage"
        }, 
        {
            "location": "/azure-create/index.html#recipes", 
            "text": "This option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to  Using custom scripts (recipes) .   Related links       Using custom scripts (recipes)", 
            "title": "Recipes"
        }, 
        {
            "location": "/azure-create/index.html#management-packs", 
            "text": "This option allows you to select previously uploaded management packs. For more information on management packs, refer to  Using management packs .   Related links       Using management packs", 
            "title": "Management packs"
        }, 
        {
            "location": "/azure-create/index.html#external-sources", 
            "text": "You can register external sources with Cloudbreak, and then select and attach them during cluster create. To register external sources with Cloudbreak, refer to:   Using an external authentication source       Using an external database     Register a proxy", 
            "title": "External sources"
        }, 
        {
            "location": "/azure-create/index.html#custom-properties", 
            "text": "This option allows you to set custom properties based on the template defined in your custom blueprint. For more information, refer to  Set custom properties .   Related links       Set custom properties", 
            "title": "Custom properties"
        }, 
        {
            "location": "/azure-create/index.html#single-sign-on-sso", 
            "text": "This option allows you to configure the gateway to be the SSO identity provider.    This option is technical preview.    For more information, refer to  Configuring the Gateway  documentation.", 
            "title": "Single sign-on (SSO)"
        }, 
        {
            "location": "/azure-create/index.html#dont-create-public-ip", 
            "text": "This option is available if you are creating a cluster in an existing network and subnet. Select this option if you don't want to use public IPs for the network.", 
            "title": "Don't create public IP"
        }, 
        {
            "location": "/azure-create/index.html#dont-create-new-firewall-rules", 
            "text": "This option is available if you are creating a cluster in an existing network and subnet. Select this option if you don't want to create new firewall rules for the network.", 
            "title": "Don't create new firewall rules"
        }, 
        {
            "location": "/azure-create/index.html#ambari-server-master-key", 
            "text": "The Ambari server master key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.", 
            "title": "Ambari server master key"
        }, 
        {
            "location": "/azure-create/index.html#enable-azure-disk-encryption", 
            "text": "Check this option if you would like to have your virtual machine disks encrypted using the Azure Disk Encryption capability provided by Azure. For more information, refer to  Azure documentation .", 
            "title": "Enable Azure disk encryption"
        }, 
        {
            "location": "/azure-create/index.html#enable-kerberos-security", 
            "text": "Select this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to  Enabling Kerberos security .   Related links       Enabling Kerberos security    Introduction to Microsoft Azure storage  (External)     Next: Configure Access to ADLS", 
            "title": "Enable Kerberos security"
        }, 
        {
            "location": "/azure-data-adls/index.html", 
            "text": "Configuring access to ADLS\n\n\nHortonworks Data Platform (HDP) supports reading and writing block blobs and page blobs from and to \nWindows Azure Storage Blob (WASB)\n object store, as well as reading and writing files stored in an \nAzure Data Lake Storage (ADLS)\n account. \n\n\nAzure Data Lake Store (ADLS)\n is an enterprise-wide hyper-scale repository for big data analytic workloads. ADLS is not supported as a default file system, but access to data in ADLS is possible via the adl connector. \n\n\nThese steps assume that you are using an HDP version that supports the adl cloud storage connector (HDP 2.6.1 or newer).  \n\n\nPrerequisites\n\n\nIf you would like to use ADLS to store your data, you must enable Azure subscription for Data Lake Store, and then create an Azure Data Lake Store \nstorage account\n.\n\n\nConfigure access to ADLS\n\n\nTo configure authentication with ADLS using the client credential, you must register a new application with Active Directory service and then give your application access to your ADL account. After you've performed these steps, you can provide your application's information when creating a cluster. \n\n\nSteps\n\n\n\n\n\n\nRegister an application and add it to the ADLS account, as described in \nStep 1\n and \nStep 2\n of \nHow to configure authentication with ADLS\n. \n\n\n\n\nDo not perform the \nStep 3\n described in this article. Cloudbreak automates this step.  \n\n\n\n\n\n\n\n\nIn Cloudbreak web UI, on the advanced \nCloud Storage\n page of the create a cluster wizard, select \nUse existing ADLS storage\n.\n\n\n\n\n\n\nProvide the following parameters for your registered application:    \n\n\n\n\nADLS Account Name\n: This is the ADL account that your application was assigned to.    \n\n\nApplication ID\n: You can find it in your application's settings. \n\n\nKey\n: This is the key that you generated for your application. If you did not copy the it, you must create a new key from the Keys page in your application's settings.   \n\n\n\n\n\n\n\n\nOnce your cluster is in the running state, you will be able to access the Azure blob storage account from the cluster nodes. \n\n\nTest access to ADLS\n\n\nTo tests access to ADLS, SSH to a cluster node and run a few hadoop fs shell commands against your existing ADLS account.\n\n\nADLS access path syntax is:\n\n\nadl://\naccount_name\n.azuredatalakestore.net/\ndir/file\n\n\n\nFor example, the following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\":\n\n\nhadoop fs -mkdir adl://myaccount.azuredatalakestore.net/testdir\n\n\n\nhadoop fs -put testfile adl://myaccount.azuredatalakestore.net/testdir/testfile\n\n\n\nTo use DistCp against ADLS, use the following syntax:\n\nhadoop distcp\n    [-D hadoop.security.credential.provider.path=localjceks://file/home/user/adls.jceks]\n    hdfs://\nnamenode_hostname\n:9001/user/foo/007020615\n    adl://\nmyaccount\n.azuredatalakestore.net/testDir/\n\n\nFor more information about configuring the ADLS connector and working with data stored in ADLS, refer to \nCloud Data Access\n documentation.\n\n\nRelated links\n \n\n\nCloud Data Access\n (Hortonworks)\n\n\nHow to configure authentication with ADLS\n (Hortonworks)\n\n\nAzure Data Lake Store\n (External)   \n\n\nCreate a storage account\n (External)   \n\n\nGet started with Azure Data Lake Store\n (External)  \n\n\nConfigure ADLS storage locations\n\n\nAfter configuring access to ADLS, you can optionally use that ADLS storage account as a base storage location; this storage location is mainly for the Hive Warehouse Directory (used for storing the table data for managed tables).   \n\n\nSteps\n\n\n\n\nWhen creating a cluster, on the \nCloud Storage\n page in the advanced cluster wizard view, select \nUse existing ADLS storage\n and select the instance profile to use, as described in \nConfigure access to ADLS\n.    \n\n\nUnder \nStorage Locations\n, enable \nConfigure Storage Locations\n by clicking the \n button.   \n\n\n\n\nProvide your existing directory name under \nBase Storage Location\n. \n\n\n\n\nMake sure that the directory exists within the account.\n\n\n\n\n\n\n\n\nUnder \nPath for Hive Warehouse Directory property (hive.metastore.warehouse.dir)\n, Cloudbreak automatically suggests a location within the bucket. For example, if the directory that you specified is \nmy-test-dir\n then the suggested location will be \nmy-test-adls-account.azuredatalakestore.net/my-test-dir/apps/hive/warehouse\n. You may optionally update this path.        \n\n\nCloudbreak automatically creates this directory structure in your bucket.  \n\n\n\n\n\n\n\n\nNext: Configure Access to WASB", 
            "title": "Configure access to ADLS"
        }, 
        {
            "location": "/azure-data-adls/index.html#configuring-access-to-adls", 
            "text": "Hortonworks Data Platform (HDP) supports reading and writing block blobs and page blobs from and to  Windows Azure Storage Blob (WASB)  object store, as well as reading and writing files stored in an  Azure Data Lake Storage (ADLS)  account.   Azure Data Lake Store (ADLS)  is an enterprise-wide hyper-scale repository for big data analytic workloads. ADLS is not supported as a default file system, but access to data in ADLS is possible via the adl connector.   These steps assume that you are using an HDP version that supports the adl cloud storage connector (HDP 2.6.1 or newer).", 
            "title": "Configuring access to ADLS"
        }, 
        {
            "location": "/azure-data-adls/index.html#prerequisites", 
            "text": "If you would like to use ADLS to store your data, you must enable Azure subscription for Data Lake Store, and then create an Azure Data Lake Store  storage account .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/azure-data-adls/index.html#configure-access-to-adls", 
            "text": "To configure authentication with ADLS using the client credential, you must register a new application with Active Directory service and then give your application access to your ADL account. After you've performed these steps, you can provide your application's information when creating a cluster.   Steps    Register an application and add it to the ADLS account, as described in  Step 1  and  Step 2  of  How to configure authentication with ADLS .    Do not perform the  Step 3  described in this article. Cloudbreak automates this step.       In Cloudbreak web UI, on the advanced  Cloud Storage  page of the create a cluster wizard, select  Use existing ADLS storage .    Provide the following parameters for your registered application:       ADLS Account Name : This is the ADL account that your application was assigned to.      Application ID : You can find it in your application's settings.   Key : This is the key that you generated for your application. If you did not copy the it, you must create a new key from the Keys page in your application's settings.        Once your cluster is in the running state, you will be able to access the Azure blob storage account from the cluster nodes.", 
            "title": "Configure access to ADLS"
        }, 
        {
            "location": "/azure-data-adls/index.html#test-access-to-adls", 
            "text": "To tests access to ADLS, SSH to a cluster node and run a few hadoop fs shell commands against your existing ADLS account.  ADLS access path syntax is:  adl:// account_name .azuredatalakestore.net/ dir/file  For example, the following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\":  hadoop fs -mkdir adl://myaccount.azuredatalakestore.net/testdir  hadoop fs -put testfile adl://myaccount.azuredatalakestore.net/testdir/testfile  To use DistCp against ADLS, use the following syntax: hadoop distcp\n    [-D hadoop.security.credential.provider.path=localjceks://file/home/user/adls.jceks]\n    hdfs:// namenode_hostname :9001/user/foo/007020615\n    adl:// myaccount .azuredatalakestore.net/testDir/  For more information about configuring the ADLS connector and working with data stored in ADLS, refer to  Cloud Data Access  documentation.  Related links    Cloud Data Access  (Hortonworks)  How to configure authentication with ADLS  (Hortonworks)  Azure Data Lake Store  (External)     Create a storage account  (External)     Get started with Azure Data Lake Store  (External)", 
            "title": "Test access to ADLS"
        }, 
        {
            "location": "/azure-data-adls/index.html#configure-adls-storage-locations", 
            "text": "After configuring access to ADLS, you can optionally use that ADLS storage account as a base storage location; this storage location is mainly for the Hive Warehouse Directory (used for storing the table data for managed tables).     Steps   When creating a cluster, on the  Cloud Storage  page in the advanced cluster wizard view, select  Use existing ADLS storage  and select the instance profile to use, as described in  Configure access to ADLS .      Under  Storage Locations , enable  Configure Storage Locations  by clicking the   button.      Provide your existing directory name under  Base Storage Location .    Make sure that the directory exists within the account.     Under  Path for Hive Warehouse Directory property (hive.metastore.warehouse.dir) , Cloudbreak automatically suggests a location within the bucket. For example, if the directory that you specified is  my-test-dir  then the suggested location will be  my-test-adls-account.azuredatalakestore.net/my-test-dir/apps/hive/warehouse . You may optionally update this path.          Cloudbreak automatically creates this directory structure in your bucket.       Next: Configure Access to WASB", 
            "title": "Configure ADLS storage locations"
        }, 
        {
            "location": "/azure-data-wasb/index.html", 
            "text": "Configuring access to WASB\n\n\nHortonworks Data Platform (HDP) supports reading and writing block blobs and page blobs from and to \nWindows Azure Storage Blob (WASB)\n object store, as well as reading and writing files stored in an \nAzure Data Lake Storage (ADLS)\n account.  \n\n\nWindows Azure Storage Blob (WASB) is an object store service available on Azure. WASB is not supported as a default file system, but access to data in WASB is possible via the wasb connector.\n\n\nThese steps assume that you are using an HDP version that supports the wasb cloud storage connector (HDP 2.6.1 or newer).  \n\n\nPrerequisites\n\n\nIf you want to use Windows Azure Storage Blob to store your data, you must enable Azure subscription for Blob Storage, and then create a \nstorage account\n.\n\n\nHere is an example of how to create a a storage account:\n\n\n\n\nConfigure access to WASB\n\n\nIn order to access data stored in your Azure blob storage account, you must obtain your access key from your storage account settings, and then provide the storage account name and its corresponding access key when creating a cluster.     \n\n\nSteps\n\n\n\n\n\n\nObtain your storage account name and storage access key from the \nAccess keys\n page in your storage account settings:\n\n\n\n\n\n\n\n\nIn Cloudbreak web UI, on the advanced \nCloud Storage\n page of the create a cluster wizard,  select \nUse existing WASB storage\n.  \n\n\n\n\n\n\nProvide the \nStorage Account Name\n and \nAccess Key\n parameters obtained in the previous step.   \n\n\n\n\n\n\nOnce your cluster is in the running state, you will be able to access the Azure blob storage account from the cluster nodes. \n\n\nTest access to WASB\n\n\nTo tests access to WASB, SSH to a cluster node and run a few hadoop fs shell commands against your existing WASB account.\n\n\nWASB access path syntax is:\n\n\nwasb://\ncontainer_name\n@\nstorage_account_name\n.blob.core.windows.net/\ndir/file\n\n\n\nFor example, to access a file called \"testfile\" located in a directory called \"testdir\", stored in the container called \"testcontainer\" on the account called \"hortonworks\", the URL is:\n\n\nwasb://testcontainer@hortonworks.blob.core.windows.net/testdir/testfile\n\n\n\nYou can also use \"wasbs\" prefix to utilize SSL-encrypted HTTPS access:\n\n\nwasbs://\n@\n.blob.core.windows.net/dir/file\n\n\n\nThe following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\" and a container named \"mycontainer\":\n\n\nhadoop fs -ls wasb://mycontainer@myaccount.blob.core.windows.net/\n\nhadoop fs -mkdir wasb://mycontainer@myaccount.blob.core.windows.net/testDir\n\nhadoop fs -put testFile wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\n\nhadoop fs -cat wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\ntest file content\n\n\n\nFor more information about configuring the WASB connector and working with data stored in WASB, refer to \nCloud Data Access\n documentation.\n\n\nRelated links\n \n\n\nCloud Data Access\n (Hortonworks) \n\n\nCreate a storage account\n (External)   \n\n\nConfigure WASB storage locations\n\n\nAfter configuring access to WASB, you can optionally use that WASB storage account as a base storage location; this storage location is mainly for the Hive Warehouse Directory (used for storing the table data for managed tables).   \n\n\nSteps\n\n\n\n\nWhen creating a cluster, on the \nCloud Storage\n page in the advanced cluster wizard view, select \nUse existing WASB storage\n and select the instance profile to use, as described in \nConfigure access to WASB\n.    \n\n\nUnder \nStorage Locations\n, enable \nConfigure Storage Locations\n by clicking the \n button.   \n\n\n\n\nProvide your existing directory name under \nBase Storage Location\n. \n\n\n\n\nMake sure that the container exists within the account.\n\n\n\n\n\n\n\n\nUnder \nPath for Hive Warehouse Directory property (hive.metastore.warehouse.dir)\n, Cloudbreak automatically suggests a location within the bucket. For example, if the directory that you specified is \nmy-test-container\n then the suggested location will be \nmy-test-container@my-wasb-account.blob.core.windows.net/apps/hive/warehouse\n. You may optionally update this path.  \n\n\nCloudbreak automatically creates this directory structure in your bucket.", 
            "title": "Configure access to WASB"
        }, 
        {
            "location": "/azure-data-wasb/index.html#configuring-access-to-wasb", 
            "text": "Hortonworks Data Platform (HDP) supports reading and writing block blobs and page blobs from and to  Windows Azure Storage Blob (WASB)  object store, as well as reading and writing files stored in an  Azure Data Lake Storage (ADLS)  account.    Windows Azure Storage Blob (WASB) is an object store service available on Azure. WASB is not supported as a default file system, but access to data in WASB is possible via the wasb connector.  These steps assume that you are using an HDP version that supports the wasb cloud storage connector (HDP 2.6.1 or newer).", 
            "title": "Configuring access to WASB"
        }, 
        {
            "location": "/azure-data-wasb/index.html#prerequisites", 
            "text": "If you want to use Windows Azure Storage Blob to store your data, you must enable Azure subscription for Blob Storage, and then create a  storage account .  Here is an example of how to create a a storage account:", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/azure-data-wasb/index.html#configure-access-to-wasb", 
            "text": "In order to access data stored in your Azure blob storage account, you must obtain your access key from your storage account settings, and then provide the storage account name and its corresponding access key when creating a cluster.       Steps    Obtain your storage account name and storage access key from the  Access keys  page in your storage account settings:     In Cloudbreak web UI, on the advanced  Cloud Storage  page of the create a cluster wizard,  select  Use existing WASB storage .      Provide the  Storage Account Name  and  Access Key  parameters obtained in the previous step.       Once your cluster is in the running state, you will be able to access the Azure blob storage account from the cluster nodes.", 
            "title": "Configure access to WASB"
        }, 
        {
            "location": "/azure-data-wasb/index.html#test-access-to-wasb", 
            "text": "To tests access to WASB, SSH to a cluster node and run a few hadoop fs shell commands against your existing WASB account.  WASB access path syntax is:  wasb:// container_name @ storage_account_name .blob.core.windows.net/ dir/file  For example, to access a file called \"testfile\" located in a directory called \"testdir\", stored in the container called \"testcontainer\" on the account called \"hortonworks\", the URL is:  wasb://testcontainer@hortonworks.blob.core.windows.net/testdir/testfile  You can also use \"wasbs\" prefix to utilize SSL-encrypted HTTPS access:  wasbs:// @ .blob.core.windows.net/dir/file  The following Hadoop FileSystem shell commands demonstrate access to a storage account named \"myaccount\" and a container named \"mycontainer\":  hadoop fs -ls wasb://mycontainer@myaccount.blob.core.windows.net/\n\nhadoop fs -mkdir wasb://mycontainer@myaccount.blob.core.windows.net/testDir\n\nhadoop fs -put testFile wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\n\nhadoop fs -cat wasb://mycontainer@myaccount.blob.core.windows.net/testDir/testFile\ntest file content  For more information about configuring the WASB connector and working with data stored in WASB, refer to  Cloud Data Access  documentation.  Related links    Cloud Data Access  (Hortonworks)   Create a storage account  (External)", 
            "title": "Test access to WASB"
        }, 
        {
            "location": "/azure-data-wasb/index.html#configure-wasb-storage-locations", 
            "text": "After configuring access to WASB, you can optionally use that WASB storage account as a base storage location; this storage location is mainly for the Hive Warehouse Directory (used for storing the table data for managed tables).     Steps   When creating a cluster, on the  Cloud Storage  page in the advanced cluster wizard view, select  Use existing WASB storage  and select the instance profile to use, as described in  Configure access to WASB .      Under  Storage Locations , enable  Configure Storage Locations  by clicking the   button.      Provide your existing directory name under  Base Storage Location .    Make sure that the container exists within the account.     Under  Path for Hive Warehouse Directory property (hive.metastore.warehouse.dir) , Cloudbreak automatically suggests a location within the bucket. For example, if the directory that you specified is  my-test-container  then the suggested location will be  my-test-container@my-wasb-account.blob.core.windows.net/apps/hive/warehouse . You may optionally update this path.    Cloudbreak automatically creates this directory structure in your bucket.", 
            "title": "Configure WASB storage locations"
        }, 
        {
            "location": "/gcp-pre/index.html", 
            "text": "Prerequisites on GCP\n\n\nBefore launching Cloudbreak on GCP, you must meet the following prerequisites.\n\n\nGCP account\n\n\nIn order to launch Cloudbreak on GCP, you must log in to your GCP account. If you don't have an account, you can create one at \nhttps://console.cloud.google.com\n.\n\n\nOnce you log in to your GCP account, you must either create a project or use an existing project. \n\n\nService account\n\n\nIn order to launch clusters on GCP via Cloudbreak, you must have a service account that Cloudbreak can use to create resources. In addition, you must also have a JSON key associated with the account. \n\n\nThe service account must have the following roles are enabled:\n\n\n\n\nCompute Engine \n Compute Image User   \n\n\nCompute Engine \n Compute Instance Admin (v1)  \n\n\nCompute Engine \n Compute Network Admin  \n\n\nCompute Engine \n Compute Security Admin  \n\n\nStorage \n Storage Admin \n\n\n\n\nA user with an \"Owner\" role can assign roles to new and existing service accounts from \nIAM \n admin\n \n \nService accounts\n, as presented in the following screenshots: \n\n\n \n\n\n \n\n\nFor more information on creating a service account and generating a JSON key, refer to \nGCP documentation\n. \n\n\nRelated links\n\n\nService account credentials\n (External) \n\n\nSSH key pair\n\n\nGenerate a new SSH key pair\n or use an existing SSH key pair. You will be required to provide it when launching the VM. \n\n\nVirtual network\n\n\nYou must have a virtual network configured on your cloud provider.\n\n\nSecurity group\n\n\nPorts 22 (SSH), 80 (HTTPS), and 443 (HTTPS) must be open on the security group\n\n\nRegion and zone\n\n\nDecide in which region and zone you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions \nsupported by GCP\n.  \n\n\nClusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it. \n\n\nRelated links\n\n\nRegions and zones\n (External) \n\n\n\n\nNext: Launch Cloudbreak", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/gcp-pre/index.html#prerequisites-on-gcp", 
            "text": "Before launching Cloudbreak on GCP, you must meet the following prerequisites.", 
            "title": "Prerequisites on GCP"
        }, 
        {
            "location": "/gcp-pre/index.html#gcp-account", 
            "text": "In order to launch Cloudbreak on GCP, you must log in to your GCP account. If you don't have an account, you can create one at  https://console.cloud.google.com .  Once you log in to your GCP account, you must either create a project or use an existing project.", 
            "title": "GCP account"
        }, 
        {
            "location": "/gcp-pre/index.html#service-account", 
            "text": "In order to launch clusters on GCP via Cloudbreak, you must have a service account that Cloudbreak can use to create resources. In addition, you must also have a JSON key associated with the account.   The service account must have the following roles are enabled:   Compute Engine   Compute Image User     Compute Engine   Compute Instance Admin (v1)    Compute Engine   Compute Network Admin    Compute Engine   Compute Security Admin    Storage   Storage Admin    A user with an \"Owner\" role can assign roles to new and existing service accounts from  IAM   admin     Service accounts , as presented in the following screenshots:         For more information on creating a service account and generating a JSON key, refer to  GCP documentation .   Related links  Service account credentials  (External)", 
            "title": "Service account"
        }, 
        {
            "location": "/gcp-pre/index.html#ssh-key-pair", 
            "text": "Generate a new SSH key pair  or use an existing SSH key pair. You will be required to provide it when launching the VM.", 
            "title": "SSH key pair"
        }, 
        {
            "location": "/gcp-pre/index.html#virtual-network", 
            "text": "You must have a virtual network configured on your cloud provider.", 
            "title": "Virtual network"
        }, 
        {
            "location": "/gcp-pre/index.html#security-group", 
            "text": "Ports 22 (SSH), 80 (HTTPS), and 443 (HTTPS) must be open on the security group", 
            "title": "Security group"
        }, 
        {
            "location": "/gcp-pre/index.html#region-and-zone", 
            "text": "Decide in which region and zone you would like to launch Cloudbreak. You can launch Cloudbreak and provision your clusters in all regions  supported by GCP .    Clusters created via Cloudbreak can be in the same or different region as Cloudbreak; when you launch a cluster, you select the region in which to launch it.   Related links  Regions and zones  (External)    Next: Launch Cloudbreak", 
            "title": "Region and zone"
        }, 
        {
            "location": "/gcp-launch/index.html", 
            "text": "Launching Cloudbreak on GCP\n\n\nThese steps describe how to launch Cloudbreak on GCP for production. \nBefore launching Cloudbreak on Google Cloud, review and meet the \nprerequisites\n. Next, follow the steps below.  \n\n\nVM requirements\n\n\nTo launch the Cloudbreak deployer and install the Cloudbreak application, you must have an existing VM. \n\n\nSystem requirements\n\n\nYour system must meet the following requirements:\n\n\n\n\nMinimum VM requirements: 16GB RAM, 40GB disk, 4 cores\n\n\nSupported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)\n\n\n\n\n\n\nYou can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.\n\n\n\n\nRoot access\n\n\nEvery command must be executed as root. In order to get root privileges execute:\n\n\nsudo -i\n\n\n\nSystem updates\n\n\nEnsure that your system is up-to-date by executing:\n\n\nyum -y update\n\n\n\nReboot it if necessary.\n\n\nInstall iptables\n\n\nPerform these steps to install and configure iptables.\n\n\nSteps\n \n\n\n\n\n\n\nInstall iptables-services:\n\n\nyum -y install net-tools ntp wget lsof unzip tar iptables-services\nsystemctl enable ntpd \n systemctl start ntpd\nsystemctl disable firewalld \n systemctl stop firewalld\n\n\n\n\nWithout iptables-services installed the \niptables save\n command will not be available.\n\n\n\n\n\n\n\n\nConfigure permissive iptables on your machine:\n\n\niptables --flush INPUT \n \\\niptables --flush FORWARD \n \\\nservice iptables save\n\n\n\n\n\n\nDisable SELINUX\n\n\nPerform these steps to disable SELINUX.\n\n\nSteps\n \n\n\n\n\n\n\nDisable SELINUX:\n\n\nsetenforce 0\nsed -i 's/SELINUX=enforcing/SELINUX=disabled/g' \n/etc/selinux/config\n\n\n\n\n\n\nRun the following command to ensure that SELinux is not turned on afterwards: \n\n\ngetenforce\n\n\n\n\n\n\nThe command should return \"Disabled\".     \n\n\n\n\n\n\nInstall Docker\n\n\nPerform these steps to install Docker. The minimum Docker version is 1.13.1. If you are using an older image that comes with an older Docker version, upgrade Docker to 1.13.1 or newer. \n\n\nSteps\n    \n\n\n\n\n\n\nInstall Docker service:\n\n\nyum install -y docker\nsystemctl start docker\nsystemctl enable docker\n\n\n\n\n\n\nCheck the Docker Logging Driver configuration:\n\n\ndocker info | grep \"Logging Driver\"\n\n\n\n\n\n\nIf it is set to \nLogging Driver: journald\n, you must  set it to \"json-file\" instead. To do that:\n\n\n\n\n\n\nOpen the \ndocker\n file for editing:\n\n\nvi /etc/sysconfig/docker\n  \n\n\n\n\n\n\nEdit the following part of the file so that it looks like below (showing \nlog-driver=json-file\n):\n\n\n# Modify these options if you want to change the way the docker daemon runs\nOPTIONS='--selinux-enabled --log-driver=json-file --signature-verification=false'\n     \n\n\n\n\n\n\nRestart Docker:\n\n\nsystemctl restart docker\nsystemctl status docker\n\n\n\n\n\n\n\n\n\n\nInstall Cloudbreak on a VM\n\n\nInstall Cloudbreak using the following steps.\n\n\nSteps\n\n\n\n\n\n\nInstall the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:\n\n\nyum -y install unzip tar\ncurl -Ls public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_2.7.0_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version\n\n\nOnce the Cloudbreak deployer is installed, you can set up the Cloudbreak application.\n\n\n\n\n\n\nCreate a Cloudbreak deployment directory and navigate to it:\n\n\nmkdir cloudbreak-deployment\ncd cloudbreak-deployment\n\n\n\n\n\n\nIn the directory, create a file called \nProfile\n with the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL\nexport PUBLIC_IP=MY_VM_IP\n\n\nFor example:\n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com\nexport PUBLIC_IP=172.26.231.100\n\n\nYou will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.\n\n\nYou should set the CLOUDBREAK_SMTP_SENDER_USERNAME variable to the username you use to authenticate to your SMTP server. You should set the CLOUDBREAK_SMTP_SENDER_PASSWORD variable to the password you use to authenticate to your SMTP server.\n\n\n\n\n\n\nGenerate configurations by executing:\n\n\nrm *.yml\ncbd generate\n   \n\n\nThe cbd start command includes the cbd generate command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.  \n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.   \n\n\n\n\n\n\n\n\nStart the Cloudbreak application by using the following commands:\n\n\ncbd pull-parallel\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Cloudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\n\n\nIf you encounter errors during \ncbd start\n, refer to \nToubleshooting\n.  \n\n\n\n\n\n\n\n\nNext, check Cloudbreak application logs:\n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nRelated links\n\n\nToubleshooting\n   \n\n\nAccess Cloudbreak web UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n \n\n\n\n\n\n\nYou can log into the Cloudbreak application at \nhttps://IP_Address\n. For example \nhttps://34.212.141.253\n. You may use \ncbd start\n to obtain the login information. Alternatively, you can obtain the VM's IP address from your cloud provider console. \n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nLog in to the Cloudbreak web UI using the credentials that you configured in your \nProfile\n file:\n\n\n\n\nThe username is the \nUAA_DEFAULT_USER_EMAIL\n     \n\n\nThe password is the \nUAA_DEFAULT_USER_PW\n \n\n\n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n  \n\n\n\n\n\n\nConfigure external Cloudbreak database\n\n\nBy default, Cloudbreak uses an embedded PostgreSQL database to persist data related to Cloudbreak configuration, setup, and so on. For a production Cloudbreak deployment, you must \nconfigure an external database\n.\n\n\nRelated links\n\n\nConfigure an external database\n  \n\n\nCreate Cloudbreak credential\n\n\nCloudbreak works by connecting your GCP account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a \nCloudbreak credential\n.  \n\n\nPrerequisites\n\n\nIn order to launch clusters on GCP via Cloudbreak, you must have a service account that Cloudbreak can use to create resources. If you do not have one, refer to \nPrerequisites: Service account\n.  \n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane. \n\n\n\n\n\n\nClick \nCreate Credential\n. \n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Google Cloud Platform\":\n\n\n  \n\n\n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKey type\n\n\nSelect JSON or P12. Since activating service accounts with P12 private keys has been deprecated in the Cloud SDK, we recommend using JSON.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nProject Id\n\n\n(Only required for P12 key type) Enter the project ID. You can obtain it from your GCP account by clicking on the name of your project at the top of the page and copying the \nID\n.\n\n\n\n\n\n\nService Account Email Address\n\n\n(Only required for P12 key type) \"Service account ID\" value for your service account created in prerequisites. You can find it on GCP at \nIAM \n Admin\n \n \nService accounts\n.\n\n\n\n\n\n\nService Account Private Key\n\n\nUpload the key that you created in the prerequisites when creating a service account.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to create clusters. \n\n\n\n\n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on GCP"
        }, 
        {
            "location": "/gcp-launch/index.html#launching-cloudbreak-on-gcp", 
            "text": "These steps describe how to launch Cloudbreak on GCP for production. \nBefore launching Cloudbreak on Google Cloud, review and meet the  prerequisites . Next, follow the steps below.", 
            "title": "Launching Cloudbreak on GCP"
        }, 
        {
            "location": "/gcp-launch/index.html#vm-requirements", 
            "text": "To launch the Cloudbreak deployer and install the Cloudbreak application, you must have an existing VM.", 
            "title": "VM requirements"
        }, 
        {
            "location": "/gcp-launch/index.html#system-requirements", 
            "text": "Your system must meet the following requirements:   Minimum VM requirements: 16GB RAM, 40GB disk, 4 cores  Supported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)    You can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.", 
            "title": "System requirements"
        }, 
        {
            "location": "/gcp-launch/index.html#root-access", 
            "text": "Every command must be executed as root. In order to get root privileges execute:  sudo -i", 
            "title": "Root access"
        }, 
        {
            "location": "/gcp-launch/index.html#system-updates", 
            "text": "Ensure that your system is up-to-date by executing:  yum -y update  Reboot it if necessary.", 
            "title": "System updates"
        }, 
        {
            "location": "/gcp-launch/index.html#install-iptables", 
            "text": "Perform these steps to install and configure iptables.  Steps      Install iptables-services:  yum -y install net-tools ntp wget lsof unzip tar iptables-services\nsystemctl enable ntpd   systemctl start ntpd\nsystemctl disable firewalld   systemctl stop firewalld   Without iptables-services installed the  iptables save  command will not be available.     Configure permissive iptables on your machine:  iptables --flush INPUT   \\\niptables --flush FORWARD   \\\nservice iptables save", 
            "title": "Install iptables"
        }, 
        {
            "location": "/gcp-launch/index.html#disable-selinux", 
            "text": "Perform these steps to disable SELINUX.  Steps      Disable SELINUX:  setenforce 0\nsed -i 's/SELINUX=enforcing/SELINUX=disabled/g' \n/etc/selinux/config    Run the following command to ensure that SELinux is not turned on afterwards:   getenforce    The command should return \"Disabled\".", 
            "title": "Disable SELINUX"
        }, 
        {
            "location": "/gcp-launch/index.html#install-docker", 
            "text": "Perform these steps to install Docker. The minimum Docker version is 1.13.1. If you are using an older image that comes with an older Docker version, upgrade Docker to 1.13.1 or newer.   Steps         Install Docker service:  yum install -y docker\nsystemctl start docker\nsystemctl enable docker    Check the Docker Logging Driver configuration:  docker info | grep \"Logging Driver\"    If it is set to  Logging Driver: journald , you must  set it to \"json-file\" instead. To do that:    Open the  docker  file for editing:  vi /etc/sysconfig/docker       Edit the following part of the file so that it looks like below (showing  log-driver=json-file ):  # Modify these options if you want to change the way the docker daemon runs\nOPTIONS='--selinux-enabled --log-driver=json-file --signature-verification=false'          Restart Docker:  systemctl restart docker\nsystemctl status docker", 
            "title": "Install Docker"
        }, 
        {
            "location": "/gcp-launch/index.html#install-cloudbreak-on-a-vm", 
            "text": "Install Cloudbreak using the following steps.  Steps    Install the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:  yum -y install unzip tar\ncurl -Ls public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_2.7.0_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version  Once the Cloudbreak deployer is installed, you can set up the Cloudbreak application.    Create a Cloudbreak deployment directory and navigate to it:  mkdir cloudbreak-deployment\ncd cloudbreak-deployment    In the directory, create a file called  Profile  with the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL\nexport PUBLIC_IP=MY_VM_IP  For example:  export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com\nexport PUBLIC_IP=172.26.231.100  You will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.  You should set the CLOUDBREAK_SMTP_SENDER_USERNAME variable to the username you use to authenticate to your SMTP server. You should set the CLOUDBREAK_SMTP_SENDER_PASSWORD variable to the password you use to authenticate to your SMTP server.    Generate configurations by executing:  rm *.yml\ncbd generate      The cbd start command includes the cbd generate command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.    Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.        Start the Cloudbreak application by using the following commands:  cbd pull-parallel\ncbd start  This will start the Docker containers and initialize the application. The first time you start the Cloudbreak app, the process will take longer than usual due to the download of all the necessary docker images.   If you encounter errors during  cbd start , refer to  Toubleshooting .       Next, check Cloudbreak application logs:  cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.    Related links  Toubleshooting", 
            "title": "Install Cloudbreak on a VM"
        }, 
        {
            "location": "/gcp-launch/index.html#access-cloudbreak-web-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps      You can log into the Cloudbreak application at  https://IP_Address . For example  https://34.212.141.253 . You may use  cbd start  to obtain the login information. Alternatively, you can obtain the VM's IP address from your cloud provider console.     Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...       The login page is displayed:        Log in to the Cloudbreak web UI using the credentials that you configured in your  Profile  file:   The username is the  UAA_DEFAULT_USER_EMAIL        The password is the  UAA_DEFAULT_USER_PW       Upon a successful login, you are redirected to the dashboard:", 
            "title": "Access Cloudbreak web UI"
        }, 
        {
            "location": "/gcp-launch/index.html#configure-external-cloudbreak-database", 
            "text": "By default, Cloudbreak uses an embedded PostgreSQL database to persist data related to Cloudbreak configuration, setup, and so on. For a production Cloudbreak deployment, you must  configure an external database .  Related links  Configure an external database", 
            "title": "Configure external Cloudbreak database"
        }, 
        {
            "location": "/gcp-launch/index.html#create-cloudbreak-credential", 
            "text": "Cloudbreak works by connecting your GCP account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a  Cloudbreak credential .    Prerequisites  In order to launch clusters on GCP via Cloudbreak, you must have a service account that Cloudbreak can use to create resources. If you do not have one, refer to  Prerequisites: Service account .    Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.     Click  Create Credential .     Under  Cloud provider , select \"Google Cloud Platform\":        Provide the following information:     Parameter  Description      Key type  Select JSON or P12. Since activating service accounts with P12 private keys has been deprecated in the Cloud SDK, we recommend using JSON.    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    Project Id  (Only required for P12 key type) Enter the project ID. You can obtain it from your GCP account by clicking on the name of your project at the top of the page and copying the  ID .    Service Account Email Address  (Only required for P12 key type) \"Service account ID\" value for your service account created in prerequisites. You can find it on GCP at  IAM   Admin     Service accounts .    Service Account Private Key  Upload the key that you created in the prerequisites when creating a service account.       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to create clusters.      Next: Create a Cluster", 
            "title": "Create Cloudbreak credential"
        }, 
        {
            "location": "/gcp-create/index.html", 
            "text": "Creating a cluster on GCP\n\n\nUse these steps to create a cluster.\n\n\n\n\nTroubleshooting cluster creation\n\n\nIf you experience problems during cluster creation, refer to \nTroubleshooting cluster creation\n.\n\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick the \nCreate Cluster\n button and the \nCreate Cluster\n wizard is displayed.\n\n    By default, \nBasic\n view is displayed. To view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced cluster options\n.\n\n\n \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, specify the following general parameters for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nChoose a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nRegion\n\n\nSelect the GCP region in which you would like to launch your cluster. For information on available GCP regions, refer to \nGCP documentation\n.\n\n\n\n\n\n\nPlatform Version\n\n\nChoose the HDP or HDF version to use for this cluster. Blueprints available for this platform version will be populated under \"Cluster Type\" below. If you selected the \nHDF\n platform, refer to \nCreating HDF clusters\n for HDF cluster configuration tips.\n\n\n\n\n\n\nCluster Type\n\n\nChoose one of the default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to \nUsing custom blueprints\n.\n\n\n\n\n\n\nFlex Subscription\n\n\nThis option will appear if you have configured your deployment for a \nflex support subscription\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, for each host group provide the following information to define your cluster nodes and attached storage. \n\n\nTo edit this section, click on the \n. When done editing, click on the \n to save the changes.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server by clicking the \n button. The \"Instance Count\" for that host group must be set to \"1\". If you are using one of the default blueprints, this is set by default.\n\n\n\n\n\n\nInstance Type\n\n\nSelect a VM instance type. For information about instance types on GCP refer to \nGCP documentation\n.\n\n\n\n\n\n\nInstance Count\n\n\nEnter the number of instances of a given type. Default is 1.\n\n\n\n\n\n\nStorage Type\n\n\nSelect the volume type. The options are:\nStandard persistent disks (HDD)\nSolid-state persistent disks (SSD)\n For more information about these options refer to \nGCP documentation\n.\n\n\n\n\n\n\nAttached Volumes Per Instance\n\n\nEnter the number of volumes attached per instance. Default is 1.\n\n\n\n\n\n\nVolume Size\n\n\nEnter the size in GBs for each volume. Default is 100.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nGateway Configuration\n page, you can access gateway configuration options. \n\n\nWhen creating a cluster, Cloudbreak installs and configures a gateway (powered by Apache Knox) to protect access to the cluster resources. By default, the gateway is enabled for Ambari; You can optionally enable it for other cluster services. \n\n\nFor more information, refer to \nConfiguring the Gateway\n documentation.\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following to specify the networking resources that will be used for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Network\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.\n\n\n\n\n\n\nSelect Subnet\n\n\nSelect the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.\n\n\n\n\n\n\nSubnet (CIDR)\n\n\nIf you selected to create a new subnet, you must define a valid \nCIDR\n for the subnet. Default is 10.0.0.0/16.\n\n\n\n\n\n\n\n\n\n\nCloudbreak uses public IP addresses when communicating with cluster nodes.  \n\n\n\n\n\n\n\n\nDefine security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:\n\n\n\n\nExisting security groups are only available for an existing network. \n\n\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNew Security Group\n\n\n(Default) Creates a new security group with the rules that you defined:\nA set of \ndefault rules\n is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied. \nYou may open ports by defining the CIDR, entering port range, selecting protocol and clicking \n+\n.\nYou may delete default or previously added rules using the delete icon.\nIf you don't want to use security group, remove the default rules.\n\n\n\n\n\n\nExisting Security Groups\n\n\nAllows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.\n\n\n\n\n\n\n\n\nThe default experience of creating network resources such as network, subnet and security group automatically is provided for convenience. We strongly recommend you review these options and for production cluster deployments leverage your existing network resources that you have defined and validated to meet your enterprise requirements. For more information, refer to \nRestricting inbound access from Cloudbreak to cluster\n. \n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nPassword\n\n\nYou can log in to the Ambari UI using this password.\n\n\n\n\n\n\nConfirm Password\n\n\nConfirm the password.\n\n\n\n\n\n\nNew SSH public key\n\n\nCheck this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.\n\n\n\n\n\n\nExisting SSH public key\n\n\nSelect an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nRelated Links\n\n\nFlex support subscription\n\n\nUsing custom blueprints\n \n\n\nDefault cluster security groups\n\n\nTroubleshooting cluster creation\n    \n\n\nCIDR\n (External) \n\n\nCloud locations\n (External)\n\n\nMachine types\n (External)     \n\n\nAdvanced options\n\n\nClick on \nAdvanced\n to view and enter additional configuration options\n\n\nAvailability zone\n\n\nChoose one of the availability zones within the selected region. \n\n\nEnable lifetime management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes). \n\n\nTags\n\n\nYou can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. \n\n\nBy default, the following tags are created:\n\n\n\n\n\n\n\n\nTag\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncb-version\n\n\nCloudbreak version\n\n\n\n\n\n\nOwner\n\n\nYour Cloudbreak admin email.\n\n\n\n\n\n\ncb-account-name\n\n\nYour automatically generated Cloudbreak account name stored in the identity server.\n\n\n\n\n\n\ncb-user-name\n\n\nYour Cloudbreak admin email.\n\n\n\n\n\n\n\n\nFor more information, refer to \nTagging resources\n.\n\n\nRelated links\n    \n\n\nTagging resources\n \n\n\nChoose image catalog\n\n\nBy default, \nChoose Image Catalog\n is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to \nUsing custom images\n.\n\n\nRelated links\n   \n\n\nUsing custom images\n  \n\n\nChoose image type\n\n\nCloudbreak supports the following types of images for launching clusters:\n\n\n\n\n\n\n\n\nImage type\n\n\nDescription\n\n\nDefault images provided\n\n\nSupport for custom images\n\n\n\n\n\n\n\n\n\n\nPrewarmed Image\n\n\nBy default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP/HDF. The Ambari and HDP/HDF version used by prewarmed images cannot be customized.\n\n\nYes\n\n\nNo\n\n\n\n\n\n\nBase Image\n\n\nBase images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP/HDF software.\n\n\nYes\n\n\nYes\n\n\n\n\n\n\n\n\nBy default, Cloudbreak uses the included default \nprewarmed images\n, which include the operating system, as well as\nAmbari and HDP/HDF packages installed. You can optionally select the \nbase image\n option if you would like to:\n\n\n\n\nUse an Ambari and HDP/HDF versions different than what the prewarmed image includes and/or  \n\n\nChoose a previously created custom base image\n\n\n\n\nChoose image\n\n\nIf under \nChoose image catalog\n, you selected a custom image catalog, under \nChoose Image\n you can select an image from that catalog. For complete instructions, refer to \nUsing custom images\n. \n\n\nIf you are trying to customize Ambari and HDP/HDF versions, you can ignore the \nChoose Image\n option; in this case default base image is used.\n\n\nAmbari repo specification\n\n\nIf you would like to use a custom Ambari version, provide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nVersion\n\n\nEnter Ambari version.\n\n\n2.6.1.3\n\n\n\n\n\n\nRepo Url\n\n\nProvide a URL to the Ambari version repo that you would like to use.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.3\n\n\n\n\n\n\nRepo Gpg Key Url\n\n\nProvide a URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins\n\n\n\n\n\n\n\n\nHDP/HDF repo specification\n\n\nIf you would like to use a custom HDP or HDF version, provide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nThis is populated by default based on the \"Platform Version\" parameter.\n\n\nHDP\n\n\n\n\n\n\nVersion\n\n\nThis is populated by default based on the \"Platform Version\" parameter.\n\n\n2.6\n\n\n\n\n\n\nOS\n\n\nOperating system.\n\n\ncentos7 (Azure, GCP, OpenStack) or amazonlinux (AWS)\n\n\n\n\n\n\nRepository Version\n\n\nEnter repository version.\n\n\n2.6.4.0-91\n\n\n\n\n\n\nVersion Definition File\n\n\nEnter the URL of the VDF file.\n\n\nhttp://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.6.4.0/HDP-2.6.4.0-91.xml\n\n\n\n\n\n\n(HDF only) MPack Url\n\n\n(HDF only) Provide MPack URL.\n\n\nhttp://public-repo-1.hortonworks.com/HDF/centos7/3.x/updates/3.1.1.0/tars/hdf_ambari_mp/hdf-ambari-mpack-3.1.1.0-35.tar.gz\n\n\n\n\n\n\nEnable Ambari Server to download and install GPL Licensed LZO packages?\n\n\n(Optional, only available if using Ambari 2.6.1.0 or newer) Use this option to enable LZO compression in your HDP/HDF cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to \nEnabling LZO\n.\n\n\n\n\n\n\n\n\n\n\nIf you choose to use a base image with custom Ambari and/or HDP/HDF version, Cloudbreak validates the information entered. When Cloudbreak detects that the information entered is incorrect, it displays a warning marked with the \n sign. You should review all the warnings before proceeding and make sure that the information that you entered is correct. If you choose to proceed in spite of the warnings, check \"Ignore repository warnings\".  \n\n\nRelated links\n    \n\n\nUsing custom images\n      \n\n\nRoot volume size\n\n\nUse this option to increase the root volume size. Default is 50 GB. This option is useful if your custom image requires more space than the default 50 GB. \n\n\nUse preemptible instances\n\n\nCheck this option to use Google Cloud preemptive VM instances as your cluster nodes. To learn more, refer to \nGoogle Cloud documentation\n.    \n\n\nNote that: \n\n\n\n\nWe recommend not using preemptible instances for any host group that includes Ambari server components.  \n\n\nIf you choose to use preemptible instances for a given host group when creating your cluster, any nodes that you add to that host group (during cluster creation or later) will be using preemptible instances.   \n\n\nIf you decide not to use preemptible instances when creating your cluster, any nodes that you add to your host group (during cluster creation or later) will be using standard on-demand instances.     \n\n\nOnce someone outbids you, the preemptible instances are taken away, removing the nodes from the cluster. \n\n\nIf the preemptible instances are not available right away, creating a cluster will take longer than usual. \n\n\n\n\nCloud storage\n\n\nIf you would like to access Google Cloud Storage (GCS) from your cluster, you must configure access as described in \nAccessing data on GCS\n. \n\n\nRelated links\n\n\nAccessing data on GCS\n     \n\n\nRecipes\n\n\nThis option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to \nUsing custom scripts (recipes)\n. \n\n\nRelated links\n    \n\n\nUsing custom scripts (recipes)\n \n\n\nManagement packs\n\n\nThis option allows you to select previously uploaded management packs. For more information on management packs, refer to \nUsing management packs\n. \n\n\nRelated links\n    \n\n\nUsing management packs\n  \n\n\nExternal sources\n\n\nYou can register external sources with Cloudbreak, and then select and attach them during cluster create. To register external sources with Cloudbreak, refer to:\n\n\n\n\nUsing an external authentication source\n    \n\n\nUsing an external database\n  \n\n\nRegister a proxy\n  \n\n\n\n\nCustom properties\n\n\nThis option allows you to set custom properties based on the template defined in your custom blueprint. For more information, refer to \nSet custom properties\n. \n\n\nRelated links\n    \n\n\nSet custom properties\n   \n\n\nSingle sign-on (SSO)\n\n\nThis option allows you to configure the gateway to be the SSO identity provider. \n\n\n\n\nThis option is technical preview. \n\n\n\n\nFor more information, refer to \nConfiguring the Gateway\n documentation.  \n\n\nAmbari server master key\n\n\nThe Ambari server master key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.  \n\n\nEnable Kerberos security\n\n\nSelect this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to \nEnabling Kerberos security\n. \n\n\nRelated links\n    \n\n\nEnabling Kerberos security\n \n\n\nPreemptible VM instances\n (External) \n\n\nStorage options\n (External)  \n\n\n\n\nNext: Configure Access to GCS", 
            "title": "Create a cluster"
        }, 
        {
            "location": "/gcp-create/index.html#creating-a-cluster-on-gcp", 
            "text": "Use these steps to create a cluster.   Troubleshooting cluster creation  If you experience problems during cluster creation, refer to  Troubleshooting cluster creation .  Steps    Log in to the Cloudbreak UI.    Click the  Create Cluster  button and the  Create Cluster  wizard is displayed. \n    By default,  Basic  view is displayed. To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced cluster options .       On the  General Configuration  page, specify the following general parameters for your cluster:     Parameter  Description      Select Credential  Choose a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.    Region  Select the GCP region in which you would like to launch your cluster. For information on available GCP regions, refer to  GCP documentation .    Platform Version  Choose the HDP or HDF version to use for this cluster. Blueprints available for this platform version will be populated under \"Cluster Type\" below. If you selected the  HDF  platform, refer to  Creating HDF clusters  for HDF cluster configuration tips.    Cluster Type  Choose one of the default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to  Using custom blueprints .    Flex Subscription  This option will appear if you have configured your deployment for a  flex support subscription .       On the  Hardware and Storage  page, for each host group provide the following information to define your cluster nodes and attached storage.   To edit this section, click on the  . When done editing, click on the   to save the changes.     Parameter  Description      Ambari Server  You must select one node for Ambari Server by clicking the   button. The \"Instance Count\" for that host group must be set to \"1\". If you are using one of the default blueprints, this is set by default.    Instance Type  Select a VM instance type. For information about instance types on GCP refer to  GCP documentation .    Instance Count  Enter the number of instances of a given type. Default is 1.    Storage Type  Select the volume type. The options are: Standard persistent disks (HDD) Solid-state persistent disks (SSD)  For more information about these options refer to  GCP documentation .    Attached Volumes Per Instance  Enter the number of volumes attached per instance. Default is 1.    Volume Size  Enter the size in GBs for each volume. Default is 100.       On the  Gateway Configuration  page, you can access gateway configuration options.   When creating a cluster, Cloudbreak installs and configures a gateway (powered by Apache Knox) to protect access to the cluster resources. By default, the gateway is enabled for Ambari; You can optionally enable it for other cluster services.   For more information, refer to  Configuring the Gateway  documentation.    On the  Network  page, provide the following to specify the networking resources that will be used for your cluster:     Parameter  Description      Select Network  Select the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.    Select Subnet  Select the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.    Subnet (CIDR)  If you selected to create a new subnet, you must define a valid  CIDR  for the subnet. Default is 10.0.0.0/16.      Cloudbreak uses public IP addresses when communicating with cluster nodes.       Define security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:   Existing security groups are only available for an existing network.       Option  Description      New Security Group  (Default) Creates a new security group with the rules that you defined: A set of  default rules  is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied.  You may open ports by defining the CIDR, entering port range, selecting protocol and clicking  + . You may delete default or previously added rules using the delete icon. If you don't want to use security group, remove the default rules.    Existing Security Groups  Allows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.     The default experience of creating network resources such as network, subnet and security group automatically is provided for convenience. We strongly recommend you review these options and for production cluster deployments leverage your existing network resources that you have defined and validated to meet your enterprise requirements. For more information, refer to  Restricting inbound access from Cloudbreak to cluster .     On the  Security  page, provide the following parameters:     Parameter  Description      Cluster User  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Password  You can log in to the Ambari UI using this password.    Confirm Password  Confirm the password.    New SSH public key  Check this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.    Existing SSH public key  Select an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.       Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.    Related Links  Flex support subscription  Using custom blueprints    Default cluster security groups  Troubleshooting cluster creation       CIDR  (External)   Cloud locations  (External)  Machine types  (External)", 
            "title": "Creating a cluster on GCP"
        }, 
        {
            "location": "/gcp-create/index.html#advanced-options", 
            "text": "Click on  Advanced  to view and enter additional configuration options", 
            "title": "Advanced options"
        }, 
        {
            "location": "/gcp-create/index.html#availability-zone", 
            "text": "Choose one of the availability zones within the selected region.", 
            "title": "Availability zone"
        }, 
        {
            "location": "/gcp-create/index.html#enable-lifetime-management", 
            "text": "Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes).", 
            "title": "Enable lifetime management"
        }, 
        {
            "location": "/gcp-create/index.html#tags", 
            "text": "You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.   By default, the following tags are created:     Tag  Description      cb-version  Cloudbreak version    Owner  Your Cloudbreak admin email.    cb-account-name  Your automatically generated Cloudbreak account name stored in the identity server.    cb-user-name  Your Cloudbreak admin email.     For more information, refer to  Tagging resources .  Related links       Tagging resources", 
            "title": "Tags"
        }, 
        {
            "location": "/gcp-create/index.html#choose-image-catalog", 
            "text": "By default,  Choose Image Catalog  is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to  Using custom images .  Related links      Using custom images", 
            "title": "Choose image catalog"
        }, 
        {
            "location": "/gcp-create/index.html#choose-image-type", 
            "text": "Cloudbreak supports the following types of images for launching clusters:     Image type  Description  Default images provided  Support for custom images      Prewarmed Image  By default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP/HDF. The Ambari and HDP/HDF version used by prewarmed images cannot be customized.  Yes  No    Base Image  Base images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP/HDF software.  Yes  Yes     By default, Cloudbreak uses the included default  prewarmed images , which include the operating system, as well as\nAmbari and HDP/HDF packages installed. You can optionally select the  base image  option if you would like to:   Use an Ambari and HDP/HDF versions different than what the prewarmed image includes and/or    Choose a previously created custom base image", 
            "title": "Choose image type"
        }, 
        {
            "location": "/gcp-create/index.html#choose-image", 
            "text": "If under  Choose image catalog , you selected a custom image catalog, under  Choose Image  you can select an image from that catalog. For complete instructions, refer to  Using custom images .   If you are trying to customize Ambari and HDP/HDF versions, you can ignore the  Choose Image  option; in this case default base image is used.", 
            "title": "Choose image"
        }, 
        {
            "location": "/gcp-create/index.html#ambari-repo-specification", 
            "text": "If you would like to use a custom Ambari version, provide the following information:      Parameter  Description  Example      Version  Enter Ambari version.  2.6.1.3    Repo Url  Provide a URL to the Ambari version repo that you would like to use.  http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.3    Repo Gpg Key Url  Provide a URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.  http://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins", 
            "title": "Ambari repo specification"
        }, 
        {
            "location": "/gcp-create/index.html#hdphdf-repo-specification", 
            "text": "If you would like to use a custom HDP or HDF version, provide the following information:      Parameter  Description  Example      Stack  This is populated by default based on the \"Platform Version\" parameter.  HDP    Version  This is populated by default based on the \"Platform Version\" parameter.  2.6    OS  Operating system.  centos7 (Azure, GCP, OpenStack) or amazonlinux (AWS)    Repository Version  Enter repository version.  2.6.4.0-91    Version Definition File  Enter the URL of the VDF file.  http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.6.4.0/HDP-2.6.4.0-91.xml    (HDF only) MPack Url  (HDF only) Provide MPack URL.  http://public-repo-1.hortonworks.com/HDF/centos7/3.x/updates/3.1.1.0/tars/hdf_ambari_mp/hdf-ambari-mpack-3.1.1.0-35.tar.gz    Enable Ambari Server to download and install GPL Licensed LZO packages?  (Optional, only available if using Ambari 2.6.1.0 or newer) Use this option to enable LZO compression in your HDP/HDF cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to  Enabling LZO .      If you choose to use a base image with custom Ambari and/or HDP/HDF version, Cloudbreak validates the information entered. When Cloudbreak detects that the information entered is incorrect, it displays a warning marked with the   sign. You should review all the warnings before proceeding and make sure that the information that you entered is correct. If you choose to proceed in spite of the warnings, check \"Ignore repository warnings\".    Related links       Using custom images", 
            "title": "HDP/HDF repo specification"
        }, 
        {
            "location": "/gcp-create/index.html#root-volume-size", 
            "text": "Use this option to increase the root volume size. Default is 50 GB. This option is useful if your custom image requires more space than the default 50 GB.", 
            "title": "Root volume size"
        }, 
        {
            "location": "/gcp-create/index.html#use-preemptible-instances", 
            "text": "Check this option to use Google Cloud preemptive VM instances as your cluster nodes. To learn more, refer to  Google Cloud documentation .      Note that:    We recommend not using preemptible instances for any host group that includes Ambari server components.    If you choose to use preemptible instances for a given host group when creating your cluster, any nodes that you add to that host group (during cluster creation or later) will be using preemptible instances.     If you decide not to use preemptible instances when creating your cluster, any nodes that you add to your host group (during cluster creation or later) will be using standard on-demand instances.       Once someone outbids you, the preemptible instances are taken away, removing the nodes from the cluster.   If the preemptible instances are not available right away, creating a cluster will take longer than usual.", 
            "title": "Use preemptible instances"
        }, 
        {
            "location": "/gcp-create/index.html#cloud-storage", 
            "text": "If you would like to access Google Cloud Storage (GCS) from your cluster, you must configure access as described in  Accessing data on GCS .   Related links  Accessing data on GCS", 
            "title": "Cloud storage"
        }, 
        {
            "location": "/gcp-create/index.html#recipes", 
            "text": "This option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to  Using custom scripts (recipes) .   Related links       Using custom scripts (recipes)", 
            "title": "Recipes"
        }, 
        {
            "location": "/gcp-create/index.html#management-packs", 
            "text": "This option allows you to select previously uploaded management packs. For more information on management packs, refer to  Using management packs .   Related links       Using management packs", 
            "title": "Management packs"
        }, 
        {
            "location": "/gcp-create/index.html#external-sources", 
            "text": "You can register external sources with Cloudbreak, and then select and attach them during cluster create. To register external sources with Cloudbreak, refer to:   Using an external authentication source       Using an external database     Register a proxy", 
            "title": "External sources"
        }, 
        {
            "location": "/gcp-create/index.html#custom-properties", 
            "text": "This option allows you to set custom properties based on the template defined in your custom blueprint. For more information, refer to  Set custom properties .   Related links       Set custom properties", 
            "title": "Custom properties"
        }, 
        {
            "location": "/gcp-create/index.html#single-sign-on-sso", 
            "text": "This option allows you to configure the gateway to be the SSO identity provider.    This option is technical preview.    For more information, refer to  Configuring the Gateway  documentation.", 
            "title": "Single sign-on (SSO)"
        }, 
        {
            "location": "/gcp-create/index.html#ambari-server-master-key", 
            "text": "The Ambari server master key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.", 
            "title": "Ambari server master key"
        }, 
        {
            "location": "/gcp-create/index.html#enable-kerberos-security", 
            "text": "Select this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to  Enabling Kerberos security .   Related links       Enabling Kerberos security    Preemptible VM instances  (External)   Storage options  (External)     Next: Configure Access to GCS", 
            "title": "Enable Kerberos security"
        }, 
        {
            "location": "/gcp-data/index.html", 
            "text": "Configuring access to Google Cloud Storage (GCS)\n\n\nGoogle Cloud Storage (GCS) is not supported as a default file system, but access to data in GCS is possible via the gs connector. Use these steps to configure access from your cluster to GCS. \n\n\nThese steps assume that you are using an HDP version that supports the gs cloud storage connector (HDP 2.6.5 introduced this feature as a TP).\n\n\nPrerequisites\n\n\nAccess to Google Cloud Storage is via a service account. The service account that you provide to Cloudbreak for GCS data access must have the following permissions: \n\n\n\n\n\n\nThe service account must have the project-wide \nOwner\n role:\n\n\n  \n\n\n\n\n\n\nThe service account must have the \nStorage Object Admin\n role for the bucket that you are planning to use. You can set this in the bucket's permissions settings:\n\n\n      \n\n\n\n\n\n\nConfigure access to GCS\n\n\nAccess to Google Cloud Storage is via a service account. \n\n\nSteps\n\n\n\n\n\n\nOn the \nCloud Storage\n page in the advanced cluster wizard view, select \nUse existing GSC storage\n. \n\n\n\n\n\n\nUnder \nService Account Email Address\n provide the email for your GCS storage account.  \n\n\n\n\n\n\nOnce your cluster is in the running state, you should be able to access buckets that the configured storage account has access to.  \n\n\nTesting access to GCS\n\n\nTest access to the Google Cloud Storage bucket by running a few commands from any cluster node. For example, you can use the command listed below (replace \u201cmytestbucket\u201d with the name of your bucket):\n\n\nhadoop fs -ls gs://my-test-bucket/\n\n\n\nConfigure GCS storage locations\n\n\nAfter configuring access to GCS via service account, you can optionally use a GCS bucket as a base storage location; this storage location is mainly for the Hive Warehouse Directory (used for storing the table data for managed tables).   \n\n\nPrerequisites\n\n\n\n\nYou must have an existing bucket. For instructions on how to create a bucket on GCS, refer to \nGCP documentation\n.  \n\n\nThe service account that you configured under \nConfigure access to GCS\n must allow access to the bucket.   \n\n\n\n\nSteps\n\n\n\n\nWhen creating a cluster, on the \nCloud Storage\n page in the advanced cluster wizard view, select \nUse existing GCS storage\n and select the existing service account, as described in \nConfigure access to GCS\n.    \n\n\nUnder \nStorage Locations\n, enable \nConfigure Storage Locations\n by clicking the \n button.   \n\n\nProvide your existing bucket name under \nBase Storage Location\n.  \n\n\n\n\nUnder \nPath for Hive Warehouse Directory property (hive.metastore.warehouse.dir)\n, Cloudbreak automatically suggests a location within the bucket. For example, if the bucket that you specified is \nmy-test-bucket\n then the suggested location will be \nmy-test-bucket/apps/hive/warehouse\n.  You may optionally update this path.        \n\n\nCloudbreak automatically creates this directory structure in your bucket.", 
            "title": "Configure access to GCS"
        }, 
        {
            "location": "/gcp-data/index.html#configuring-access-to-google-cloud-storage-gcs", 
            "text": "Google Cloud Storage (GCS) is not supported as a default file system, but access to data in GCS is possible via the gs connector. Use these steps to configure access from your cluster to GCS.   These steps assume that you are using an HDP version that supports the gs cloud storage connector (HDP 2.6.5 introduced this feature as a TP).", 
            "title": "Configuring access to Google Cloud Storage (GCS)"
        }, 
        {
            "location": "/gcp-data/index.html#prerequisites", 
            "text": "Access to Google Cloud Storage is via a service account. The service account that you provide to Cloudbreak for GCS data access must have the following permissions:     The service account must have the project-wide  Owner  role:        The service account must have the  Storage Object Admin  role for the bucket that you are planning to use. You can set this in the bucket's permissions settings:", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/gcp-data/index.html#configure-access-to-gcs", 
            "text": "Access to Google Cloud Storage is via a service account.   Steps    On the  Cloud Storage  page in the advanced cluster wizard view, select  Use existing GSC storage .     Under  Service Account Email Address  provide the email for your GCS storage account.      Once your cluster is in the running state, you should be able to access buckets that the configured storage account has access to.", 
            "title": "Configure access to GCS"
        }, 
        {
            "location": "/gcp-data/index.html#testing-access-to-gcs", 
            "text": "Test access to the Google Cloud Storage bucket by running a few commands from any cluster node. For example, you can use the command listed below (replace \u201cmytestbucket\u201d with the name of your bucket):  hadoop fs -ls gs://my-test-bucket/", 
            "title": "Testing access to GCS"
        }, 
        {
            "location": "/gcp-data/index.html#configure-gcs-storage-locations", 
            "text": "After configuring access to GCS via service account, you can optionally use a GCS bucket as a base storage location; this storage location is mainly for the Hive Warehouse Directory (used for storing the table data for managed tables).     Prerequisites   You must have an existing bucket. For instructions on how to create a bucket on GCS, refer to  GCP documentation .    The service account that you configured under  Configure access to GCS  must allow access to the bucket.      Steps   When creating a cluster, on the  Cloud Storage  page in the advanced cluster wizard view, select  Use existing GCS storage  and select the existing service account, as described in  Configure access to GCS .      Under  Storage Locations , enable  Configure Storage Locations  by clicking the   button.     Provide your existing bucket name under  Base Storage Location .     Under  Path for Hive Warehouse Directory property (hive.metastore.warehouse.dir) , Cloudbreak automatically suggests a location within the bucket. For example, if the bucket that you specified is  my-test-bucket  then the suggested location will be  my-test-bucket/apps/hive/warehouse .  You may optionally update this path.          Cloudbreak automatically creates this directory structure in your bucket.", 
            "title": "Configure GCS storage locations"
        }, 
        {
            "location": "/os-pre/index.html", 
            "text": "Meet minimum system requirements\n\n\nBefore launching Cloudbreak on your OpenStack, make sure that your OpenStack deployment fulfills the following requirements.\n\n\nSupported Linux distributions\n\n\nThe following versions of the \nRed Hat distribution of OpenStack\n (RDO) are supported:\n\n\n\n\nJuno\n\n\nKilo\n\n\nLiberty\n\n\nMitaka\n\n\n\n\nStandard modules\n\n\nCloudbreak requires that the following standard modules are installed and configured on OpenStack:\n\n\n\n\nKeystone V2 or Keystone V3\n\n\nNeutron (Self-service and provider networking)\n\n\nNova (KVM or Xen hypervisor)\n\n\nGlance\n\n\nCinder (Optional)\n\n\nHeat (Optional but highly recommended, since provisioning through native API calls will be deprecated in the future)\n\n\n\n\nRelated links\n\n\nRed Hat distribution of OpenStack\n (External)\n\n\nMeet the prerequisites\n\n\nBefore launching Cloudbreak on OpenStack, you must meet the following prerequisites.\n\n\nSSH key pair\n\n\nGenerate a new SSH key pair\n or use an existing SSH key pair to your OpenStack account. You will be required to select it when launching the VM.\n\n\nVirtual network\n\n\nYou must have a virtual network configured on your cloud provider.\n\n\nSecurity group\n\n\nPorts 22 (SSH), 80 (HTTPS), and 443 (HTTPS) must be open on the security group\n\n\n\n\nNext: Launch Cloudbreak", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/os-pre/index.html#meet-minimum-system-requirements", 
            "text": "Before launching Cloudbreak on your OpenStack, make sure that your OpenStack deployment fulfills the following requirements.", 
            "title": "Meet minimum system requirements"
        }, 
        {
            "location": "/os-pre/index.html#supported-linux-distributions", 
            "text": "The following versions of the  Red Hat distribution of OpenStack  (RDO) are supported:   Juno  Kilo  Liberty  Mitaka", 
            "title": "Supported Linux distributions"
        }, 
        {
            "location": "/os-pre/index.html#standard-modules", 
            "text": "Cloudbreak requires that the following standard modules are installed and configured on OpenStack:   Keystone V2 or Keystone V3  Neutron (Self-service and provider networking)  Nova (KVM or Xen hypervisor)  Glance  Cinder (Optional)  Heat (Optional but highly recommended, since provisioning through native API calls will be deprecated in the future)   Related links  Red Hat distribution of OpenStack  (External)", 
            "title": "Standard modules"
        }, 
        {
            "location": "/os-pre/index.html#meet-the-prerequisites", 
            "text": "Before launching Cloudbreak on OpenStack, you must meet the following prerequisites.", 
            "title": "Meet the prerequisites"
        }, 
        {
            "location": "/os-pre/index.html#ssh-key-pair", 
            "text": "Generate a new SSH key pair  or use an existing SSH key pair to your OpenStack account. You will be required to select it when launching the VM.", 
            "title": "SSH key pair"
        }, 
        {
            "location": "/os-pre/index.html#virtual-network", 
            "text": "You must have a virtual network configured on your cloud provider.", 
            "title": "Virtual network"
        }, 
        {
            "location": "/os-pre/index.html#security-group", 
            "text": "Ports 22 (SSH), 80 (HTTPS), and 443 (HTTPS) must be open on the security group   Next: Launch Cloudbreak", 
            "title": "Security group"
        }, 
        {
            "location": "/os-launch/index.html", 
            "text": "Launching Cloudbreak on OpenStack\n\n\nThese steps describe how to launch Cloudbreak on OpenStack. This is the only deployment options available on OpenStack.\n\nBefore launching Cloudbreak on OpenStack, review and meet the \nprerequisites\n. Next, follow the steps below. \n\n\nVM requirements\n\n\nTo launch the Cloudbreak deployer and install the Cloudbreak application, you must have an existing VM. \n\n\nSystem requirements\n\n\nYour system must meet the following requirements:\n\n\n\n\nMinimum VM requirements: 16GB RAM, 40GB disk, 4 cores\n\n\nSupported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)\n\n\n\n\n\n\nYou can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.\n\n\n\n\nRoot access\n\n\nEvery command must be executed as root. In order to get root privileges execute:\n\n\nsudo -i\n\n\n\nSystem updates\n\n\nEnsure that your system is up-to-date by executing:\n\n\nyum -y update\n\n\n\nReboot it if necessary.\n\n\nInstall iptables\n\n\nPerform these steps to install and configure iptables.\n\n\nSteps\n \n\n\n\n\n\n\nInstall iptables-services:\n\n\nyum -y install net-tools ntp wget lsof unzip tar iptables-services\nsystemctl enable ntpd \n systemctl start ntpd\nsystemctl disable firewalld \n systemctl stop firewalld\n\n\n\n\nWithout iptables-services installed the \niptables save\n command will not be available.\n\n\n\n\n\n\n\n\nConfigure permissive iptables on your machine:\n\n\niptables --flush INPUT \n \\\niptables --flush FORWARD \n \\\nservice iptables save\n\n\n\n\n\n\nDisable SELINUX\n\n\nPerform these steps to disable SELINUX.\n\n\nSteps\n \n\n\n\n\n\n\nDisable SELINUX:\n\n\nsetenforce 0\nsed -i 's/SELINUX=enforcing/SELINUX=disabled/g' \n/etc/selinux/config\n\n\n\n\n\n\nRun the following command to ensure that SELinux is not turned on afterwards: \n\n\ngetenforce\n\n\n\n\n\n\nThe command should return \"Disabled\".     \n\n\n\n\n\n\nInstall Docker\n\n\nPerform these steps to install Docker. The minimum Docker version is 1.13.1. If you are using an older image that comes with an older Docker version, upgrade Docker to 1.13.1 or newer. \n\n\nSteps\n    \n\n\n\n\n\n\nInstall Docker service:\n\n\nyum install -y docker\nsystemctl start docker\nsystemctl enable docker\n\n\n\n\n\n\nCheck the Docker Logging Driver configuration:\n\n\ndocker info | grep \"Logging Driver\"\n\n\n\n\n\n\nIf it is set to \nLogging Driver: journald\n, you must  set it to \"json-file\" instead. To do that:\n\n\n\n\n\n\nOpen the \ndocker\n file for editing:\n\n\nvi /etc/sysconfig/docker\n  \n\n\n\n\n\n\nEdit the following part of the file so that it looks like below (showing \nlog-driver=json-file\n):\n\n\n# Modify these options if you want to change the way the docker daemon runs\nOPTIONS='--selinux-enabled --log-driver=json-file --signature-verification=false'\n     \n\n\n\n\n\n\nRestart Docker:\n\n\nsystemctl restart docker\nsystemctl status docker\n\n\n\n\n\n\n\n\n\n\nInstall Cloudbreak on a VM\n\n\nInstall Cloudbreak using the following steps.\n\n\nSteps\n\n\n\n\n\n\nInstall the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:\n\n\nyum -y install unzip tar\ncurl -Ls public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_2.7.0_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version\n\n\nOnce the Cloudbreak deployer is installed, you can set up the Cloudbreak application.\n\n\n\n\n\n\nCreate a Cloudbreak deployment directory and navigate to it:\n\n\nmkdir cloudbreak-deployment\ncd cloudbreak-deployment\n\n\n\n\n\n\nIn the directory, create a file called \nProfile\n with the following content:\n\n\nexport UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL\nexport PUBLIC_IP=MY_VM_IP\n\n\nFor example:\n\n\nexport UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com\nexport PUBLIC_IP=172.26.231.100\n\n\nYou will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.\n\n\nYou should set the CLOUDBREAK_SMTP_SENDER_USERNAME variable to the username you use to authenticate to your SMTP server. You should set the CLOUDBREAK_SMTP_SENDER_PASSWORD variable to the password you use to authenticate to your SMTP server.\n\n\n\n\n\n\nGenerate configurations by executing:\n\n\nrm *.yml\ncbd generate\n   \n\n\nThe cbd start command includes the cbd generate command which applies the following steps:\n\n\n\n\nCreates the \ndocker-compose.yml\n file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.  \n\n\nCreates the \nuaa.yml\n file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.   \n\n\n\n\n\n\n\n\nStart the Cloudbreak application by using the following commands:\n\n\ncbd pull-parallel\ncbd start\n\n\nThis will start the Docker containers and initialize the application. The first time you start the Cloudbreak app, the process will take longer than usual due to the download of all the necessary docker images.\n\n\n\n\nIf you encounter errors during \ncbd start\n, refer to \nToubleshooting\n.  \n\n\n\n\n\n\n\n\nNext, check Cloudbreak application logs:\n\n\ncbd logs cloudbreak\n\n\nYou should see a message like this in the log: \nStarted CloudbreakApplication in 36.823 seconds.\n Cloudbreak normally takes less than a minute to start.\n\n\n\n\n\n\nRelated links\n\n\nToubleshooting\n   \n\n\nAccess Cloudbreak web UI\n\n\nLog in to the Cloudbreak UI using the following steps.\n\n\nSteps\n \n\n\n\n\n\n\nYou can log into the Cloudbreak application at \nhttps://IP_Address\n. For example \nhttps://34.212.141.253\n. You may use \ncbd start\n to obtain the login information. Alternatively, you can obtain the VM's IP address from your cloud provider console. \n\n\n\n\n\n\nConfirm the security exception to proceed to the Cloudbreak web UI.\n\n\nThe first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nThe login page is displayed:\n\n\n  \n\n\n\n\n\n\nLog in to the Cloudbreak web UI using the credentials that you configured in your \nProfile\n file:\n\n\n\n\nThe username is the \nUAA_DEFAULT_USER_EMAIL\n     \n\n\nThe password is the \nUAA_DEFAULT_USER_PW\n \n\n\n\n\n\n\n\n\nUpon a successful login, you are redirected to the dashboard:\n\n\n  \n\n\n\n\n\n\nConfigure external Cloudbreak database\n\n\nBy default, Cloudbreak uses an embedded PostgreSQL database to persist data related to Cloudbreak configuration, setup, and so on. For a production Cloudbreak deployment, you must \nconfigure an external database\n.\n\n\nRelated links\n\n\nConfigure an external database\n   \n\n\nConfigure a self-signed certificate\n\n\nIf your OpenStack is secured with a self-signed certificate, you need to import that certificate into Cloudbreak, or else Cloudbreak won't be able to communicate with your OpenStack.\n\n\nTo import the certificate, place the certificate file in the \n/certs/trusted/\n directory, follow these steps.\n\n\nSteps\n\n\n\n\nNavigate to the \ncerts\n directory (automatically generated).\n\n\nCreate the \ntrusted\n directory.\n\n\nCopy the certificate to the \ntrusted\n directory.\n\n\n\n\nCloudbreak will automatically pick up the certificate and import it into its trust store upon start.\n\n\nCreate Cloudbreak credential\n\n\nCloudbreak works by connecting your OpenStack account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a \nCloudbreak credential\n.\n\n\nSteps\n\n\n\n\n\n\nIn the Cloudbreak web UI, select \nCredentials\n from the navigation pane.\n\n\n\n\n\n\nClick \nCreate Credential\n.\n\n\n\n\n\n\nUnder \nCloud provider\n, select \"Google Cloud Platform\".\n\n\n\n\n\n\n\n\nSelect the keystone version.\n\n\n\n\n\n\nProvide the  following information:\n\n\nFor Keystone v2:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nUser\n\n\nEnter your OpenStack user name.\n\n\n\n\n\n\nPassword\n\n\nEnter your OpenStack password.\n\n\n\n\n\n\nTenant Name\n\n\nEnter the OpenStack tenant name.\n\n\n\n\n\n\nEndpoint\n\n\nEnter the OpenStack endpoint.\n\n\n\n\n\n\nAPI Facing\n\n\n(Optional) Select \npublic\n, \nprivate\n, or \ninternal\n.\n\n\n\n\n\n\n\n\nFor Keystone v3:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKeystone scope\n\n\nSelect the scope: default, domain, or project.\n\n\n\n\n\n\nName\n\n\nEnter a name for your credential.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nUser\n\n\nEnter your OpenStack user name.\n\n\n\n\n\n\nPassword\n\n\nEnter your OpenStack password.\n\n\n\n\n\n\nUser Domain\n\n\nEnter your OpenStack user domain.\n\n\n\n\n\n\nEndpoint\n\n\nEnter the OpenStack endpoint.\n\n\n\n\n\n\nAPI Facing\n\n\n(Optional) Select \npublic\n, \nprivate\n, or \ninternal\n.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nCreate\n.\n\n\n\n\n\n\nYour credential should now be displayed in the \nCredentials\n pane.\n\n\nCongratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to \ncreate clusters\n.\n\n\n\n\n\n\n\n\nNext: Create a Cluster", 
            "title": "Launch on OpenStack"
        }, 
        {
            "location": "/os-launch/index.html#launching-cloudbreak-on-openstack", 
            "text": "These steps describe how to launch Cloudbreak on OpenStack. This is the only deployment options available on OpenStack. \nBefore launching Cloudbreak on OpenStack, review and meet the  prerequisites . Next, follow the steps below.", 
            "title": "Launching Cloudbreak on OpenStack"
        }, 
        {
            "location": "/os-launch/index.html#vm-requirements", 
            "text": "To launch the Cloudbreak deployer and install the Cloudbreak application, you must have an existing VM.", 
            "title": "VM requirements"
        }, 
        {
            "location": "/os-launch/index.html#system-requirements", 
            "text": "Your system must meet the following requirements:   Minimum VM requirements: 16GB RAM, 40GB disk, 4 cores  Supported operating systems: RHEL, CentOS, and Oracle Linux 7 (64-bit)    You can install Cloudbreak on Mac OS X for evaluation purposes only. Mac OS X is not supported for a production deployment of Cloudbreak.", 
            "title": "System requirements"
        }, 
        {
            "location": "/os-launch/index.html#root-access", 
            "text": "Every command must be executed as root. In order to get root privileges execute:  sudo -i", 
            "title": "Root access"
        }, 
        {
            "location": "/os-launch/index.html#system-updates", 
            "text": "Ensure that your system is up-to-date by executing:  yum -y update  Reboot it if necessary.", 
            "title": "System updates"
        }, 
        {
            "location": "/os-launch/index.html#install-iptables", 
            "text": "Perform these steps to install and configure iptables.  Steps      Install iptables-services:  yum -y install net-tools ntp wget lsof unzip tar iptables-services\nsystemctl enable ntpd   systemctl start ntpd\nsystemctl disable firewalld   systemctl stop firewalld   Without iptables-services installed the  iptables save  command will not be available.     Configure permissive iptables on your machine:  iptables --flush INPUT   \\\niptables --flush FORWARD   \\\nservice iptables save", 
            "title": "Install iptables"
        }, 
        {
            "location": "/os-launch/index.html#disable-selinux", 
            "text": "Perform these steps to disable SELINUX.  Steps      Disable SELINUX:  setenforce 0\nsed -i 's/SELINUX=enforcing/SELINUX=disabled/g' \n/etc/selinux/config    Run the following command to ensure that SELinux is not turned on afterwards:   getenforce    The command should return \"Disabled\".", 
            "title": "Disable SELINUX"
        }, 
        {
            "location": "/os-launch/index.html#install-docker", 
            "text": "Perform these steps to install Docker. The minimum Docker version is 1.13.1. If you are using an older image that comes with an older Docker version, upgrade Docker to 1.13.1 or newer.   Steps         Install Docker service:  yum install -y docker\nsystemctl start docker\nsystemctl enable docker    Check the Docker Logging Driver configuration:  docker info | grep \"Logging Driver\"    If it is set to  Logging Driver: journald , you must  set it to \"json-file\" instead. To do that:    Open the  docker  file for editing:  vi /etc/sysconfig/docker       Edit the following part of the file so that it looks like below (showing  log-driver=json-file ):  # Modify these options if you want to change the way the docker daemon runs\nOPTIONS='--selinux-enabled --log-driver=json-file --signature-verification=false'          Restart Docker:  systemctl restart docker\nsystemctl status docker", 
            "title": "Install Docker"
        }, 
        {
            "location": "/os-launch/index.html#install-cloudbreak-on-a-vm", 
            "text": "Install Cloudbreak using the following steps.  Steps    Install the Cloudbreak deployer and unzip the platform-specific single binary to your PATH. For example:  yum -y install unzip tar\ncurl -Ls public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_2.7.0_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version  Once the Cloudbreak deployer is installed, you can set up the Cloudbreak application.    Create a Cloudbreak deployment directory and navigate to it:  mkdir cloudbreak-deployment\ncd cloudbreak-deployment    In the directory, create a file called  Profile  with the following content:  export UAA_DEFAULT_SECRET=MY-SECRET\nexport UAA_DEFAULT_USER_PW=MY-PASSWORD\nexport UAA_DEFAULT_USER_EMAIL=MY-EMAIL\nexport PUBLIC_IP=MY_VM_IP  For example:  export UAA_DEFAULT_SECRET=MySecret123\nexport UAA_DEFAULT_USER_PW=MySecurePassword123\nexport UAA_DEFAULT_USER_EMAIL=dbialek@hortonworks.com\nexport PUBLIC_IP=172.26.231.100  You will need to provide the email and password when logging in to the Cloudbreak web UI and when using the Cloudbreak CLI. The secret will be used by Cloudbreak for authentication.  You should set the CLOUDBREAK_SMTP_SENDER_USERNAME variable to the username you use to authenticate to your SMTP server. You should set the CLOUDBREAK_SMTP_SENDER_PASSWORD variable to the password you use to authenticate to your SMTP server.    Generate configurations by executing:  rm *.yml\ncbd generate      The cbd start command includes the cbd generate command which applies the following steps:   Creates the  docker-compose.yml  file, which describes the configuration of all the Docker containers required for the Cloudbreak deployment.    Creates the  uaa.yml  file, which holds the configuration of the identity server used to authenticate users with Cloudbreak.        Start the Cloudbreak application by using the following commands:  cbd pull-parallel\ncbd start  This will start the Docker containers and initialize the application. The first time you start the Cloudbreak app, the process will take longer than usual due to the download of all the necessary docker images.   If you encounter errors during  cbd start , refer to  Toubleshooting .       Next, check Cloudbreak application logs:  cbd logs cloudbreak  You should see a message like this in the log:  Started CloudbreakApplication in 36.823 seconds.  Cloudbreak normally takes less than a minute to start.    Related links  Toubleshooting", 
            "title": "Install Cloudbreak on a VM"
        }, 
        {
            "location": "/os-launch/index.html#access-cloudbreak-web-ui", 
            "text": "Log in to the Cloudbreak UI using the following steps.  Steps      You can log into the Cloudbreak application at  https://IP_Address . For example  https://34.212.141.253 . You may use  cbd start  to obtain the login information. Alternatively, you can obtain the VM's IP address from your cloud provider console.     Confirm the security exception to proceed to the Cloudbreak web UI.  The first time you access Cloudbreak UI, Cloudbreak will automatically generate a self-signed certificate, due to which your browser will warn you about an untrusted connection and will ask you to confirm a security exception.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...       The login page is displayed:        Log in to the Cloudbreak web UI using the credentials that you configured in your  Profile  file:   The username is the  UAA_DEFAULT_USER_EMAIL        The password is the  UAA_DEFAULT_USER_PW       Upon a successful login, you are redirected to the dashboard:", 
            "title": "Access Cloudbreak web UI"
        }, 
        {
            "location": "/os-launch/index.html#configure-external-cloudbreak-database", 
            "text": "By default, Cloudbreak uses an embedded PostgreSQL database to persist data related to Cloudbreak configuration, setup, and so on. For a production Cloudbreak deployment, you must  configure an external database .  Related links  Configure an external database", 
            "title": "Configure external Cloudbreak database"
        }, 
        {
            "location": "/os-launch/index.html#configure-a-self-signed-certificate", 
            "text": "If your OpenStack is secured with a self-signed certificate, you need to import that certificate into Cloudbreak, or else Cloudbreak won't be able to communicate with your OpenStack.  To import the certificate, place the certificate file in the  /certs/trusted/  directory, follow these steps.  Steps   Navigate to the  certs  directory (automatically generated).  Create the  trusted  directory.  Copy the certificate to the  trusted  directory.   Cloudbreak will automatically pick up the certificate and import it into its trust store upon start.", 
            "title": "Configure a self-signed certificate"
        }, 
        {
            "location": "/os-launch/index.html#create-cloudbreak-credential", 
            "text": "Cloudbreak works by connecting your OpenStack account through this credential, and then uses it to create resources on your behalf. Before you can start provisioning cluster using Cloudbreak, you must create a  Cloudbreak credential .  Steps    In the Cloudbreak web UI, select  Credentials  from the navigation pane.    Click  Create Credential .    Under  Cloud provider , select \"Google Cloud Platform\".     Select the keystone version.    Provide the  following information:  For Keystone v2:     Parameter  Description      Name  Enter a name for your credential.    Description  (Optional) Enter a description.    User  Enter your OpenStack user name.    Password  Enter your OpenStack password.    Tenant Name  Enter the OpenStack tenant name.    Endpoint  Enter the OpenStack endpoint.    API Facing  (Optional) Select  public ,  private , or  internal .     For Keystone v3:     Parameter  Description      Keystone scope  Select the scope: default, domain, or project.    Name  Enter a name for your credential.    Description  (Optional) Enter a description.    User  Enter your OpenStack user name.    Password  Enter your OpenStack password.    User Domain  Enter your OpenStack user domain.    Endpoint  Enter the OpenStack endpoint.    API Facing  (Optional) Select  public ,  private , or  internal .       Click  Create .    Your credential should now be displayed in the  Credentials  pane.  Congratulations! You have successfully launched Cloudbreak and created a Cloudbreak credential. Now you can use Cloudbreak to  create clusters .     Next: Create a Cluster", 
            "title": "Create Cloudbreak credential"
        }, 
        {
            "location": "/os-create/index.html", 
            "text": "Creating a cluster on OpenStack\n\n\nUse these steps to create a cluster.\n\n\n\n\nTroubleshooting cluster creation\n\n\nIf you experience problems during cluster creation, refer to \nTroubleshooting cluster creation\n.\n\n\n\nSteps\n\n\n\n\n\n\nLog in to the Cloudbreak UI.\n\n\n\n\n\n\nClick the \nCreate Cluster\n button and the \nCreate Cluster\n wizard is displayed.\n\n    By default, \nBasic\n view is displayed. To view advanced options, click \nAdvanced\n. To learn about advanced options, refer to \nAdvanced cluster options\n.\n\n\n \n\n\n\n\n\n\nOn the \nGeneral Configuration\n page, specify the following general parameters for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Credential\n\n\nChoose a previously created credential.\n\n\n\n\n\n\nCluster Name\n\n\nEnter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.\n\n\n\n\n\n\nRegion\n\n\nSelect the region in which you would like to launch your cluster.\n\n\n\n\n\n\nPlatform Version\n\n\nChoose the HDP or HDF version to use for this cluster. Blueprints available for this platform version will be populated under \"Cluster Type\" below. If you selected the \nHDF\n platform, refer to \nCreating HDF clusters\n for HDF cluster configuration tips.\n\n\n\n\n\n\nCluster Type\n\n\nChoose one of the default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to \nUsing custom blueprints\n.\n\n\n\n\n\n\nFlex Subscription\n\n\nThis option will appear if you have configured your deployment for a \nflex support subscription\n.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nHardware and Storage\n page, for each host group provide the following information to define your cluster nodes and attached storage. \n\n\nTo edit this section, click on the \n. When done editing, click on the \n to save the changes.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAmbari Server\n\n\nYou must select one node for Ambari Server by clicking the \n button. The \"Instance Count\" for that host group must be set to \"1\". If you are using one of the default blueprints, this is set by default.\n\n\n\n\n\n\nInstance Type\n\n\nSelect an instance type.\n\n\n\n\n\n\nInstance Count\n\n\nEnter the number of instances of a given type. Default is 1.\n\n\n\n\n\n\nStorage Type\n\n\nSelect the volume type. The options are:\nMagnetic\nEphemeral\nGeneral Purpose (SSD)\nThroughput Optimized HDD\nFor more information about these options refer to \nAWS documentation\n.\n\n\n\n\n\n\nAttached Volumes Per Instance\n\n\nEnter the number of volumes attached per instance. Default is 1.\n\n\n\n\n\n\nVolume Size\n\n\nEnter the size in GBs for each volume. Default is 100.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the \nGateway Configuration\n page, you can access gateway configuration options. \n\n\nWhen creating a cluster, Cloudbreak installs and configures a gateway (powered by Apache Knox) to protect access to the cluster resources. By default, the gateway is enabled for Ambari; You can optionally enable it for other cluster services. \n\n\nFor more information, refer to \nConfiguring the Gateway\n documentation.\n\n\n\n\n\n\nOn the \nNetwork\n page, provide the following to specify the networking resources that will be used for your cluster:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSelect Network\n\n\nSelect the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.\n\n\n\n\n\n\nSelect Subnet\n\n\nSelect the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.\n\n\n\n\n\n\nSubnet (CIDR)\n\n\nIf you selected to create a new subnet, you must define a valid \nCIDR\n for the subnet. Default is 10.0.0.0/16.\n\n\n\n\n\n\n\n\n\n\nCloudbreak uses public IP addresses when communicating with cluster nodes.  \n\n\n\n\n\n\n\n\nDefine security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNew Security Group\n\n\n(Default) Creates a new security group with the rules that you defined:\nA set of \ndefault rules\n is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied. \nYou may open ports by defining the CIDR, entering port range, selecting protocol and clicking \n+\n.\nYou may delete default or previously added rules using the delete icon.\nIf you don't want to use security group, remove the default rules.\n\n\n\n\n\n\nExisting Security Groups\n\n\nAllows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.\n\n\n\n\n\n\n\n\nThe default experience of creating network resources such as network, subnet and security group automatically is provided for convenience. We strongly recommend you review these options and for production cluster deployments leverage your existing network resources that you have defined and validated to meet your enterprise requirements. For more information, refer to \nRestricting inbound access from Cloudbreak to cluster\n. \n\n\n\n\n\n\nOn the \nSecurity\n page, provide the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nYou can log in to the Ambari UI using this username. By default, this is set to \nadmin\n.\n\n\n\n\n\n\nPassword\n\n\nYou can log in to the Ambari UI using this password.\n\n\n\n\n\n\nConfirm Password\n\n\nConfirm the password.\n\n\n\n\n\n\nNew SSH public key\n\n\nCheck this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.\n\n\n\n\n\n\nExisting SSH public key\n\n\nSelect an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on \nCreate Cluster\n to create a cluster.\n\n\n\n\n\n\nYou will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.\n\n\n\n\n\n\nRelated links\n\n\nFlex support subscription\n\n\nUsing custom blueprints\n \n\n\nDefault cluster security groups\n\n\nTroubleshooting cluster creation\n  \n\n\nCIDR\n (External)    \n\n\nAdvanced cluster options\n\n\nClick on \nAdvanced\n to view and enter additional configuration options\n\n\nAvailability zone\n\n\nChoose one of the availability zones within the selected region. \n\n\nEnable lifetime management\n\n\nCheck this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes). \n\n\nTags\n\n\nYou can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account. \n\n\nBy default, the following tags are created:\n\n\n\n\n\n\n\n\nTag\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncb-version\n\n\nCloudbreak version\n\n\n\n\n\n\nOwner\n\n\nYour Cloudbreak admin email.\n\n\n\n\n\n\ncb-account-name\n\n\nYour automatically generated Cloudbreak account name stored in the identity server.\n\n\n\n\n\n\ncb-user-name\n\n\nYour Cloudbreak admin email.\n\n\n\n\n\n\n\n\nFor more information, refer to \nTagging resources\n.\n\n\nRelated links\n    \n\n\nTagging resources\n \n\n\nChoose image catalog\n\n\nBy default, \nChoose Image Catalog\n is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to \nUsing custom images\n.\n\n\nRelated links\n   \n\n\nUsing custom images\n  \n\n\nChoose image type\n\n\nCloudbreak supports the following types of images for launching clusters:\n\n\n\n\n\n\n\n\nImage type\n\n\nDescription\n\n\nDefault images provided\n\n\nSupport for custom images\n\n\n\n\n\n\n\n\n\n\nPrewarmed Image\n\n\nBy default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP/HDF. The Ambari and HDP/HDF version used by prewarmed images cannot be customized.\n\n\nYes\n\n\nNo\n\n\n\n\n\n\nBase Image\n\n\nBase images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP/HDF software.\n\n\nYes\n\n\nYes\n\n\n\n\n\n\n\n\nBy default, Cloudbreak uses the included default \nprewarmed images\n, which include the operating system, as well as\nAmbari and HDP/HDF packages installed. You can optionally select the \nbase image\n option if you would like to:\n\n\n\n\nUse an Ambari and HDP/HDF versions different than what the prewarmed image includes and/or  \n\n\nChoose a previously created custom base image\n\n\n\n\nChoose image\n\n\nIf under \nChoose image catalog\n, you selected a custom image catalog, under \nChoose Image\n you can select an image from that catalog. For complete instructions, refer to \nUsing custom images\n. \n\n\nIf you are trying to customize Ambari and HDP/HDF versions, you can ignore the \nChoose Image\n option; in this case default base image is used.\n\n\nAmbari repo specification\n\n\nIf you would like to use a custom Ambari version, provide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nVersion\n\n\nEnter Ambari version.\n\n\n2.6.1.3\n\n\n\n\n\n\nRepo Url\n\n\nProvide a URL to the Ambari version repo that you would like to use.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.3\n\n\n\n\n\n\nRepo Gpg Key Url\n\n\nProvide a URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.\n\n\nhttp://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins\n\n\n\n\n\n\n\n\nHDP/HDF repo specification\n\n\nIf you would like to use a custom HDP or HDF version, provide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nStack\n\n\nThis is populated by default based on the \"Platform Version\" parameter.\n\n\nHDP\n\n\n\n\n\n\nVersion\n\n\nThis is populated by default based on the \"Platform Version\" parameter.\n\n\n2.6\n\n\n\n\n\n\nOS\n\n\nOperating system.\n\n\ncentos7 (Azure, GCP, OpenStack) or amazonlinux (AWS)\n\n\n\n\n\n\nRepository Version\n\n\nEnter repository version.\n\n\n2.6.4.0-91\n\n\n\n\n\n\nVersion Definition File\n\n\nEnter the URL of the VDF file.\n\n\nhttp://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.6.4.0/HDP-2.6.4.0-91.xml\n\n\n\n\n\n\n(HDF only) MPack Url\n\n\n(HDF only) Provide MPack URL.\n\n\nhttp://public-repo-1.hortonworks.com/HDF/centos7/3.x/updates/3.1.1.0/tars/hdf_ambari_mp/hdf-ambari-mpack-3.1.1.0-35.tar.gz\n\n\n\n\n\n\nEnable Ambari Server to download and install GPL Licensed LZO packages?\n\n\n(Optional, only available if using Ambari 2.6.1.0 or newer) Use this option to enable LZO compression in your HDP/HDF cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to \nEnabling LZO\n.\n\n\n\n\n\n\n\n\n\n\nIf you choose to use a base image with custom Ambari and/or HDP/HDF version, Cloudbreak validates the information entered. When Cloudbreak detects that the information entered is incorrect, it displays a warning marked with the \n sign. You should review all the warnings before proceeding and make sure that the information that you entered is correct. If you choose to proceed in spite of the warnings, check \"Ignore repository warnings\".  \n\n\nRelated links\n    \n\n\nUsing custom images\n      \n\n\nRecipes\n\n\nThis option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to \nUsing custom scripts (recipes)\n. \n\n\nRelated links\n    \n\n\nUsing custom scripts (recipes)\n \n\n\nManagement packs\n\n\nThis option allows you to select previously uploaded management packs. For more information on management packs, refer to \nUsing management packs\n. \n\n\nRelated links\n    \n\n\nUsing management packs\n  \n\n\nExternal sources\n\n\nYou can register external sources with Cloudbreak, and then select and attach them during cluster create. To register external sources with Cloudbreak, refer to:\n\n\n\n\nUsing an external authentication source\n    \n\n\nUsing an external database\n  \n\n\nRegister a proxy\n  \n\n\n\n\nCustom properties\n\n\nThis option allows you to set custom properties based on the template defined in your custom blueprint. For more information, refer to \nSet custom properties\n. \n\n\nRelated links\n    \n\n\nSet custom properties\n   \n\n\nSingle sign-on (SSO)\n\n\nThis option allows you to configure the gateway to be the SSO identity provider. \n\n\n\n\nThis option is technical preview. \n\n\n\n\nFor more information, refer to \nConfiguring the Gateway\n documentation. \n\n\nAmbari server master key\n\n\nThe Ambari server master key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.  \n\n\nEnable Kerberos security\n\n\nSelect this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to \nEnabling Kerberos security\n. \n\n\nRelated links\n    \n\n\nEnabling Kerberos security\n \n\n\n\n\nNext: Access Cluster", 
            "title": "Create a cluster"
        }, 
        {
            "location": "/os-create/index.html#creating-a-cluster-on-openstack", 
            "text": "Use these steps to create a cluster.   Troubleshooting cluster creation  If you experience problems during cluster creation, refer to  Troubleshooting cluster creation .  Steps    Log in to the Cloudbreak UI.    Click the  Create Cluster  button and the  Create Cluster  wizard is displayed. \n    By default,  Basic  view is displayed. To view advanced options, click  Advanced . To learn about advanced options, refer to  Advanced cluster options .       On the  General Configuration  page, specify the following general parameters for your cluster:     Parameter  Description      Select Credential  Choose a previously created credential.    Cluster Name  Enter a name for your cluster. The name must be between 5 and 40 characters, must start with a letter, and must only include lowercase letters, numbers, and hyphens.    Region  Select the region in which you would like to launch your cluster.    Platform Version  Choose the HDP or HDF version to use for this cluster. Blueprints available for this platform version will be populated under \"Cluster Type\" below. If you selected the  HDF  platform, refer to  Creating HDF clusters  for HDF cluster configuration tips.    Cluster Type  Choose one of the default cluster configurations, or, if you have defined your own cluster configuration via Ambari blueprint, you can choose it here. For more information on default and custom blueprints, refer to  Using custom blueprints .    Flex Subscription  This option will appear if you have configured your deployment for a  flex support subscription .       On the  Hardware and Storage  page, for each host group provide the following information to define your cluster nodes and attached storage.   To edit this section, click on the  . When done editing, click on the   to save the changes.     Parameter  Description      Ambari Server  You must select one node for Ambari Server by clicking the   button. The \"Instance Count\" for that host group must be set to \"1\". If you are using one of the default blueprints, this is set by default.    Instance Type  Select an instance type.    Instance Count  Enter the number of instances of a given type. Default is 1.    Storage Type  Select the volume type. The options are: Magnetic Ephemeral General Purpose (SSD) Throughput Optimized HDD For more information about these options refer to  AWS documentation .    Attached Volumes Per Instance  Enter the number of volumes attached per instance. Default is 1.    Volume Size  Enter the size in GBs for each volume. Default is 100.       On the  Gateway Configuration  page, you can access gateway configuration options.   When creating a cluster, Cloudbreak installs and configures a gateway (powered by Apache Knox) to protect access to the cluster resources. By default, the gateway is enabled for Ambari; You can optionally enable it for other cluster services.   For more information, refer to  Configuring the Gateway  documentation.    On the  Network  page, provide the following to specify the networking resources that will be used for your cluster:     Parameter  Description      Select Network  Select the virtual network in which you would like your cluster to be provisioned. You can select an existing network or create a new network.    Select Subnet  Select the subnet in which you would like your cluster to be provisioned. If you are using a new network, create a new subnet. If you are using an existing network, select an existing subnet.    Subnet (CIDR)  If you selected to create a new subnet, you must define a valid  CIDR  for the subnet. Default is 10.0.0.0/16.      Cloudbreak uses public IP addresses when communicating with cluster nodes.       Define security groups for each host group. You can either create new security groups and define their rules or reuse existing security groups:     Option  Description      New Security Group  (Default) Creates a new security group with the rules that you defined: A set of  default rules  is provided. You should review and adjust these default rules. If you do not make any modifications, default rules will be applied.  You may open ports by defining the CIDR, entering port range, selecting protocol and clicking  + . You may delete default or previously added rules using the delete icon. If you don't want to use security group, remove the default rules.    Existing Security Groups  Allows you to select an existing security group that is already available in the selected provider region. This selection is disabled if no existing security groups are available in your chosen region.     The default experience of creating network resources such as network, subnet and security group automatically is provided for convenience. We strongly recommend you review these options and for production cluster deployments leverage your existing network resources that you have defined and validated to meet your enterprise requirements. For more information, refer to  Restricting inbound access from Cloudbreak to cluster .     On the  Security  page, provide the following parameters:     Parameter  Description      Cluster User  You can log in to the Ambari UI using this username. By default, this is set to  admin .    Password  You can log in to the Ambari UI using this password.    Confirm Password  Confirm the password.    New SSH public key  Check this option to specify a new public key and then enter the public key. You will use the matching private key to access your cluster nodes via SSH.    Existing SSH public key  Select an existing public key. You will use the matching private key to access your cluster nodes via SSH. This is a default option as long as an existing SSH public key is available.       Click on  Create Cluster  to create a cluster.    You will be redirected to the Cloudbreak dashboard, and a new tile representing your cluster will appear at the top of the page.    Related links  Flex support subscription  Using custom blueprints    Default cluster security groups  Troubleshooting cluster creation     CIDR  (External)", 
            "title": "Creating a cluster on OpenStack"
        }, 
        {
            "location": "/os-create/index.html#advanced-cluster-options", 
            "text": "Click on  Advanced  to view and enter additional configuration options", 
            "title": "Advanced cluster options"
        }, 
        {
            "location": "/os-create/index.html#availability-zone", 
            "text": "Choose one of the availability zones within the selected region.", 
            "title": "Availability zone"
        }, 
        {
            "location": "/os-create/index.html#enable-lifetime-management", 
            "text": "Check this option if you would like your cluster to be automatically terminated after a specific amount of time (defined as \"Time to Live\" in minutes).", 
            "title": "Enable lifetime management"
        }, 
        {
            "location": "/os-create/index.html#tags", 
            "text": "You can optionally add tags, which will help you find your cluster-related resources, such as VMs, in your cloud provider account.   By default, the following tags are created:     Tag  Description      cb-version  Cloudbreak version    Owner  Your Cloudbreak admin email.    cb-account-name  Your automatically generated Cloudbreak account name stored in the identity server.    cb-user-name  Your Cloudbreak admin email.     For more information, refer to  Tagging resources .  Related links       Tagging resources", 
            "title": "Tags"
        }, 
        {
            "location": "/os-create/index.html#choose-image-catalog", 
            "text": "By default,  Choose Image Catalog  is set to the default image catalog that is provided with Cloudbreak. If you would like to use a different image catalog, you must first create and register it. For complete instructions, refer to  Using custom images .  Related links      Using custom images", 
            "title": "Choose image catalog"
        }, 
        {
            "location": "/os-create/index.html#choose-image-type", 
            "text": "Cloudbreak supports the following types of images for launching clusters:     Image type  Description  Default images provided  Support for custom images      Prewarmed Image  By default, Cloudbreak launches clusters from prewarmed images. Prewarmed images include the operating system as well as Ambari and HDP/HDF. The Ambari and HDP/HDF version used by prewarmed images cannot be customized.  Yes  No    Base Image  Base images include default configuration and default tooling. These images include the operating system but do not include Ambari or HDP/HDF software.  Yes  Yes     By default, Cloudbreak uses the included default  prewarmed images , which include the operating system, as well as\nAmbari and HDP/HDF packages installed. You can optionally select the  base image  option if you would like to:   Use an Ambari and HDP/HDF versions different than what the prewarmed image includes and/or    Choose a previously created custom base image", 
            "title": "Choose image type"
        }, 
        {
            "location": "/os-create/index.html#choose-image", 
            "text": "If under  Choose image catalog , you selected a custom image catalog, under  Choose Image  you can select an image from that catalog. For complete instructions, refer to  Using custom images .   If you are trying to customize Ambari and HDP/HDF versions, you can ignore the  Choose Image  option; in this case default base image is used.", 
            "title": "Choose image"
        }, 
        {
            "location": "/os-create/index.html#ambari-repo-specification", 
            "text": "If you would like to use a custom Ambari version, provide the following information:      Parameter  Description  Example      Version  Enter Ambari version.  2.6.1.3    Repo Url  Provide a URL to the Ambari version repo that you would like to use.  http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.3    Repo Gpg Key Url  Provide a URL to the repo GPG key. Each stable RPM package that is published by CentOS Project is signed with a GPG signature. By default, yum and the graphical update tools will verify these signatures and refuse to install any packages that are not signed, or have an incorrect signature.  http://public-repo-1.hortonworks.com/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins", 
            "title": "Ambari repo specification"
        }, 
        {
            "location": "/os-create/index.html#hdphdf-repo-specification", 
            "text": "If you would like to use a custom HDP or HDF version, provide the following information:      Parameter  Description  Example      Stack  This is populated by default based on the \"Platform Version\" parameter.  HDP    Version  This is populated by default based on the \"Platform Version\" parameter.  2.6    OS  Operating system.  centos7 (Azure, GCP, OpenStack) or amazonlinux (AWS)    Repository Version  Enter repository version.  2.6.4.0-91    Version Definition File  Enter the URL of the VDF file.  http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.6.4.0/HDP-2.6.4.0-91.xml    (HDF only) MPack Url  (HDF only) Provide MPack URL.  http://public-repo-1.hortonworks.com/HDF/centos7/3.x/updates/3.1.1.0/tars/hdf_ambari_mp/hdf-ambari-mpack-3.1.1.0-35.tar.gz    Enable Ambari Server to download and install GPL Licensed LZO packages?  (Optional, only available if using Ambari 2.6.1.0 or newer) Use this option to enable LZO compression in your HDP/HDF cluster. LZO is a lossless data compression library that favors speed over compression ratio. Ambari does not install nor enable LZO compression libraries by default, and must be explicitly configured to do so. For more information, refer to  Enabling LZO .      If you choose to use a base image with custom Ambari and/or HDP/HDF version, Cloudbreak validates the information entered. When Cloudbreak detects that the information entered is incorrect, it displays a warning marked with the   sign. You should review all the warnings before proceeding and make sure that the information that you entered is correct. If you choose to proceed in spite of the warnings, check \"Ignore repository warnings\".    Related links       Using custom images", 
            "title": "HDP/HDF repo specification"
        }, 
        {
            "location": "/os-create/index.html#recipes", 
            "text": "This option allows you to select previously uploaded recipes (scripts that can be run pre or post cluster deployment) for each host group. For more information on recipes, refer to  Using custom scripts (recipes) .   Related links       Using custom scripts (recipes)", 
            "title": "Recipes"
        }, 
        {
            "location": "/os-create/index.html#management-packs", 
            "text": "This option allows you to select previously uploaded management packs. For more information on management packs, refer to  Using management packs .   Related links       Using management packs", 
            "title": "Management packs"
        }, 
        {
            "location": "/os-create/index.html#external-sources", 
            "text": "You can register external sources with Cloudbreak, and then select and attach them during cluster create. To register external sources with Cloudbreak, refer to:   Using an external authentication source       Using an external database     Register a proxy", 
            "title": "External sources"
        }, 
        {
            "location": "/os-create/index.html#custom-properties", 
            "text": "This option allows you to set custom properties based on the template defined in your custom blueprint. For more information, refer to  Set custom properties .   Related links       Set custom properties", 
            "title": "Custom properties"
        }, 
        {
            "location": "/os-create/index.html#single-sign-on-sso", 
            "text": "This option allows you to configure the gateway to be the SSO identity provider.    This option is technical preview.    For more information, refer to  Configuring the Gateway  documentation.", 
            "title": "Single sign-on (SSO)"
        }, 
        {
            "location": "/os-create/index.html#ambari-server-master-key", 
            "text": "The Ambari server master key is used to configure Ambari to encrypt database and Kerberos credentials that are retained by Ambari as part of the Ambari setup.", 
            "title": "Ambari server master key"
        }, 
        {
            "location": "/os-create/index.html#enable-kerberos-security", 
            "text": "Select this option to enable Kerberos for your cluster. For information about available Kerberos options, refer to  Enabling Kerberos security .   Related links       Enabling Kerberos security     Next: Access Cluster", 
            "title": "Enable Kerberos security"
        }, 
        {
            "location": "/gateway/index.html", 
            "text": "Configuring the gateway\n\n\nWhen creating a cluster, Cloudbreak installs and configures a gateway, powered by \nApache Knox\n, to protect access to the cluster resources:\n\n\n\n\nThis gateway is installed on the same host as the Ambari server.  \n\n\n\n\nBy default, transport layer security on the gateway endpoint is via a \nself-signed SSL certificate\n on port 8443. This is illustrated on the following diagram:   \n\n\n \n\n\n\n\n\n\nThe cluster resources that are accessible through the gateway are determined by the settings provided on the \nGateway Configuration\n page of the basic create cluster wizard.  \n\n\n\n\nBy default, the gateway is deployed and \nAmbari\n is proxied through the gateway.  \n\n\nThe choice of cluster services to expose and proxy through the gateway depends on your blueprint. Cloudbreak analyzes your blueprint and provides a list of services that can be exposed through the gateway. You should review this list and select the services that should be proxied through the gateway.  \n\n\nIf you do not enable the gateway, or you do not expose Ambari (or any other service) through the gateway, you must configure access to those services on the security group on your own. \n\n\n\n\nServices available via gateway\n\n\nThe following cluster resources are available for access via the gateway endpoint:\n\n\n\n\n\n\n\n\nCluster resource\n\n\nURL\n\n\n\n\n\n\n\n\n\n\nAmbari\n\n\nhttps://{gateway-host}:8443/{cluster-name}/{topology-name}/ambari/\n\n\n\n\n\n\nHive and Hive Server Interactive\n*\n\n\nhttps://{gateway-host}:8443/{cluster-name}/{topology-name}/hive/\n\n\n\n\n\n\nJob History Server\n\n\nhttps://{gateway-host}:8443/{cluster-name}/{topology-name}/jobhistory/\n\n\n\n\n\n\nName Node\n\n\nhttps://{gateway-host}:8443/{cluster-name}/{topology-name}/hdfs/\n\n\n\n\n\n\nWebHDFS\n\n\nhttps://{gateway-host}:8443/{cluster-name}/{topology-name}/webhdfs/\n\n\n\n\n\n\nResource Manager\n\n\nhttps://{gateway-host}:8443/{cluster-name}/{topology-name}/yarn/\n\n\n\n\n\n\nSpark History Server\n\n\nhttps://{gateway-host}:8443/{cluster-name}/{topology-name}/sparkhistory/\n\n\n\n\n\n\nZeppelin\n\n\nhttps://{gateway-host}:8443/{cluster-name}/{topology-name}/zeppelin/\n\n\n\n\n\n\n\n\n*\n Refer to \nAccessing Hive via JDBC\n for more information.\n\n\nThe following applies:\n\n\n\n\nThe \ngateway-host\n is the IP address or hostname of the Ambari server node.  \n\n\nThe \ncluster-name\n is the name of the cluster.  \n\n\nThe \ntopology-name\n is the name of the gateway topology that you entered when creating the cluster. By default this is set to \ndb-proxy\n.    \n\n\n\n\nConfigure the gateway\n\n\nWhen creating a cluster, you can configure the gateway on the \nGateway Configuration\n page of the basic create cluster wizard. \n\n\nSteps\n\n\n\n\n\n\nIn the create cluster wizard, navigate to the \nGateway Configuration\n page: \n\n\n  \n\n\n\n\n\n\nUnder \nGateway Topology\n:\n\n\n\n\nThe gateway option is enabled by default.  \n\n\nThe \nName\n for your gateway is set to \ndb-proxy\n by default. You can update it if you would like. This name is used in the URLs for the cluster resources, as described in \nServices available via gateway\n. \n\n\nUnder \nExposable Services\n, the choice of cluster services to expose and proxy through the gateway depends on your blueprint. Cloudbreak analyzes your blueprint and provides a list of services that can be exposed through the gateway. You should review this list and select the services that should be proxied through the gateway. By default, only Ambari is exposed through the gateway. \n\n\nWe recommend that you expose the following services through the gateway:\n\n\nAnalytics blueprint\n: Hive and Zeppelin\n\n\nETL blueprint\n: No additional services need to be exposed\n\n\nData science\n: Spark and Zeppelin   \n\n\n\n\n\n\n\n\n\n\n\n\nUnder \nExposable Services\n, use the dropdown to select services that should be exposed via the gateway. To expose a service, select it and click \nExpose\n. Select \nALL\n to expose all. \n\n\n\n\n\n\nObtain URLs for cluster resources\n\n\nOnce your cluster is running, you can obtain Ambari URL and the URLs of the cluster services from the \nGateway\n tab in the cluster details:\n\n\n\n\nThis tab is only available when gateway is enabled. \n\n\n\n\n\n\nThe URL structure is as described in \nServices available via gateway\n.\n\n\nConfigure single sign-on (SSO)\n\n\nWhen creating a cluster, if you selected to configure a gateway, on the advanced \nGateway Configuration\n page of the advanced create cluster wizard, you can also configure the gateway to be the SSO identity provider. \n\n\n\n\nThis option is technical preview. \n\n\n\n\nPrerequisites\n\n\nYou must have an existing authentication source (LDAP or AD) and register it with Cloudbreak, as described in \nUsing an external authentication source for clusters\n. \n\n\nSteps\n\n\n\n\nIn the create cluster wizard, select the advanced mode.  \n\n\nOn the \nExternal Sources\n page, under \nConfigure Authentication\n, select to attach a previously configured LDAP to the cluster.  \n\n\nOn the \nGateway Configuration\n page, under \nSingle Sign On (SSO)\n, click the toggle button to enable SSO.", 
            "title": "Configure the gateway"
        }, 
        {
            "location": "/gateway/index.html#configuring-the-gateway", 
            "text": "When creating a cluster, Cloudbreak installs and configures a gateway, powered by  Apache Knox , to protect access to the cluster resources:   This gateway is installed on the same host as the Ambari server.     By default, transport layer security on the gateway endpoint is via a  self-signed SSL certificate  on port 8443. This is illustrated on the following diagram:          The cluster resources that are accessible through the gateway are determined by the settings provided on the  Gateway Configuration  page of the basic create cluster wizard.     By default, the gateway is deployed and  Ambari  is proxied through the gateway.    The choice of cluster services to expose and proxy through the gateway depends on your blueprint. Cloudbreak analyzes your blueprint and provides a list of services that can be exposed through the gateway. You should review this list and select the services that should be proxied through the gateway.    If you do not enable the gateway, or you do not expose Ambari (or any other service) through the gateway, you must configure access to those services on the security group on your own.", 
            "title": "Configuring the gateway"
        }, 
        {
            "location": "/gateway/index.html#services-available-via-gateway", 
            "text": "The following cluster resources are available for access via the gateway endpoint:     Cluster resource  URL      Ambari  https://{gateway-host}:8443/{cluster-name}/{topology-name}/ambari/    Hive and Hive Server Interactive *  https://{gateway-host}:8443/{cluster-name}/{topology-name}/hive/    Job History Server  https://{gateway-host}:8443/{cluster-name}/{topology-name}/jobhistory/    Name Node  https://{gateway-host}:8443/{cluster-name}/{topology-name}/hdfs/    WebHDFS  https://{gateway-host}:8443/{cluster-name}/{topology-name}/webhdfs/    Resource Manager  https://{gateway-host}:8443/{cluster-name}/{topology-name}/yarn/    Spark History Server  https://{gateway-host}:8443/{cluster-name}/{topology-name}/sparkhistory/    Zeppelin  https://{gateway-host}:8443/{cluster-name}/{topology-name}/zeppelin/     *  Refer to  Accessing Hive via JDBC  for more information.  The following applies:   The  gateway-host  is the IP address or hostname of the Ambari server node.    The  cluster-name  is the name of the cluster.    The  topology-name  is the name of the gateway topology that you entered when creating the cluster. By default this is set to  db-proxy .", 
            "title": "Services available via gateway"
        }, 
        {
            "location": "/gateway/index.html#configure-the-gateway", 
            "text": "When creating a cluster, you can configure the gateway on the  Gateway Configuration  page of the basic create cluster wizard.   Steps    In the create cluster wizard, navigate to the  Gateway Configuration  page:         Under  Gateway Topology :   The gateway option is enabled by default.    The  Name  for your gateway is set to  db-proxy  by default. You can update it if you would like. This name is used in the URLs for the cluster resources, as described in  Services available via gateway .   Under  Exposable Services , the choice of cluster services to expose and proxy through the gateway depends on your blueprint. Cloudbreak analyzes your blueprint and provides a list of services that can be exposed through the gateway. You should review this list and select the services that should be proxied through the gateway. By default, only Ambari is exposed through the gateway.   We recommend that you expose the following services through the gateway:  Analytics blueprint : Hive and Zeppelin  ETL blueprint : No additional services need to be exposed  Data science : Spark and Zeppelin          Under  Exposable Services , use the dropdown to select services that should be exposed via the gateway. To expose a service, select it and click  Expose . Select  ALL  to expose all.", 
            "title": "Configure the gateway"
        }, 
        {
            "location": "/gateway/index.html#obtain-urls-for-cluster-resources", 
            "text": "Once your cluster is running, you can obtain Ambari URL and the URLs of the cluster services from the  Gateway  tab in the cluster details:   This tab is only available when gateway is enabled.     The URL structure is as described in  Services available via gateway .", 
            "title": "Obtain URLs for cluster resources"
        }, 
        {
            "location": "/gateway/index.html#configure-single-sign-on-sso", 
            "text": "When creating a cluster, if you selected to configure a gateway, on the advanced  Gateway Configuration  page of the advanced create cluster wizard, you can also configure the gateway to be the SSO identity provider.    This option is technical preview.    Prerequisites  You must have an existing authentication source (LDAP or AD) and register it with Cloudbreak, as described in  Using an external authentication source for clusters .   Steps   In the create cluster wizard, select the advanced mode.    On the  External Sources  page, under  Configure Authentication , select to attach a previously configured LDAP to the cluster.    On the  Gateway Configuration  page, under  Single Sign On (SSO) , click the toggle button to enable SSO.", 
            "title": "Configure single sign-on (SSO)"
        }, 
        {
            "location": "/blueprints/index.html", 
            "text": "Using custom blueprints\n\n\nThis option allows you to create and save your custom blueprints. \n\n\nAmbari blueprints\n are your declarative definition of your HDP or HDF cluster, defining the host groups and which components to install on which host group. Ambari uses them as a base for your clusters. \n\n\nYou have two options concerning using blueprints with Cloudbreak:\n\n\n\n\nUse one of the pre-defined blueprints\n: To use one of the default blueprints, simply select them when creating a cluster. The option is available on the \nGeneral Configuration\n page. First select the \nStack Version\n and then select your chosen blueprint under \nCluster Type\n. For the list of default blueprints, refer to \nDefault cluster configurations\n.       \n\n\nAdd your custom blueprint\n by uploading a JSON file or pasting the JSON text. \n\n\n\n\nWe recommend that you review the default blueprints to check if they meet your requirements. You can do this by selecting \nBlueprints\n from the navigation pane in the Cloudbreak web UI or by reading the documentation below.\n\n\nCreating blueprints\n\n\nAmbari blueprints are specified in JSON format. A blueprint can be exported from a running Ambari cluster and can be reused in Cloudbreak after slight modifications. When a blueprint is exported, it includes  some hardcoded configurations such as domain names, memory configurations, and so on, that are not applicable to the Cloudbreak cluster. There is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually. \n\n\nIn general, the blueprint should include the following elements:\n\n\n\"Blueprints\": {\n\u2002\u2002\u2002\u2002\"blueprint_name\": \"hdp-small-default\",\n\u2002\u2002\u2002\u2002\"stack_name\": \"HDP\",\n\u2002\u2002\u2002\u2002\"stack_version\": \"2.6\"\n\u2002\u2002},\n\u2002\u2002\"settings\": [],\n\u2002\u2002\"configurations\": [],\n\u2002\u2002\"host_groups\": [\n\u2002\u2002{\n      \"name\": \"master\",\n      \"configurations\": [],\n      \"components\": []\n    },\n    {\n      \"name\": \"worker\",\n      \"configurations\": [],\n      \"components\": [ ]\n    },\n    {\n      \"name\": \"compute\",\n      \"configurations\": [],\n      \"components\": []\n    }\n\u2002\u2002 ]\n\u2002\u2002}\n\n\n\nFor correct blueprint layout and other information about Ambari blueprints, refer to the \nAmbari cwiki\n page. \n\n\n\n    \nCreating Blueprints for Ambari 2.6.1+\n\n    \n\nAmbari 2.6.1 or newer does not install the mysqlconnector; therefore, when creating a blueprint for Ambari 2.6.1 or newer \nyou should not include the MYSQL_SERVER component\n for Hive Metastore in your blueprint. Instead, you have two options:\n\n\n\nConfigure an external RDBMS instance for Hive Metastore and include the JDBC connection information in your blueprint. If you choose to use an external database that is not PostgreSQL (such as Oracle, mysql) you must also set up Ambari with the appropriate connector; to do this, create a pre-ambari-start recipe and pass it when creating a cluster.\n\n\nIf a remote Hive RDBMS is not provided, Cloudbreak installs a Postgres instance and configures it for Hive Metastore during the cluster launch.\n\n\n\nFor information on how to configure an external database and pass your external database connection parameters, refer to \nAmbari blueprint\n documentation.\n\n\n\n\n\n\nCloudbreak requires you to define an additional element in the blueprint called \"blueprint_name\".\u2002This should be a unique name within Cloudbreak list of blueprints. For example:\n\n\n\"Blueprints\": {\n\u2002\u2002\u2002\u2002\"blueprint_name\": \"hdp-small-default\",\n\u2002\u2002\u2002\u2002\"stack_name\": \"HDP\",\n\u2002\u2002\u2002\u2002\"stack_version\": \"2.6\"\n\u2002\u2002},\n\u2002\u2002\"settings\": [],\n\u2002\u2002\"configurations\": [],\n\u2002\u2002\"host_groups\": [\n\u2002\u2002...\n\n\n\n\nThe \"blueprint_name\" is not included in the Ambari export. \n\n\nAfter you provide the blueprint to Cloudbreak, the host groups in the JSON will be mapped to a set of instances when starting the cluster, and the specified services and components will be installed on the corresponding nodes. It is not necessary to define a complete configuration in the blueprint. If a configuration is missing, Ambari will use a default value. \n\n\nHere are a few \nblueprint examples\n. You can also refer to the default blueprints provided in the Cloudbreak UI.\n\n\nRelated links\n\n\nBlueprint examples\n (Hortonworks)   \n\n\nAmbari cwiki\n (External)  \n\n\nCreating dynamic blueprints\n\n\nCloudbreak allows you to create \ndynamic blueprints\n, which include templating: the values of the variables specified in the blueprint are dynamically replaced in the cluster creation phase, picking up the parameter values that you provided in the Cloudbreak UI or CLI.\nCloudbreak supports mustache kind of templating with {{{variable}}} syntax. \n\n\n\n\nYou cannot use functions in the blueprint file; only variable injection is supported.\n\n\n\n\nExternal authentication source (LDAP/AD)\n\n\nWhen using \nexternal authentication sources\n, the following variables can be specified in your blueprint for replacement:\n\n\n\n\n\n\n\n\nVariable\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nldap.connectionURL\n\n\nthe URL of the LDAP (host:port)\n\n\nldap://10.1.1.1:389\n\n\n\n\n\n\nldap.bindDn\n\n\nThe root Distinguished Name to search in the directory for users\n\n\nCN=Administrator,CN=Users,DC=ad,DC=hdc,DC=com\n\n\n\n\n\n\nldap.bindPassword\n\n\nThe root Distinguished Name password\n\n\nPassword1234!\n\n\n\n\n\n\nldap.directoryType\n\n\nThe directory of type\n\n\nLDAP or ACTIVE_DIRECTORY\n\n\n\n\n\n\nldap.userSearchBase\n\n\nUser search base\n\n\nCN=Users,DC=ad,DC=hdc,DC=com\n\n\n\n\n\n\nldap.userNameAttribute\n\n\nUsername attribute\n\n\ncn\n\n\n\n\n\n\nldap.userObjectClass\n\n\nObject class for users\n\n\nperson\n\n\n\n\n\n\nldap.groupSearchBase\n\n\nGroup search base\n\n\nOU=Groups,DC=ad,DC=hdc,DC=com\n\n\n\n\n\n\nldap.groupNameAttribute\n\n\nGroup attribute\n\n\ncb\n\n\n\n\n\n\nldap.groupObjectClass\n\n\nGroup object class\n\n\ngroup\n\n\n\n\n\n\nldap.groupMemberAttribute\n\n\nAttribute for membershio\n\n\nmember\n\n\n\n\n\n\nldap.domain\n\n\nYour domain\n\n\nexample.com\n\n\n\n\n\n\n\n\nExternal database (RDBMS)\n\n\nWhen using \nexternal databases\n, the following variables can be specified in your blueprint for replacement:\n\n\n\n\n\n\n\n\nVariable\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nrds.[type].connectionString\n\n\nThe jdbc url to the RDBMS\n\n\njdbc:postgresql://db.test:5432/test\n\n\n\n\n\n\nrds.[type].connectionDriver\n\n\nThe connection driver\n\n\norg.postgresql.Driver\n\n\n\n\n\n\nrds.[type].connectionUserName\n\n\nThe user name to the database\n\n\nadmin\n\n\n\n\n\n\nrds.[type].connectionPassword\n\n\nThe password for the connection\n\n\nPassword1234!\n\n\n\n\n\n\nrds.[type].subprotocol\n\n\nParsed from jdbc url\n\n\npostgres\n\n\n\n\n\n\nrds.[type].databaseEngine\n\n\nCapital database name\n\n\nPOSTGRES\n\n\n\n\n\n\n\n\nUpload blueprints\n\n\nOnce you have your blueprint ready, perform these steps.\n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, select \nBlueprints\n from the navigation pane. \n\n\n\n\nTo add your own blueprint, click \nCreate Blueprint\n and enter the following parameters:\n\n\n\n\n\n\n\n\nParameter\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your blueprint.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description for your blueprint.\n\n\n\n\n\n\nBlueprint Source\n\n\nSelect one of: \nText\n: Paste blueprint in JSON format.\n \nFile\n: Upload a file that contains the blueprint.\n \nURL\n: Specify the URL for your blueprint.\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nTo use the uploaded blueprints, select it when creating a cluster. The option is available on the \nGeneral Configuration\n page. First select the \nPlatform Version\n and then select your chosen blueprint under \nCluster Type\n.", 
            "title": "Use custom blueprints"
        }, 
        {
            "location": "/blueprints/index.html#using-custom-blueprints", 
            "text": "This option allows you to create and save your custom blueprints.   Ambari blueprints  are your declarative definition of your HDP or HDF cluster, defining the host groups and which components to install on which host group. Ambari uses them as a base for your clusters.   You have two options concerning using blueprints with Cloudbreak:   Use one of the pre-defined blueprints : To use one of the default blueprints, simply select them when creating a cluster. The option is available on the  General Configuration  page. First select the  Stack Version  and then select your chosen blueprint under  Cluster Type . For the list of default blueprints, refer to  Default cluster configurations .         Add your custom blueprint  by uploading a JSON file or pasting the JSON text.    We recommend that you review the default blueprints to check if they meet your requirements. You can do this by selecting  Blueprints  from the navigation pane in the Cloudbreak web UI or by reading the documentation below.", 
            "title": "Using custom blueprints"
        }, 
        {
            "location": "/blueprints/index.html#creating-blueprints", 
            "text": "Ambari blueprints are specified in JSON format. A blueprint can be exported from a running Ambari cluster and can be reused in Cloudbreak after slight modifications. When a blueprint is exported, it includes  some hardcoded configurations such as domain names, memory configurations, and so on, that are not applicable to the Cloudbreak cluster. There is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the modifications have to be done manually.   In general, the blueprint should include the following elements:  \"Blueprints\": {\n\u2002\u2002\u2002\u2002\"blueprint_name\": \"hdp-small-default\",\n\u2002\u2002\u2002\u2002\"stack_name\": \"HDP\",\n\u2002\u2002\u2002\u2002\"stack_version\": \"2.6\"\n\u2002\u2002},\n\u2002\u2002\"settings\": [],\n\u2002\u2002\"configurations\": [],\n\u2002\u2002\"host_groups\": [\n\u2002\u2002{\n      \"name\": \"master\",\n      \"configurations\": [],\n      \"components\": []\n    },\n    {\n      \"name\": \"worker\",\n      \"configurations\": [],\n      \"components\": [ ]\n    },\n    {\n      \"name\": \"compute\",\n      \"configurations\": [],\n      \"components\": []\n    }\n\u2002\u2002 ]\n\u2002\u2002}  For correct blueprint layout and other information about Ambari blueprints, refer to the  Ambari cwiki  page.   \n     Creating Blueprints for Ambari 2.6.1+ \n     \nAmbari 2.6.1 or newer does not install the mysqlconnector; therefore, when creating a blueprint for Ambari 2.6.1 or newer  you should not include the MYSQL_SERVER component  for Hive Metastore in your blueprint. Instead, you have two options:  Configure an external RDBMS instance for Hive Metastore and include the JDBC connection information in your blueprint. If you choose to use an external database that is not PostgreSQL (such as Oracle, mysql) you must also set up Ambari with the appropriate connector; to do this, create a pre-ambari-start recipe and pass it when creating a cluster.  If a remote Hive RDBMS is not provided, Cloudbreak installs a Postgres instance and configures it for Hive Metastore during the cluster launch.  \nFor information on how to configure an external database and pass your external database connection parameters, refer to  Ambari blueprint  documentation.   Cloudbreak requires you to define an additional element in the blueprint called \"blueprint_name\".\u2002This should be a unique name within Cloudbreak list of blueprints. For example:  \"Blueprints\": {\n\u2002\u2002\u2002\u2002\"blueprint_name\": \"hdp-small-default\",\n\u2002\u2002\u2002\u2002\"stack_name\": \"HDP\",\n\u2002\u2002\u2002\u2002\"stack_version\": \"2.6\"\n\u2002\u2002},\n\u2002\u2002\"settings\": [],\n\u2002\u2002\"configurations\": [],\n\u2002\u2002\"host_groups\": [\n\u2002\u2002...  The \"blueprint_name\" is not included in the Ambari export.   After you provide the blueprint to Cloudbreak, the host groups in the JSON will be mapped to a set of instances when starting the cluster, and the specified services and components will be installed on the corresponding nodes. It is not necessary to define a complete configuration in the blueprint. If a configuration is missing, Ambari will use a default value.   Here are a few  blueprint examples . You can also refer to the default blueprints provided in the Cloudbreak UI.  Related links  Blueprint examples  (Hortonworks)     Ambari cwiki  (External)", 
            "title": "Creating blueprints"
        }, 
        {
            "location": "/blueprints/index.html#creating-dynamic-blueprints", 
            "text": "Cloudbreak allows you to create  dynamic blueprints , which include templating: the values of the variables specified in the blueprint are dynamically replaced in the cluster creation phase, picking up the parameter values that you provided in the Cloudbreak UI or CLI.\nCloudbreak supports mustache kind of templating with {{{variable}}} syntax.    You cannot use functions in the blueprint file; only variable injection is supported.", 
            "title": "Creating dynamic blueprints"
        }, 
        {
            "location": "/blueprints/index.html#external-authentication-source-ldapad", 
            "text": "When using  external authentication sources , the following variables can be specified in your blueprint for replacement:     Variable  Description  Example      ldap.connectionURL  the URL of the LDAP (host:port)  ldap://10.1.1.1:389    ldap.bindDn  The root Distinguished Name to search in the directory for users  CN=Administrator,CN=Users,DC=ad,DC=hdc,DC=com    ldap.bindPassword  The root Distinguished Name password  Password1234!    ldap.directoryType  The directory of type  LDAP or ACTIVE_DIRECTORY    ldap.userSearchBase  User search base  CN=Users,DC=ad,DC=hdc,DC=com    ldap.userNameAttribute  Username attribute  cn    ldap.userObjectClass  Object class for users  person    ldap.groupSearchBase  Group search base  OU=Groups,DC=ad,DC=hdc,DC=com    ldap.groupNameAttribute  Group attribute  cb    ldap.groupObjectClass  Group object class  group    ldap.groupMemberAttribute  Attribute for membershio  member    ldap.domain  Your domain  example.com", 
            "title": "External authentication source (LDAP/AD)"
        }, 
        {
            "location": "/blueprints/index.html#external-database-rdbms", 
            "text": "When using  external databases , the following variables can be specified in your blueprint for replacement:     Variable  Description  Example      rds.[type].connectionString  The jdbc url to the RDBMS  jdbc:postgresql://db.test:5432/test    rds.[type].connectionDriver  The connection driver  org.postgresql.Driver    rds.[type].connectionUserName  The user name to the database  admin    rds.[type].connectionPassword  The password for the connection  Password1234!    rds.[type].subprotocol  Parsed from jdbc url  postgres    rds.[type].databaseEngine  Capital database name  POSTGRES", 
            "title": "External database (RDBMS)"
        }, 
        {
            "location": "/blueprints/index.html#upload-blueprints", 
            "text": "Once you have your blueprint ready, perform these steps.  Steps   In the Cloudbreak UI, select  Blueprints  from the navigation pane.    To add your own blueprint, click  Create Blueprint  and enter the following parameters:     Parameter  Value      Name  Enter a name for your blueprint.    Description  (Optional) Enter a description for your blueprint.    Blueprint Source  Select one of:  Text : Paste blueprint in JSON format.   File : Upload a file that contains the blueprint.   URL : Specify the URL for your blueprint.          To use the uploaded blueprints, select it when creating a cluster. The option is available on the  General Configuration  page. First select the  Platform Version  and then select your chosen blueprint under  Cluster Type .", 
            "title": "Upload blueprints"
        }, 
        {
            "location": "/recipes/index.html", 
            "text": "Creating custom scripts (recipes)\n\n\nAlthough Cloudbreak lets you provision clusters in the cloud based on custom Ambari blueprints, Cloudbreak provisioning options don't consider all possible use cases. For that reason, we introduced recipes. \n\n\nA recipe is a script that runs on all nodes of a selected node group at a specific time. You can use recipes for tasks such as installing additional software or performing advanced cluster configuration. For example, you can use a recipe to put a JAR file on the Hadoop classpath.\n\n\nAvailable recipe execution times are:  \n\n\n\n\nBefore Ambari server start    \n\n\nAfter Ambari server start    \n\n\nAfter cluster installation    \n\n\nBefore cluster termination   \n\n\n\n\nYou can upload your recipes to Cloudbreak via the UI or CLI. Then, when creating a cluster, you can optionally attach one or more \"recipes\" and they will be executed on a specific host group at a specified time. \n\n\nWriting recipes\n\n\nWhen using recipes, consider the following:\n\n\n\n\nThe scripts will be executed on the node types you specify (such as \"master\", \"worker\", \"compute\"). If you want to run a a script on all nodes, define the recipe one per node type.  \n\n\nThe script will execute on all of the nodes of that type as root.  \n\n\nIn order to be executed, your script must be in a network location which is accessible from the cloud controller and cluster instances VPC.  \n\n\nMake sure to follow Linux best practices when creating your scripts. For example, don't forget to script \"Yes\" auto-answers where needed.  \n\n\nDo not execute yum update \u2013y since it may update other components on the instances (such as salt), which can create unintended or unstable behavior.   \n\n\nThe scripts will be executed as root. The recipe output is written to \n/var/log/recipes\n on each node on which it was executed.\n\n\n\n\nSample recipe for yum proxy setting\n\n\n#!/bin/bash\ncat \n /etc/yum.conf \nENDOF\nproxy=http://10.0.0.133:3128\nENDOF\n\n\n\n\nAdd recipes\n\n\nIn order to use your recipe for clusters, you must register it first by using the steps below.\n\n\nSteps\n\n\n\n\n\n\nPlace your script in a network location accessible from Cloudbreak and cluster instances virtual network. \n\n\n\n\n\n\nSelect \nExternal Sources \n Recipes\n from the navigation menu. \n\n\n\n\n\n\nClick on \nCreate Recipe\n. \n\n\n\n\n\n\nProvide the following:\n\n\n\n\n\n\n\n\nParameter\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your recipe.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description for your recipe.\n\n\n\n\n\n\nExecution Type\n\n\nSelect one of the following options: \npre-ambari-start\n: The script will be executed prior to Ambari server start.\npost-ambari-start\n: The script will be executed after Ambari server start but prior to cluster installation.\npost-cluster-install\n: The script will be executed after cluster deployment.\npre-termination\n: The script will be executed before cluster termination.\n\n\n\n\n\n\nScript\n\n\nSelect one of: \nScript\n: Paste the script.\n \nFile\n: Point to a file on your machine that contains the recipe.\n \nURL\n: Specify the URL for your recipe.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen creating a cluster, you can select previously added recipes on the advanced \nCluster Extensions\n page of the create cluster wizard. \n\n\n\n\n\n\nReusable rceipes\n\n\nInstall mysql connector recipe\n\n\nStarting from Ambari version 2.6, if you have 'MYSQL_SERVER' component in your blueprint, then you have to \nmanually install and register\n the 'mysql-connector-java.jar'.\n\n\nIf you would like to automate this process in Cloudbreak:\n\n\n\n\nReview the recipe content to ensure that the version of the connector provided in the recipe is as desired; if it is not adjust the version.     \n\n\nApply the recipe as \"pre-ambari-start\".  \n\n\n\n\nThe recipe content is:\n\n\n\n#!/bin/bash\n\ndownload_mysql_jdbc_driver() {\n  wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.39.tar.gz -P /tmp\n  tar xzf /tmp/mysql-connector-java-5.1.39.tar.gz -C /tmp/\n  cp /tmp/mysql-connector-java-5.1.39/mysql-connector-java-5.1.39-bin.jar /opt/jdbc-drivers/mysql-connector-java.jar\n}\n\nmain() {\n  download_mysql_jdbc_driver\n}\n\nmain", 
            "title": "Use custom scripts (recipes)"
        }, 
        {
            "location": "/recipes/index.html#creating-custom-scripts-recipes", 
            "text": "Although Cloudbreak lets you provision clusters in the cloud based on custom Ambari blueprints, Cloudbreak provisioning options don't consider all possible use cases. For that reason, we introduced recipes.   A recipe is a script that runs on all nodes of a selected node group at a specific time. You can use recipes for tasks such as installing additional software or performing advanced cluster configuration. For example, you can use a recipe to put a JAR file on the Hadoop classpath.  Available recipe execution times are:     Before Ambari server start      After Ambari server start      After cluster installation      Before cluster termination      You can upload your recipes to Cloudbreak via the UI or CLI. Then, when creating a cluster, you can optionally attach one or more \"recipes\" and they will be executed on a specific host group at a specified time.", 
            "title": "Creating custom scripts (recipes)"
        }, 
        {
            "location": "/recipes/index.html#writing-recipes", 
            "text": "When using recipes, consider the following:   The scripts will be executed on the node types you specify (such as \"master\", \"worker\", \"compute\"). If you want to run a a script on all nodes, define the recipe one per node type.    The script will execute on all of the nodes of that type as root.    In order to be executed, your script must be in a network location which is accessible from the cloud controller and cluster instances VPC.    Make sure to follow Linux best practices when creating your scripts. For example, don't forget to script \"Yes\" auto-answers where needed.    Do not execute yum update \u2013y since it may update other components on the instances (such as salt), which can create unintended or unstable behavior.     The scripts will be executed as root. The recipe output is written to  /var/log/recipes  on each node on which it was executed.", 
            "title": "Writing recipes"
        }, 
        {
            "location": "/recipes/index.html#sample-recipe-for-yum-proxy-setting", 
            "text": "#!/bin/bash\ncat   /etc/yum.conf  ENDOF\nproxy=http://10.0.0.133:3128\nENDOF", 
            "title": "Sample recipe for yum proxy setting"
        }, 
        {
            "location": "/recipes/index.html#add-recipes", 
            "text": "In order to use your recipe for clusters, you must register it first by using the steps below.  Steps    Place your script in a network location accessible from Cloudbreak and cluster instances virtual network.     Select  External Sources   Recipes  from the navigation menu.     Click on  Create Recipe .     Provide the following:     Parameter  Value      Name  Enter a name for your recipe.    Description  (Optional) Enter a description for your recipe.    Execution Type  Select one of the following options:  pre-ambari-start : The script will be executed prior to Ambari server start. post-ambari-start : The script will be executed after Ambari server start but prior to cluster installation. post-cluster-install : The script will be executed after cluster deployment. pre-termination : The script will be executed before cluster termination.    Script  Select one of:  Script : Paste the script.   File : Point to a file on your machine that contains the recipe.   URL : Specify the URL for your recipe.       When creating a cluster, you can select previously added recipes on the advanced  Cluster Extensions  page of the create cluster wizard.", 
            "title": "Add recipes"
        }, 
        {
            "location": "/recipes/index.html#reusable-rceipes", 
            "text": "", 
            "title": "Reusable rceipes"
        }, 
        {
            "location": "/recipes/index.html#install-mysql-connector-recipe", 
            "text": "Starting from Ambari version 2.6, if you have 'MYSQL_SERVER' component in your blueprint, then you have to  manually install and register  the 'mysql-connector-java.jar'.  If you would like to automate this process in Cloudbreak:   Review the recipe content to ensure that the version of the connector provided in the recipe is as desired; if it is not adjust the version.       Apply the recipe as \"pre-ambari-start\".     The recipe content is:  \n#!/bin/bash\n\ndownload_mysql_jdbc_driver() {\n  wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.39.tar.gz -P /tmp\n  tar xzf /tmp/mysql-connector-java-5.1.39.tar.gz -C /tmp/\n  cp /tmp/mysql-connector-java-5.1.39/mysql-connector-java-5.1.39-bin.jar /opt/jdbc-drivers/mysql-connector-java.jar\n}\n\nmain() {\n  download_mysql_jdbc_driver\n}\n\nmain", 
            "title": "Install mysql connector recipe"
        }, 
        {
            "location": "/mpacks/index.html", 
            "text": "Using management packs\n\n\nManagement packs allow you to deploy a range of services to your Ambari-managed cluster. You can use a management pack to deploy a specific component or service, such as HDP Search, or to deploy an entire platform, such as HDF.\n\n\nCloudbreak supports using management packs, allowing you to register them in Cloudbreak web UI and CLI and then select to install them as part of cluster creation. \n\n\nFor general information on management packs, refer to \nAmbari cwiki: Management+Packs\n.  \n\n\nRelated Links\n\n\nAmbari cwiki: Management+Packs\n  \n\n\nAdd management pack\n\n\nIn order to have a management stack installed for a specific cluster, you must register it with Cloudbreak by using the steps below.\n\n\nSteps\n\n\n\n\n\n\nObtain the URL for the management pack tarball file that you want to register in Cloudbreak. The tarball  must be available in a location accessible to clusters created by Cloudbreak.  \n\n\n\n\n\n\nIn Cloudbreak web UI, select \nExternal Sources \n Management Packs\n from the navigation menu. \n\n\n\n\n\n\nClick on \nRegister Management Pack\n. \n\n\n\n\n\n\nProvide the following:\n\n\n\n\n\n\n\n\nParameter\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your management pack.\n\n\n\n\n\n\nDescription\n\n\n(Optional) Enter a description.\n\n\n\n\n\n\nManagement pack URL\n\n\nProvide the URL to the location where the management pack tarball file is available.\n\n\n\n\n\n\nRemove all existing Ambari stack definitions prior to installing this Management Pack (\u201cmpack --purge\u201d).\n\n\n\n\n\n\n\n\nChecking this option allows you to purge any existing stack definition and should be included only when installing a stack management pack. Do not select this when installing an add-on service management pack.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen creating a cluster, on the advanced \nCluster Extensions\n page of the create cluster wizard, you can select one or more previously registered management packs. After selecting, click \nInstall\n to use the management pack for the cluster.", 
            "title": "Use management packs"
        }, 
        {
            "location": "/mpacks/index.html#using-management-packs", 
            "text": "Management packs allow you to deploy a range of services to your Ambari-managed cluster. You can use a management pack to deploy a specific component or service, such as HDP Search, or to deploy an entire platform, such as HDF.  Cloudbreak supports using management packs, allowing you to register them in Cloudbreak web UI and CLI and then select to install them as part of cluster creation.   For general information on management packs, refer to  Ambari cwiki: Management+Packs .    Related Links  Ambari cwiki: Management+Packs", 
            "title": "Using management packs"
        }, 
        {
            "location": "/mpacks/index.html#add-management-pack", 
            "text": "In order to have a management stack installed for a specific cluster, you must register it with Cloudbreak by using the steps below.  Steps    Obtain the URL for the management pack tarball file that you want to register in Cloudbreak. The tarball  must be available in a location accessible to clusters created by Cloudbreak.      In Cloudbreak web UI, select  External Sources   Management Packs  from the navigation menu.     Click on  Register Management Pack .     Provide the following:     Parameter  Value      Name  Enter a name for your management pack.    Description  (Optional) Enter a description.    Management pack URL  Provide the URL to the location where the management pack tarball file is available.    Remove all existing Ambari stack definitions prior to installing this Management Pack (\u201cmpack --purge\u201d).     Checking this option allows you to purge any existing stack definition and should be included only when installing a stack management pack. Do not select this when installing an add-on service management pack.        When creating a cluster, on the advanced  Cluster Extensions  page of the create cluster wizard, you can select one or more previously registered management packs. After selecting, click  Install  to use the management pack for the cluster.", 
            "title": "Add management pack"
        }, 
        {
            "location": "/images/index.html", 
            "text": "Using custom images\n\n\nDefault images are available for each supported cloud provider and region. The following table lists the default base images available:\n\n\n\n\n\n\n\n\nCloud provider\n\n\nDefault image\n\n\n\n\n\n\n\n\n\n\nAWS\n\n\nAmazon Linux 2017\n\n\n\n\n\n\nAzure\n\n\nCentOS 7\n\n\n\n\n\n\nGCP\n\n\nCentOS 7\n\n\n\n\n\n\nOpenStack\n\n\nCentOS 7\n\n\n\n\n\n\n\n\nSince these default images may not fit the requirements of some users (for example when user requirements include custom OS hardening, custom libraries, custom tooling, and so on) Cloudbreak allows you to use your own \ncustom base images\n.\n\n\nIn order to use your own custom base images you must:\n\n\n\n\nBuild your custom images  \n\n\nPrepare the custom image catalog JSON file and save it in a location accessible to the Cloudbreak VM  \n\n\nRegister your custom image catalog with Cloudbreak    \n\n\nSelect a custom image when creating a cluster  \n\n\n\n\n\n    \nImportant\n\n    \n\nOnly \nbase images\n can be created and registered as custom images. Do not create or register \nprewarmed images\n as custom images.\n\n\n\n\n\n\nBuild custom images\n\n\nRefer to \nCustom images for Cloudbreak\n for information on how to build custom images.\n\n\nThis repository includes instructions and scripts to help you build custom images. Once you have the images, refer to the documentation below for information on how to create an image catalog and register it with Cloudbreak.\n\n\nPrepare the image catalog\n\n\nOnce you've built the custom images, prepare your custom image catalog JSON file. Once your image catalog JSON file is ready, save it in a location accessible via HTTP/HTTPS.\n\n\nStructure of the image catalog JSON file\n\n\nThe image catalog JSON file includes the following two high-level sections:\n\n\n\n\nimages\n: Contains information about the created images. The burned images are stored in the \nbase-images\n section.  \n\n\nversions\n: Contains the \ncloudbreak\n entry, which includes mapping between Cloudbreak versions and the image identifiers of burned images available for these Cloudbreak versions.\n\n\n\n\n\n\nAfter adding your image(s) to the \nimages\n section, make sure to also update the \nversions\n section. \n\n\n\n\nImages section\n  \n\n\nThe burned images are stored in the \nbase-images\n sub-section of \nimages\n. The \nbase-images\n section stores one or more image \"records\". Every image \"record\" must contain the date, description, images, os, os_type, and uuid fields.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndate\n\n\nDate for your image catalog entry.\n\n\n\n\n\n\ndescription\n\n\nDescription for your image catalog entry.\n\n\n\n\n\n\nimages\n\n\nThe image sets by cloud provider. An image set must store the virtual machine image IDs by the related region of the provider (AWS, Azure) or contain one default image for all regions (GCP, OpenStack). The virtual machine image IDs come from the result of the image burning process and must be an existing identifier of a virtual machine image on the related provider side. For the providers which use global rather than per-region images, the region should be replaced with \ndefault\n.\n\n\n\n\n\n\nos\n\n\nThe operating system used in the image.\n\n\n\n\n\n\nos_type\n\n\nThe type of operating system which will be used to determine the default Ambari and HDP/HDF repositories to use. Set \nos_type\n to \"redhat6\" for amazonlinux or centos6 images. Set \nos_type\n to \"redhat7\" for centos7 or rhel7 images.\n\n\n\n\n\n\nuuid\n\n\nThe \nuuid\n field must be a unique identifier within the file. You can generate it or select it manually. The utility \nuuidgen\n available from your command line is a convenient way to generate a unique ID.\n\n\n\n\n\n\n\n\nVersions section\n  \n\n\nThe \nversions\n section includes a single \"cloudbreak\" entry, which maps the uuids to a specific Cloudbreak version:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nimages\n\n\nImage \nuuid\n, same as the one that you specified in the \nbase-images\n section.\n\n\n\n\n\n\nversions\n\n\nThe Cloudbreak version(s) for which you would like to use the images.\n\n\n\n\n\n\n\n\nExample image catalog JSON file\n\n\nHere is an example image catalog JSON file that includes two sets of custom base images:\n\n\n\n\nA custom base image for AWS:\n\n\nThat is using Amazon Linux operating system\n\n\nThat will use the Redhat 6 repos as default Ambari and HDP repositories during cluster create     \n\n\nHas a unique ID of \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n\n\nIs available for Cloudbreak 2.4.0    \n\n\n\n\n\n\nA custom base image for Azure, Google, and OpenStack:\n\n\nThat is using CentOS 7 operating system\n\n\nThat will use the Redhat 7 repos as default Ambari and HDP repositories during cluster create   \n\n\nHas a unique ID of \"f6e778fc-7f17-4535-9021-515351df3692\"\n\n\nIs available to Cloudbreak 2.4.0      \n\n\n\n\n\n\n\n\nYou can also download it from \nhere\n.\n\n\n\n{\n  \"images\": {\n    \"base-images\": [\n      {\n        \"date\": \"2017-10-13\",\n        \"description\": \"Cloudbreak official base image\",\n        \"images\": {\n          \"aws\": {\n            \"ap-northeast-1\": \"ami-78e9311e\",\n            \"ap-northeast-2\": \"ami-84b613ea\",\n            \"ap-southeast-1\": \"ami-75226716\",\n            \"ap-southeast-2\": \"ami-92ce23f0\",\n            \"eu-central-1\": \"ami-d95be5b6\",\n            \"eu-west-1\": \"ami-46429e3f\",\n            \"sa-east-1\": \"ami-86d5abea\",\n            \"us-east-1\": \"ami-51a2742b\",\n            \"us-west-1\": \"ami-21ccfe41\",\n            \"us-west-2\": \"ami-2a1cdc52\"\n          }\n        },\n        \"os\": \"amazonlinux\",\n        \"os_type\": \"redhat6\",\n        \"uuid\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n      },\n      {\n        \"date\": \"2017-10-13\",\n        \"description\": \"Cloudbreak official base image\",\n        \"images\": {\n          \"azure\": {\n            \"Australia East\": \"https://hwxaustraliaeast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Australia South East\": \"https://hwxaustralisoutheast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Brazil South\": \"https://sequenceiqbrazilsouth2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Canada Central\": \"https://sequenceiqcanadacentral.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Canada East\": \"https://sequenceiqcanadaeast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Central India\": \"https://hwxcentralindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Central US\": \"https://sequenceiqcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East Asia\": \"https://sequenceiqeastasia2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East US\": \"https://sequenceiqeastus12.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East US 2\": \"https://sequenceiqeastus22.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Japan East\": \"https://sequenceiqjapaneast2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Japan West\": \"https://sequenceiqjapanwest2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Korea Central\": \"https://hwxkoreacentral.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Korea South\": \"https://hwxkoreasouth.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"North Central US\": \"https://sequenceiqorthcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"North Europe\": \"https://sequenceiqnortheurope2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"South Central US\": \"https://sequenceiqouthcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"South India\": \"https://hwxsouthindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Southeast Asia\": \"https://sequenceiqsoutheastasia2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"UK South\": \"https://hwxsouthuk.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"UK West\": \"https://hwxwestuk.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West Central US\": \"https://hwxwestcentralus.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West Europe\": \"https://sequenceiqwesteurope2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West India\": \"https://hwxwestindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West US\": \"https://sequenceiqwestus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West US 2\": \"https://hwxwestus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\"\n          },\n          \"gcp\": {\n            \"default\": \"sequenceiqimage/hdc-hdp--1710161226.tar.gz\"\n          },\n          \"openstack\": {\n            \"default\": \"hdc-hdp--1710161226\"\n          }\n        },\n        \"os\": \"centos7\",\n        \"os_type\": \"redhat7\",\n        \"uuid\": \"f6e778fc-7f17-4535-9021-515351df3691\"\n      }\n    ]\n},\n  \"versions\": {\n    \"cloudbreak\": [\n      {\n        \"images\": [\n          \"44b140a4-bd0b-457d-b174-e988bee3ca47\",\n          \"f6e778fc-7f17-4535-9021-515351df3692\"\n        ],\n        \"versions\": [\n          \"2.4.0\"\n        ]\n      }\n    ]\n  }\n}\n\n\n\n\nRegister image catalog\n\n\nNow that you have created your image catalog JSON file, register it with your Cloudbreak instance. You can do this via Cloudbreak UI, CLI, or be editing the Profile file. \n\n\n\n  \nImportant\n\n  \n\n  The content type of your image catalog file should be \n\"application/json\"\n for Cloudbreak to be able to process it.\n\n\n\n\n\n\nRegister image catalog in the UI\n\n\nUse these steps to register your custom image catalog in the Cloudbreak UI.\n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, select \nExternal Sources\n \n \nImage Catalogs\n from the navigation menu.  \n\n\nClick \nCreate Image Catalog\n.  \n\n\nEnter name for your image catalog and the URL to the location where it is stored.  \n\n\nClick \nCreate\n.\n\n\n\n\nAfter performing these steps, the image catalog will be available and automatically selected as the default entry in the image catalog drop-down list in the create cluster wizard.\n\n\nRegister image catalog in the CLI\n\n\nTo register your custom image catalog using the CLI, use the \ncb imagecatalog create\n command. Refer to \nCLI documentation\n.\n\n\nRegister image catalog in the Profile\n\n\nAs an alternative to using the UI or CLI, it is possible to place the catalog file to the Cloudbreak deployer`s etc directory and then set CB_IMAGE_CATALOG_URL variable in your Profile to IMAGE_CATALOG_FILE_NAME.JSON. \n\n\nSteps\n\n\n\n\nOn the Cloudbreak machine, switch to the root user by using \nsudo su\n  \n\n\nSave the image catalog file on your Cloudbreak machine in the /var/lib/cloudbreak-deployment/etc directory.  \n\n\nEdit the Profile file located in /var/lib/cloudbreak-deployment by adding export CB_IMAGE_CATALOG_URL to the file and set it to the name of your JSON file which declares your custom images. For example: \nexport CB_IMAGE_CATALOG_URL=custom-image-catalog.json\n    \n\n\nSave the Profile file.  \n\n\nRestart Cloudbreak by using \ncbd restart\n.  \n\n\n\n\nSelect a custom image when creating a cluster\n\n\nOnce you have registered your image catalog, you can use your custom image(s) when creating a cluster.\n\n\nSelect a custom image in Cloudbreak web UI\n\n\nPerform these steps in the advanced \nGeneral Configuration\n section of the create wizard wizard.\n\n\nSteps\n  \n\n\n\n\nIn the create cluster wizard, make sure that you are using the advanced wizard version. You need to perform the steps in the \nImage Settings\n section.  \n\n\nUnder \nChoose Image Catalog\n, select your custom image catalog.  \n\n\nUnder \nImage Type\n, select \"Base Image\".\n\n\nUnder \nChoose Image\n, select the provider-specific image that you would like to use. \n\n    The \"os\" that you specified in the image catalog will be displayed in the selection and the content of the \"description\" will be displayed in green.    \n\n\n\n\nYou can leave the default entries for the Ambari and HDP/HDF repositories, or you can customize to point to specific versions of Ambari and HDP/HDF that you want to use for the cluster.  \n\n\n\n\n\n\n\n\nSelect a custom image in the CLI\n\n\nTo use the custom image when creating a cluster via CLI, perform these steps.  \n\n\nSteps\n  \n\n\n\n\n\n\nObtain the image ID. For example:\n\n\ncb imagecatalog images aws --imagecatalog custom-catalog\n[\n  {\n\"Date\": \"2017-10-13\",\n\"Description\": \"Cloudbreak official base image\",\n\"Version\": \"2.5.1.0\",\n\"ImageID\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n  },\n  {\n\"Date\": \"2017-11-16\",\n\"Description\": \"Official Cloudbreak image\",\n\"Version\": \"2.5.1.0\",\n\"ImageID\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n  }\n]\n\n\n\n\n\n\nWhen preparing a CLI JSON template for your cluster, set the \"ImageCatalog\" parameter to the image catalog that you would like to use, and set the \"ImageId\" parameter to the uuid of the image from that catalog that you would like to use. For example:\n\n\n...\n  \"name\": \"aszegedi-cli-ci\",\n  \"network\": {\n\"subnetCIDR\": \"10.0.0.0/16\"\n  },\n  \"orchestrator\": {\n\"type\": \"SALT\"\n  },\n  \"parameters\": {\n\"instanceProfileStrategy\": \"CREATE\"\n  },\n  \"region\": \"eu-west-1\",\n  \"stackAuthentication\": {\n\"publicKeyId\": \"seq-master\"\n  },\n  \"userDefinedTags\": {\n\"owner\": \"aszegedi\"\n  },\n  \"imageCatalog\": \"custom-catalog\",\n  \"imageId\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n}\n\n\n\n\n\n\nRelated links\n\n\nCLI reference", 
            "title": "Use custom images"
        }, 
        {
            "location": "/images/index.html#using-custom-images", 
            "text": "Default images are available for each supported cloud provider and region. The following table lists the default base images available:     Cloud provider  Default image      AWS  Amazon Linux 2017    Azure  CentOS 7    GCP  CentOS 7    OpenStack  CentOS 7     Since these default images may not fit the requirements of some users (for example when user requirements include custom OS hardening, custom libraries, custom tooling, and so on) Cloudbreak allows you to use your own  custom base images .  In order to use your own custom base images you must:   Build your custom images    Prepare the custom image catalog JSON file and save it in a location accessible to the Cloudbreak VM    Register your custom image catalog with Cloudbreak      Select a custom image when creating a cluster     \n     Important \n     \nOnly  base images  can be created and registered as custom images. Do not create or register  prewarmed images  as custom images.", 
            "title": "Using custom images"
        }, 
        {
            "location": "/images/index.html#build-custom-images", 
            "text": "Refer to  Custom images for Cloudbreak  for information on how to build custom images.  This repository includes instructions and scripts to help you build custom images. Once you have the images, refer to the documentation below for information on how to create an image catalog and register it with Cloudbreak.", 
            "title": "Build custom images"
        }, 
        {
            "location": "/images/index.html#prepare-the-image-catalog", 
            "text": "Once you've built the custom images, prepare your custom image catalog JSON file. Once your image catalog JSON file is ready, save it in a location accessible via HTTP/HTTPS.", 
            "title": "Prepare the image catalog"
        }, 
        {
            "location": "/images/index.html#structure-of-the-image-catalog-json-file", 
            "text": "The image catalog JSON file includes the following two high-level sections:   images : Contains information about the created images. The burned images are stored in the  base-images  section.    versions : Contains the  cloudbreak  entry, which includes mapping between Cloudbreak versions and the image identifiers of burned images available for these Cloudbreak versions.    After adding your image(s) to the  images  section, make sure to also update the  versions  section.    Images section     The burned images are stored in the  base-images  sub-section of  images . The  base-images  section stores one or more image \"records\". Every image \"record\" must contain the date, description, images, os, os_type, and uuid fields.     Parameter  Description      date  Date for your image catalog entry.    description  Description for your image catalog entry.    images  The image sets by cloud provider. An image set must store the virtual machine image IDs by the related region of the provider (AWS, Azure) or contain one default image for all regions (GCP, OpenStack). The virtual machine image IDs come from the result of the image burning process and must be an existing identifier of a virtual machine image on the related provider side. For the providers which use global rather than per-region images, the region should be replaced with  default .    os  The operating system used in the image.    os_type  The type of operating system which will be used to determine the default Ambari and HDP/HDF repositories to use. Set  os_type  to \"redhat6\" for amazonlinux or centos6 images. Set  os_type  to \"redhat7\" for centos7 or rhel7 images.    uuid  The  uuid  field must be a unique identifier within the file. You can generate it or select it manually. The utility  uuidgen  available from your command line is a convenient way to generate a unique ID.     Versions section     The  versions  section includes a single \"cloudbreak\" entry, which maps the uuids to a specific Cloudbreak version:     Parameter  Description      images  Image  uuid , same as the one that you specified in the  base-images  section.    versions  The Cloudbreak version(s) for which you would like to use the images.", 
            "title": "Structure of the image catalog JSON file"
        }, 
        {
            "location": "/images/index.html#example-image-catalog-json-file", 
            "text": "Here is an example image catalog JSON file that includes two sets of custom base images:   A custom base image for AWS:  That is using Amazon Linux operating system  That will use the Redhat 6 repos as default Ambari and HDP repositories during cluster create       Has a unique ID of \"44b140a4-bd0b-457d-b174-e988bee3ca47\"  Is available for Cloudbreak 2.4.0        A custom base image for Azure, Google, and OpenStack:  That is using CentOS 7 operating system  That will use the Redhat 7 repos as default Ambari and HDP repositories during cluster create     Has a unique ID of \"f6e778fc-7f17-4535-9021-515351df3692\"  Is available to Cloudbreak 2.4.0           You can also download it from  here .  \n{\n  \"images\": {\n    \"base-images\": [\n      {\n        \"date\": \"2017-10-13\",\n        \"description\": \"Cloudbreak official base image\",\n        \"images\": {\n          \"aws\": {\n            \"ap-northeast-1\": \"ami-78e9311e\",\n            \"ap-northeast-2\": \"ami-84b613ea\",\n            \"ap-southeast-1\": \"ami-75226716\",\n            \"ap-southeast-2\": \"ami-92ce23f0\",\n            \"eu-central-1\": \"ami-d95be5b6\",\n            \"eu-west-1\": \"ami-46429e3f\",\n            \"sa-east-1\": \"ami-86d5abea\",\n            \"us-east-1\": \"ami-51a2742b\",\n            \"us-west-1\": \"ami-21ccfe41\",\n            \"us-west-2\": \"ami-2a1cdc52\"\n          }\n        },\n        \"os\": \"amazonlinux\",\n        \"os_type\": \"redhat6\",\n        \"uuid\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n      },\n      {\n        \"date\": \"2017-10-13\",\n        \"description\": \"Cloudbreak official base image\",\n        \"images\": {\n          \"azure\": {\n            \"Australia East\": \"https://hwxaustraliaeast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Australia South East\": \"https://hwxaustralisoutheast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Brazil South\": \"https://sequenceiqbrazilsouth2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Canada Central\": \"https://sequenceiqcanadacentral.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Canada East\": \"https://sequenceiqcanadaeast.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Central India\": \"https://hwxcentralindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Central US\": \"https://sequenceiqcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East Asia\": \"https://sequenceiqeastasia2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East US\": \"https://sequenceiqeastus12.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"East US 2\": \"https://sequenceiqeastus22.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Japan East\": \"https://sequenceiqjapaneast2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Japan West\": \"https://sequenceiqjapanwest2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Korea Central\": \"https://hwxkoreacentral.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Korea South\": \"https://hwxkoreasouth.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"North Central US\": \"https://sequenceiqorthcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"North Europe\": \"https://sequenceiqnortheurope2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"South Central US\": \"https://sequenceiqouthcentralus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"South India\": \"https://hwxsouthindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"Southeast Asia\": \"https://sequenceiqsoutheastasia2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"UK South\": \"https://hwxsouthuk.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"UK West\": \"https://hwxwestuk.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West Central US\": \"https://hwxwestcentralus.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West Europe\": \"https://sequenceiqwesteurope2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West India\": \"https://hwxwestindia.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West US\": \"https://sequenceiqwestus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\",\n            \"West US 2\": \"https://hwxwestus2.blob.core.windows.net/images/hdc-hdp--1710161226.vhd\"\n          },\n          \"gcp\": {\n            \"default\": \"sequenceiqimage/hdc-hdp--1710161226.tar.gz\"\n          },\n          \"openstack\": {\n            \"default\": \"hdc-hdp--1710161226\"\n          }\n        },\n        \"os\": \"centos7\",\n        \"os_type\": \"redhat7\",\n        \"uuid\": \"f6e778fc-7f17-4535-9021-515351df3691\"\n      }\n    ]\n},\n  \"versions\": {\n    \"cloudbreak\": [\n      {\n        \"images\": [\n          \"44b140a4-bd0b-457d-b174-e988bee3ca47\",\n          \"f6e778fc-7f17-4535-9021-515351df3692\"\n        ],\n        \"versions\": [\n          \"2.4.0\"\n        ]\n      }\n    ]\n  }\n}", 
            "title": "Example image catalog JSON file"
        }, 
        {
            "location": "/images/index.html#register-image-catalog", 
            "text": "Now that you have created your image catalog JSON file, register it with your Cloudbreak instance. You can do this via Cloudbreak UI, CLI, or be editing the Profile file.   \n   Important \n   \n  The content type of your image catalog file should be  \"application/json\"  for Cloudbreak to be able to process it.", 
            "title": "Register image catalog"
        }, 
        {
            "location": "/images/index.html#register-image-catalog-in-the-ui", 
            "text": "Use these steps to register your custom image catalog in the Cloudbreak UI.  Steps   In the Cloudbreak UI, select  External Sources     Image Catalogs  from the navigation menu.    Click  Create Image Catalog .    Enter name for your image catalog and the URL to the location where it is stored.    Click  Create .   After performing these steps, the image catalog will be available and automatically selected as the default entry in the image catalog drop-down list in the create cluster wizard.", 
            "title": "Register image catalog in the UI"
        }, 
        {
            "location": "/images/index.html#register-image-catalog-in-the-cli", 
            "text": "To register your custom image catalog using the CLI, use the  cb imagecatalog create  command. Refer to  CLI documentation .", 
            "title": "Register image catalog in the CLI"
        }, 
        {
            "location": "/images/index.html#register-image-catalog-in-the-profile", 
            "text": "As an alternative to using the UI or CLI, it is possible to place the catalog file to the Cloudbreak deployer`s etc directory and then set CB_IMAGE_CATALOG_URL variable in your Profile to IMAGE_CATALOG_FILE_NAME.JSON.   Steps   On the Cloudbreak machine, switch to the root user by using  sudo su     Save the image catalog file on your Cloudbreak machine in the /var/lib/cloudbreak-deployment/etc directory.    Edit the Profile file located in /var/lib/cloudbreak-deployment by adding export CB_IMAGE_CATALOG_URL to the file and set it to the name of your JSON file which declares your custom images. For example:  export CB_IMAGE_CATALOG_URL=custom-image-catalog.json       Save the Profile file.    Restart Cloudbreak by using  cbd restart .", 
            "title": "Register image catalog in the Profile"
        }, 
        {
            "location": "/images/index.html#select-a-custom-image-when-creating-a-cluster", 
            "text": "Once you have registered your image catalog, you can use your custom image(s) when creating a cluster.", 
            "title": "Select a custom image when creating a cluster"
        }, 
        {
            "location": "/images/index.html#select-a-custom-image-in-cloudbreak-web-ui", 
            "text": "Perform these steps in the advanced  General Configuration  section of the create wizard wizard.  Steps      In the create cluster wizard, make sure that you are using the advanced wizard version. You need to perform the steps in the  Image Settings  section.    Under  Choose Image Catalog , select your custom image catalog.    Under  Image Type , select \"Base Image\".  Under  Choose Image , select the provider-specific image that you would like to use.  \n    The \"os\" that you specified in the image catalog will be displayed in the selection and the content of the \"description\" will be displayed in green.       You can leave the default entries for the Ambari and HDP/HDF repositories, or you can customize to point to specific versions of Ambari and HDP/HDF that you want to use for the cluster.", 
            "title": "Select a custom image in Cloudbreak web UI"
        }, 
        {
            "location": "/images/index.html#select-a-custom-image-in-the-cli", 
            "text": "To use the custom image when creating a cluster via CLI, perform these steps.    Steps       Obtain the image ID. For example:  cb imagecatalog images aws --imagecatalog custom-catalog\n[\n  {\n\"Date\": \"2017-10-13\",\n\"Description\": \"Cloudbreak official base image\",\n\"Version\": \"2.5.1.0\",\n\"ImageID\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n  },\n  {\n\"Date\": \"2017-11-16\",\n\"Description\": \"Official Cloudbreak image\",\n\"Version\": \"2.5.1.0\",\n\"ImageID\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n  }\n]    When preparing a CLI JSON template for your cluster, set the \"ImageCatalog\" parameter to the image catalog that you would like to use, and set the \"ImageId\" parameter to the uuid of the image from that catalog that you would like to use. For example:  ...\n  \"name\": \"aszegedi-cli-ci\",\n  \"network\": {\n\"subnetCIDR\": \"10.0.0.0/16\"\n  },\n  \"orchestrator\": {\n\"type\": \"SALT\"\n  },\n  \"parameters\": {\n\"instanceProfileStrategy\": \"CREATE\"\n  },\n  \"region\": \"eu-west-1\",\n  \"stackAuthentication\": {\n\"publicKeyId\": \"seq-master\"\n  },\n  \"userDefinedTags\": {\n\"owner\": \"aszegedi\"\n  },\n  \"imageCatalog\": \"custom-catalog\",\n  \"imageId\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n}    Related links  CLI reference", 
            "title": "Select a custom image in the CLI"
        }, 
        {
            "location": "/tags/index.html", 
            "text": "Tagging resources\n\n\nWhen you manually create resources in the cloud, you have an option to add custom tags that help you track these resources. Likewise, when creating clusters, you can instruct Cloudbreak to tag the cloud resources that it creates on your behalf. The tags added during cluster creation are displayed in your cloud account on the resources that Cloudbreak provisioned for your clusters. \n\n\nYou can use tags to categorize your cloud resources by purpose, owner, and so on. Tags come in especially handy when you are using a corporate AWS account and you want to quickly identify which resources belong to your cluster(s). In fact, your corporate cloud account admin may require you to tag all the resources that you create, in particular resources, such as VMs, which incur charges.\n\n\nAdd tags when creating a cluster\n\n\nYou can tag the cloud resources used for a cluster by providing custom tag names and values when creating a cluster via UI or CLI. In the Cloudbreak UI, this option is available in the create cluster wizard, in the advanced \nGeneral Configuration\n page \n \nTags\n section. \n\n\nIt is not possible to add tags via Cloudbreak after your cluster has been created.  \n\n\nTo learn more about tags and their restrictions, refer to the cloud provider documentation. \n\n\nRelated links\n\n\nTags on AWS\n  \n\n\nTags on Azure\n\n\nLabels on GCP\n\n\nTags on OpenStack\n  \n\n\nAdd tags in Profile (AWS)\n\n\nIn order to differentiate launched instances, you can optionally define custom tags for your AWS resources deployed by Cloudbreak. \n\n\n\n\n\n\nIf you want just one custom tag for your CloudFormation resources, set this variable in the \nProfile\n:\n\n\nexport CB_AWS_DEFAULT_CF_TAG=mytagcontent\n\n\nIn this example, the name of the tag will be \nCloudbreakId\n and the value will be \nmytagcontent\n.\n\n\n\n\n\n\nIf you prefer to customize the tag name, set this variable:\n\n\nexport CB_AWS_CUSTOM_CF_TAGS=mytagname:mytagvalue\n\n\nIn this example the name of the tag will be \nmytagname\n and the value will be \nmytagvalue\n. \n\n\n\n\n\n\nYou can specify a list of tags with a comma separated list: \n\n\nexport CB_AWS_CUSTOM_CF_TAGS=tag1:value1,tag2:value2,tag3:value3", 
            "title": "Tag resources"
        }, 
        {
            "location": "/tags/index.html#tagging-resources", 
            "text": "When you manually create resources in the cloud, you have an option to add custom tags that help you track these resources. Likewise, when creating clusters, you can instruct Cloudbreak to tag the cloud resources that it creates on your behalf. The tags added during cluster creation are displayed in your cloud account on the resources that Cloudbreak provisioned for your clusters.   You can use tags to categorize your cloud resources by purpose, owner, and so on. Tags come in especially handy when you are using a corporate AWS account and you want to quickly identify which resources belong to your cluster(s). In fact, your corporate cloud account admin may require you to tag all the resources that you create, in particular resources, such as VMs, which incur charges.", 
            "title": "Tagging resources"
        }, 
        {
            "location": "/tags/index.html#add-tags-when-creating-a-cluster", 
            "text": "You can tag the cloud resources used for a cluster by providing custom tag names and values when creating a cluster via UI or CLI. In the Cloudbreak UI, this option is available in the create cluster wizard, in the advanced  General Configuration  page    Tags  section.   It is not possible to add tags via Cloudbreak after your cluster has been created.    To learn more about tags and their restrictions, refer to the cloud provider documentation.   Related links  Tags on AWS     Tags on Azure  Labels on GCP  Tags on OpenStack", 
            "title": "Add tags when creating a cluster"
        }, 
        {
            "location": "/tags/index.html#add-tags-in-profile-aws", 
            "text": "In order to differentiate launched instances, you can optionally define custom tags for your AWS resources deployed by Cloudbreak.     If you want just one custom tag for your CloudFormation resources, set this variable in the  Profile :  export CB_AWS_DEFAULT_CF_TAG=mytagcontent  In this example, the name of the tag will be  CloudbreakId  and the value will be  mytagcontent .    If you prefer to customize the tag name, set this variable:  export CB_AWS_CUSTOM_CF_TAGS=mytagname:mytagvalue  In this example the name of the tag will be  mytagname  and the value will be  mytagvalue .     You can specify a list of tags with a comma separated list:   export CB_AWS_CUSTOM_CF_TAGS=tag1:value1,tag2:value2,tag3:value3", 
            "title": "Add tags in Profile (AWS)"
        }, 
        {
            "location": "/properties/index.html", 
            "text": "Set custom properties\n\n\nCloudbreak allows you to create dynamic blueprints with property variables and set properties on a per-cluster basis by providing property values during cluster creation. \n\n\nIn order to set custom properties for a cluster you must:\n\n\n\n\nCreate a blueprint that includes property variables for the properties that you want to set.  \n\n\nWhen creating a cluster, select the blueprint and then specify the property values under \nCluster Extensions\n \n \nCustom Properties\n in the advanced view of the cluster wizard.     \n\n\n\n\nIn the cluster creation phase, the property values in the blueprint will be replaced based on the input, picking up the parameter values that you provided.\n\n\nSteps\n\n\n\n\n\n\nPrepare a blueprint which includes a template for the properties that you would like to set. Make sure to:\n\n\n\n\nInclude these templates in the \u201cconfigurations\u201d section of your blueprint.  \n\n\nUse the mustache format. Cloudbreak supports \nmustache\n kind of templating with {{{variable}}} syntax, so your templates must be in the mustache format.   \n\n\n\n\nExample:\n    \n\nThis example provides a template for setting three properties:\n\n\n\n\nfs.trash.interval\n  \n\n\nhadoop.tmp.dir\n  \n\n\nhive.exec.compress.output\n    \n\n\n\n\n...\n{\n  \"core-site\": {\n    \"fs.trash.interval\": \"{{{ fs.trash.interval }}}\",\n    \"hadoop.tmp.dir\": \"{{{ my.tmp.directory }}}\"\n  }\n},\n{\n  \"hive-site\": {\n    \"hive.exec.compress.output\": \"{{{ hive.exec.compress.output }}}\"\n  }\n},\n...\n\n\n\n\n\n\nWhen creating a cluster:\n\n\n\n\nUnder \nGeneral Configuration \n Cluster Type\n, select the blueprint prepared in the previous step.  \n\n\n\n\nIn the advanced view of the cluster wizard, under \nCluster Extensions \n Custom Properties\n, include a JSON file which defines the property values.\n\n\nExample:\n\nThe following JSON entry sets the values for the properties from the previous step: \n\n\n{\n    \"fs.trash.interval\": \"4320\",\n    \"hive.exec.compress.output\": \"true\",\n    \"my.tmp.directory\": \"/hadoop/tmp\"\n}\n\n\n\n\n\n\n\n\n\n\nAs a result, the values of \nhive.exec.compress.output\n, \nmy.tmp.directory\n and \nfs.trash.interval\n will be replaced in the blueprint based on the input that you provided. \n\n\nExample:\n\nThe property values will be replaced for the cluster as follows based on what was defined in the previous step: \n\n\n...\n   {\n \"core-site\": {\n   \"fs.trash.interval\": \"4320\",\n   \"hadoop.tmp.dir\": \"/hadoop/tmp\"\n }\n   },\n   {\n \"hive-site\": {\n   \"hive.exec.compress.output\": \"true\"\n }\n   },", 
            "title": "Set custom properties"
        }, 
        {
            "location": "/properties/index.html#set-custom-properties", 
            "text": "Cloudbreak allows you to create dynamic blueprints with property variables and set properties on a per-cluster basis by providing property values during cluster creation.   In order to set custom properties for a cluster you must:   Create a blueprint that includes property variables for the properties that you want to set.    When creating a cluster, select the blueprint and then specify the property values under  Cluster Extensions     Custom Properties  in the advanced view of the cluster wizard.        In the cluster creation phase, the property values in the blueprint will be replaced based on the input, picking up the parameter values that you provided.  Steps    Prepare a blueprint which includes a template for the properties that you would like to set. Make sure to:   Include these templates in the \u201cconfigurations\u201d section of your blueprint.    Use the mustache format. Cloudbreak supports  mustache  kind of templating with {{{variable}}} syntax, so your templates must be in the mustache format.      Example:      \nThis example provides a template for setting three properties:   fs.trash.interval     hadoop.tmp.dir     hive.exec.compress.output        ...\n{\n  \"core-site\": {\n    \"fs.trash.interval\": \"{{{ fs.trash.interval }}}\",\n    \"hadoop.tmp.dir\": \"{{{ my.tmp.directory }}}\"\n  }\n},\n{\n  \"hive-site\": {\n    \"hive.exec.compress.output\": \"{{{ hive.exec.compress.output }}}\"\n  }\n},\n...    When creating a cluster:   Under  General Configuration   Cluster Type , select the blueprint prepared in the previous step.     In the advanced view of the cluster wizard, under  Cluster Extensions   Custom Properties , include a JSON file which defines the property values.  Example: \nThe following JSON entry sets the values for the properties from the previous step:   {\n    \"fs.trash.interval\": \"4320\",\n    \"hive.exec.compress.output\": \"true\",\n    \"my.tmp.directory\": \"/hadoop/tmp\"\n}      As a result, the values of  hive.exec.compress.output ,  my.tmp.directory  and  fs.trash.interval  will be replaced in the blueprint based on the input that you provided.   Example: \nThe property values will be replaced for the cluster as follows based on what was defined in the previous step:   ...\n   {\n \"core-site\": {\n   \"fs.trash.interval\": \"4320\",\n   \"hadoop.tmp.dir\": \"/hadoop/tmp\"\n }\n   },\n   {\n \"hive-site\": {\n   \"hive.exec.compress.output\": \"true\"\n }\n   },", 
            "title": "Set custom properties"
        }, 
        {
            "location": "/security-kerberos/index.html", 
            "text": "Enabling Kerberos security\n\n\nWhen creating a cluster, you can optionally enable Kerberos security in that cluster and provide your Kerberos configuration details. Cloudbreak will automatically extend your blueprint configuration with the defined properties.\n\n\nKerberos overview\n\n\nKerberos is a third party authentication mechanism, in which users and services that users wish to access Hadoop rely on a third party - the Kerberos server - to authenticate each to the other. The Kerberos server itself is known as the \nKey Distribution Center\n, or \nKDC\n. At a high level, the KDC has three parts:\n\n\n\n\nA database of the users and services (known as \nprincipals\n) and their respective Kerberos passwords  \n\n\nAn \nAuthentication Server (AS)\n which performs the initial authentication and issues a Ticket Granting Ticket (TGT)  \n\n\nA \nTicket Granting Server (TGS)\n that issues subsequent service tickets based on the initial TGT  \n\n\n\n\nA user principal requests authentication from the AS. The AS returns a TGT that is encrypted using the user principal's Kerberos password, which is known only to the user principal and the AS. The user principal decrypts the TGT locally using its Kerberos password, and from that point forward, until the ticket expires, the user principal can use the TGT to get service tickets from the TGS. Service tickets are what allow the principal to access various services. \n\n\nSince cluster resources (hosts or services) cannot provide a password each time to decrypt the TGT, they use a special file, called a \nkeytab\n, which contains the resource principal authentication credentials. The set of hosts, users, and services over which the Kerberos server has control is called a \nrealm\n.\n\n\nThe following table explains the Kerberos related terminology:\n\n\n\n\n\n\n\n\nTerm\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKey Distribution Center, or KDC\n\n\nThe trusted source for authentication in a Kerberos-enabled environment.\n\n\n\n\n\n\nKerberos KDC Server\n\n\nThe machine, or server, that serves as the Key Distribution Center (KDC).\n\n\n\n\n\n\nKerberos Client\n\n\nAny machine in the cluster that authenticates against the KDC.\n\n\n\n\n\n\nPrincipal\n\n\nThe unique name of a user or service that authenticates against the KDC.\n\n\n\n\n\n\nKeytab\n\n\nA file that includes one or more principals and their keys.\n\n\n\n\n\n\nRealm\n\n\nThe Kerberos network that includes a KDC and a number of clients.\n\n\n\n\n\n\n\n\nEnabling Kerberos\n\n\nThe option to enable Kerberos is available in the advanced \nSecurity\n section of the create cluster wizard.  \n\n\nYou have the following options for enabling Kerberos in a Cloudbreak  managed cluster:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\nEnvironment\n\n\n\n\n\n\n\n\n\n\nUse existing KDC\n\n\nAllows you to leverage an existing MIT KDC or Active Directory for enabling Kerberos with the cluster.\nYou can either provide the required parameters and Cloudbreak will generate the descriptors on your behalf, or provide the exact Ambari Kerberos descriptors to be injected into your blueprint in JSON format.\n\n\nSuitable for production\n\n\n\n\n\n\nUse test KDC\n\n\nInstalls a new MIT KDC on the master node and configures the cluster to leverage that KDC.\n\n\nSuitable for evaluation and testing only, not suitable for production\n\n\n\n\n\n\n\n\nUsing existing KDC\n\n\nTo use an existing KDC, in the advanced \nSecurity\n section of the create cluster wizard select \nEnable Kerberos Security\n. By default, \nUse Existing KDC\n option is selected.  \n\n\nYou must provide the following information about your MIT KDC or Active Directory. Based on these parameters, kerberos-env and krb5-conf JSON descriptors for Ambari are generated and injected into your Blueprint:\n\n\n\n\nBefore proceeding with the configuration, you must confirm that you met the requirements by checking the boxes next to all requirements listed. The configuration options are displayed only after you have confirmed all the requirements by checking every box.    \n\n\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKerberos Admin Principal\n\n\nThe admin principal in your existing MIT KDC or AD.\n\n\n\n\n\n\nKerberos Admin Password\n\n\nThe admin principal password in your existing MIT KDC or AD.\n\n\n\n\n\n\nMIT KDC or Active Directory\n\n\nSelect MIT KDC or Active Directory.\n\n\n\n\n\n\n\n\nUse basic configuration\n\n\n\n\n\n\n\n\nParameter\n\n\nRequired if using...\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKerberos Url\n\n\nMIT, AD\n\n\nIP address or FQDN for the KDC host. Optionally a port number may be included. Example: \"kdc.example1.com:88\" or \"kdc.example1.com\"\n\n\n\n\n\n\nKerberos Admin URL\n\n\nMIT, AD\n\n\n(Optional) IP address or FQDN for the KDC admin host. Optionally a port number may be included. Example: \"kdc.example2.com:88\" or \"kdc.example2.com\"\n\n\n\n\n\n\nKerberos Realm\n\n\nMIT, AD\n\n\nThe default realm to use when creating service principals. Example: \"EXAMPLE.COM\"\n\n\n\n\n\n\nKerberos AD Ldap Url\n\n\nAD\n\n\nThe URL to the Active Directory LDAP Interface. This value must indicate a secure channel using LDAPS since it is required for creating and updating passwords for Active Directory accounts. Example: \"ldaps://ad.example.com:636\"\n\n\n\n\n\n\nKerberos AD Container DN\n\n\nAD\n\n\nThe distinguished name (DN) of the container used store service principals. Example:  \"OU=hadoop,DC=example,DC=com\"\n\n\n\n\n\n\nUse TCP Connection\n\n\nOptional\n\n\nBy default, Kerberos uses UDP. Checkmark this box to use TCP instead.\n\n\n\n\n\n\n\n\nUse advanced configuration\n \n\n\nChecking the \nUse Custom Configuration\n option allows you to provide the actual Ambari Kerberos descriptors to be injected into your blueprint (instead of Cloudbreak generating the descriptors on your behalf). This is the most powerful option which gives you full control of the Ambari Kerberos options that are available. You must provide: \n\n\n\n\nKerberos-env JSON Descriptor (required)\n\n\nkrb5-conf JSON Descriptor (optional)\n\n\n\n\nTo learn more about the Ambari Kerberos JSON descriptors, refer to \nApache cwiki\n.  \n\n\nUsing test KDC\n\n\nTo use a test KDC, in the advanced \nSecurity\n section of the create cluster wizard select \nEnable Kerberos Security\n and then select \nUse Test KDC\n.\n\n\n\n\nImportant\n\n\n\nUsing the Test KDC is for evaluation and testing purposes only, and cannot be used for production clusters. To enable Kerberos for production use, you must use the \nUse Existing KDC\n option. \n\n\n\n\n\nYou must provide the following parameters for your new test KDC:  \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKerberos Master Key\n\n\nThe master key for the KDC database.\n\n\n\n\n\n\nKerberos Admin Username\n\n\nThe admin principal to create that can administer the KDC.\n\n\n\n\n\n\nKerberos Admin Password\n\n\nThe admin principal password.\n\n\n\n\n\n\nConfirm Kerberos Admin Password\n\n\nThe admin principal password.\n\n\n\n\n\n\n\n\nWhen using the test KDC option:\n\n\n\n\nCloudbreak installs an MIT KDC instance on the Ambari server node.  \n\n\nKerberos clients are installed on all cluster nodes, and the krb5.conf is configured to use the MIT KDC.  \n\n\nThe cluster is configured for Kerberos to use the MIT KDC. Very basic Ambari KSON Kerberos descriptors are generated and used accordingly.\n\n\n\n\nExample kerberos-env JSON descriptor file:\n\n\n{\n      \"kerberos-env\" : {\n        \"properties\" : {\n          \"kdc_type\" : \"mit-kdc\",\n          \"kdc_hosts\" : \"ip-10-0-121-81.ec2.internal\",\n          \"realm\" : \"EC2.INTERNAL\",\n          \"encryption_types\" : \"aes des3-cbc-sha1 rc4 des-cbc-md5\",\n          \"ldap_url\" : \"\",\n          \"admin_server_host\" : \"ip-10-0-121-81.ec2.internal\",\n          \"container_dn\" : \"\"\n        }\n      }\n    }\n\n\n\nExample krb5-conf JSON  descriptor file: \n\n\n {\n      \"krb5-conf\" : {\n        \"properties\" : {\n          \"domains\" : \".ec2.internal\",\n          \"manage_krb5_conf\" : \"true\"\n        }\n      }\n    }\n\n\n\nRelated links\n \n\n\nApache cwiki", 
            "title": "Enable Kerberos security"
        }, 
        {
            "location": "/security-kerberos/index.html#enabling-kerberos-security", 
            "text": "When creating a cluster, you can optionally enable Kerberos security in that cluster and provide your Kerberos configuration details. Cloudbreak will automatically extend your blueprint configuration with the defined properties.", 
            "title": "Enabling Kerberos security"
        }, 
        {
            "location": "/security-kerberos/index.html#kerberos-overview", 
            "text": "Kerberos is a third party authentication mechanism, in which users and services that users wish to access Hadoop rely on a third party - the Kerberos server - to authenticate each to the other. The Kerberos server itself is known as the  Key Distribution Center , or  KDC . At a high level, the KDC has three parts:   A database of the users and services (known as  principals ) and their respective Kerberos passwords    An  Authentication Server (AS)  which performs the initial authentication and issues a Ticket Granting Ticket (TGT)    A  Ticket Granting Server (TGS)  that issues subsequent service tickets based on the initial TGT     A user principal requests authentication from the AS. The AS returns a TGT that is encrypted using the user principal's Kerberos password, which is known only to the user principal and the AS. The user principal decrypts the TGT locally using its Kerberos password, and from that point forward, until the ticket expires, the user principal can use the TGT to get service tickets from the TGS. Service tickets are what allow the principal to access various services.   Since cluster resources (hosts or services) cannot provide a password each time to decrypt the TGT, they use a special file, called a  keytab , which contains the resource principal authentication credentials. The set of hosts, users, and services over which the Kerberos server has control is called a  realm .  The following table explains the Kerberos related terminology:     Term  Description      Key Distribution Center, or KDC  The trusted source for authentication in a Kerberos-enabled environment.    Kerberos KDC Server  The machine, or server, that serves as the Key Distribution Center (KDC).    Kerberos Client  Any machine in the cluster that authenticates against the KDC.    Principal  The unique name of a user or service that authenticates against the KDC.    Keytab  A file that includes one or more principals and their keys.    Realm  The Kerberos network that includes a KDC and a number of clients.", 
            "title": "Kerberos overview"
        }, 
        {
            "location": "/security-kerberos/index.html#enabling-kerberos", 
            "text": "The option to enable Kerberos is available in the advanced  Security  section of the create cluster wizard.    You have the following options for enabling Kerberos in a Cloudbreak  managed cluster:     Option  Description  Environment      Use existing KDC  Allows you to leverage an existing MIT KDC or Active Directory for enabling Kerberos with the cluster. You can either provide the required parameters and Cloudbreak will generate the descriptors on your behalf, or provide the exact Ambari Kerberos descriptors to be injected into your blueprint in JSON format.  Suitable for production    Use test KDC  Installs a new MIT KDC on the master node and configures the cluster to leverage that KDC.  Suitable for evaluation and testing only, not suitable for production", 
            "title": "Enabling Kerberos"
        }, 
        {
            "location": "/security-kerberos/index.html#using-existing-kdc", 
            "text": "To use an existing KDC, in the advanced  Security  section of the create cluster wizard select  Enable Kerberos Security . By default,  Use Existing KDC  option is selected.    You must provide the following information about your MIT KDC or Active Directory. Based on these parameters, kerberos-env and krb5-conf JSON descriptors for Ambari are generated and injected into your Blueprint:   Before proceeding with the configuration, you must confirm that you met the requirements by checking the boxes next to all requirements listed. The configuration options are displayed only after you have confirmed all the requirements by checking every box.          Parameter  Description      Kerberos Admin Principal  The admin principal in your existing MIT KDC or AD.    Kerberos Admin Password  The admin principal password in your existing MIT KDC or AD.    MIT KDC or Active Directory  Select MIT KDC or Active Directory.     Use basic configuration     Parameter  Required if using...  Description      Kerberos Url  MIT, AD  IP address or FQDN for the KDC host. Optionally a port number may be included. Example: \"kdc.example1.com:88\" or \"kdc.example1.com\"    Kerberos Admin URL  MIT, AD  (Optional) IP address or FQDN for the KDC admin host. Optionally a port number may be included. Example: \"kdc.example2.com:88\" or \"kdc.example2.com\"    Kerberos Realm  MIT, AD  The default realm to use when creating service principals. Example: \"EXAMPLE.COM\"    Kerberos AD Ldap Url  AD  The URL to the Active Directory LDAP Interface. This value must indicate a secure channel using LDAPS since it is required for creating and updating passwords for Active Directory accounts. Example: \"ldaps://ad.example.com:636\"    Kerberos AD Container DN  AD  The distinguished name (DN) of the container used store service principals. Example:  \"OU=hadoop,DC=example,DC=com\"    Use TCP Connection  Optional  By default, Kerberos uses UDP. Checkmark this box to use TCP instead.     Use advanced configuration    Checking the  Use Custom Configuration  option allows you to provide the actual Ambari Kerberos descriptors to be injected into your blueprint (instead of Cloudbreak generating the descriptors on your behalf). This is the most powerful option which gives you full control of the Ambari Kerberos options that are available. You must provide:    Kerberos-env JSON Descriptor (required)  krb5-conf JSON Descriptor (optional)   To learn more about the Ambari Kerberos JSON descriptors, refer to  Apache cwiki .", 
            "title": "Using existing KDC"
        }, 
        {
            "location": "/security-kerberos/index.html#using-test-kdc", 
            "text": "To use a test KDC, in the advanced  Security  section of the create cluster wizard select  Enable Kerberos Security  and then select  Use Test KDC .   Important  \nUsing the Test KDC is for evaluation and testing purposes only, and cannot be used for production clusters. To enable Kerberos for production use, you must use the  Use Existing KDC  option.    You must provide the following parameters for your new test KDC:       Parameter  Description      Kerberos Master Key  The master key for the KDC database.    Kerberos Admin Username  The admin principal to create that can administer the KDC.    Kerberos Admin Password  The admin principal password.    Confirm Kerberos Admin Password  The admin principal password.     When using the test KDC option:   Cloudbreak installs an MIT KDC instance on the Ambari server node.    Kerberos clients are installed on all cluster nodes, and the krb5.conf is configured to use the MIT KDC.    The cluster is configured for Kerberos to use the MIT KDC. Very basic Ambari KSON Kerberos descriptors are generated and used accordingly.   Example kerberos-env JSON descriptor file:  {\n      \"kerberos-env\" : {\n        \"properties\" : {\n          \"kdc_type\" : \"mit-kdc\",\n          \"kdc_hosts\" : \"ip-10-0-121-81.ec2.internal\",\n          \"realm\" : \"EC2.INTERNAL\",\n          \"encryption_types\" : \"aes des3-cbc-sha1 rc4 des-cbc-md5\",\n          \"ldap_url\" : \"\",\n          \"admin_server_host\" : \"ip-10-0-121-81.ec2.internal\",\n          \"container_dn\" : \"\"\n        }\n      }\n    }  Example krb5-conf JSON  descriptor file:    {\n      \"krb5-conf\" : {\n        \"properties\" : {\n          \"domains\" : \".ec2.internal\",\n          \"manage_krb5_conf\" : \"true\"\n        }\n      }\n    }  Related links    Apache cwiki", 
            "title": "Using test KDC"
        }, 
        {
            "location": "/autoscaling/index.html", 
            "text": "Configuring autoscaling\n\n\nAutoscaling allows you to adjust cluster capacity based on Ambari metrics and alerts, as well as schedule time-based capacity adjustment. When creating an autoscaling policy, you define:\n\n\n\n\nAn \nalert\n that triggers a scaling policy. An alert can be based on an Ambari metric or can be time-based.     \n\n\nA \nscaling policy\n that adds or removes a set number of nodes to a selected host group when the conditions defined in the attached alert are met.    \n\n\n\n\nMetric-based autoscaling\n\n\nCloudbreak accesses all available Ambari metrics and allows you to define alerts based on these metrics. For example:\n\n\n\n\n\n\n\n\nAlert Definition\n\n\nPolicy Definition\n\n\n\n\n\n\n\n\n\n\nResourceManager CPU\n alert with \nCRITICAL\n status for 5 minutes\n\n\nAdd 10 worker nodes\n\n\n\n\n\n\nHDFS Capacity Utilization\n alert with \nWARN\n status for 20 minutes\n\n\nSet the number of worker nodes to 50\n\n\n\n\n\n\nAmbari Server Alerts\n alert with \nCRITICAL\n status for 15 minutes\n\n\nDecrease the number of worker nodes by 80%\n\n\n\n\n\n\n\n\nTime-based autoscaling\n\n\nTime-based alerts can be defined by providing a cron expression. For example: \n\n\n\n\n\n\n\n\nAlert Definition\n\n\nPolicy Definition\n\n\n\n\n\n\n\n\n\n\nEvery day at 07:00 AM (GMT-8)\n\n\nAdd 90 worker nodes\n\n\n\n\n\n\nEvery day at 08:00 PM (GMT-8)\n\n\nRemove 90 worker nodes\n\n\n\n\n\n\n\n\n\n\nCluster resizing is not supported for HDF clusters. \n\n\n\n\nEnable autoscaling\n\n\nFor each newly created cluster, autoscaling is disabled by default but it can be enabled once the cluster is in a running state. \n\n\n\n\nAutoscaling configuration is only available in the UI. It is currently not available in the CLI. \n\n\n\n\nSteps\n\n\n\n\nOn the cluster details page, navigate to the \nAutoscaling\n tab.   \n\n\n\n\nClick the toggle button to enable autoscaling:\n\n\n  \n\n\n\n\n\n\nThe toggle button turns green and you can see that \"Autoscaling is enabled\".   \n\n\n\n\nDefine alerts\n and then \ndefine scaling policies\n. You can also \nadjust the autoscaling settings\n. \n\n\n\n\nIf you decide to disable autoscaling, your previously defined alerts and policies will be preserved. \n\n\nDefining an alert\n\n\nAfter you have enabled autoscaling, define a metric-based or time-based alert.  \n\n\nDefine a metric-based alert\n\n\nAfter \nenabling autoscaling\n, perform the following steps to create a metric-based alert.  \n\n\n\n\nIf you would like to change default thresholds for an Ambari metric, refer to \nModifying alerts\n in Ambari documentation. \n\n\nIf you would like to create a custom Ambari alert, refer to \nHow to create a custom Ambari alert and use it for Cloudbreak autoscaling policies\n.\n\n\n\n\nSteps\n\n\n\n\nIn the \nAlert Configuration\n section, select \nMetric Based\n alert type.      \n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nEnter alert name\n\n\nEnter a unique name for the alert.\n\n\n\n\n\n\nChoose metric type\n\n\nSelect the Ambari metric that should trigger the alert.\n\n\n\n\n\n\nAlert status\n\n\nSelect the alert status that should trigger an alert for the selected metric. One of: OK, CRITICAL, WARNING.\n\n\n\n\n\n\nAlert duration\n\n\nSelect the alert duration that should trigger an alert.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \n+\n to save the alert.  \n\n\n\n\n\n\nOnce you have defined an alert, \ncreate a scaling policy\n that this metric should trigger.\n\n\nRelated links:\n\n\nHow to create a custom Ambari alert and use it for Cloudbreak autoscaling policies\n (HCC) \n\n\nModifying alerts\n (Hortonworks)   \n\n\nDefine a time-based alert\n\n\nAfter \nenabling autoscaling\n, perform the following steps to create a time-based alert.\n\n\nSteps\n\n\n\n\nIn the \nAlert Configuration\n section, select the \nTime Based\n alert type. \n\n\n\n\nProvide the following information: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nEnter alert name.\n\n\nEnter a unique name for the alert.\n\n\n\n\n\n\nSelect timezone.\n\n\nSelect your timezone.\n\n\n\n\n\n\nEnter cron expression\n\n\nEnter a cron expression that defines the frequency of the alert. Refer to \nCron expression generator\n.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \n+\n to save the alert.   \n\n\n\n\n\n\nOnce you have defined an alert, \ncreate a scaling policy\n that this metric should trigger.\n\n\nCreate a scaling policy\n\n\nAfter \nenabling autoscaling\n and \ncreating at least one alert\n, perform the following steps to create a scaling policy.\n\n\nSteps\n\n\n\n\n\n\nIn the \nPolicy Configuration\n section, provide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nEnter policy name\n\n\nEnter a unique name for the policy.\n\n\n\n\n\n\nSelect action\n\n\nSelect one of the following actions: Add (to add nodes to a host group) Remove (to delete nodes from a host group), or Set (to set the number of nodes in a host group to the chosen number).\n\n\n\n\n\n\nEnter number or percentage\n\n\nEnter a number defining how many or what percentage of nodes to add or remove. If the action selected is \"set\", this defines the number of nodes that a host group will be set to after scaling.\n\n\n\n\n\n\nSelect nodes of percent\n\n\nSelect \"nodes\" or \"percent\", depending on whether you want to scale to a specific number, or percent of current number of nodes.\n\n\n\n\n\n\nSelect host group\n\n\nSelect the host group to which to apply the scaling.\n\n\n\n\n\n\nChoose an alert\n\n\nSelect the alert based on which the scaling should be applied.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \n+\n to save the alert.   \n\n\n\n\n\n\nConfigure autoscaling settings\n\n\nAfter \nenabling autoscaling\n, perform these steps to configure the auto scaling settings for your cluster.   \n\n\nSteps\n\n\n\n\n\n\nIn the \nCluster Scaling Configuration\n, provide the following information: \n\n\n\n\n\n\n\n\nSetting\n\n\nDescription\n\n\nDefault Value\n\n\n\n\n\n\n\n\n\n\nCooldown time\n\n\nAfter an auto scaling event occurs, the amount of time to wait before enforcing another scaling policy. This means that the scaling events scheduled during cooldown time are dropped.\n\n\n30 minutes\n\n\n\n\n\n\nMinimum Cluster Size\n\n\nThe minimum size allowed for the cluster. Auto scaling policies cannot scale the cluster below or above this size.\n\n\n2 nodes\n\n\n\n\n\n\nMaximum Cluster Size\n\n\nThe maximum size allowed for the cluster. Auto scaling policies cannot scale the cluster below or above this size.\n\n\n100 nodes\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nSave\n to save the changes.", 
            "title": "Configure autoscaling"
        }, 
        {
            "location": "/autoscaling/index.html#configuring-autoscaling", 
            "text": "Autoscaling allows you to adjust cluster capacity based on Ambari metrics and alerts, as well as schedule time-based capacity adjustment. When creating an autoscaling policy, you define:   An  alert  that triggers a scaling policy. An alert can be based on an Ambari metric or can be time-based.       A  scaling policy  that adds or removes a set number of nodes to a selected host group when the conditions defined in the attached alert are met.       Metric-based autoscaling  Cloudbreak accesses all available Ambari metrics and allows you to define alerts based on these metrics. For example:     Alert Definition  Policy Definition      ResourceManager CPU  alert with  CRITICAL  status for 5 minutes  Add 10 worker nodes    HDFS Capacity Utilization  alert with  WARN  status for 20 minutes  Set the number of worker nodes to 50    Ambari Server Alerts  alert with  CRITICAL  status for 15 minutes  Decrease the number of worker nodes by 80%     Time-based autoscaling  Time-based alerts can be defined by providing a cron expression. For example:      Alert Definition  Policy Definition      Every day at 07:00 AM (GMT-8)  Add 90 worker nodes    Every day at 08:00 PM (GMT-8)  Remove 90 worker nodes      Cluster resizing is not supported for HDF clusters.", 
            "title": "Configuring autoscaling"
        }, 
        {
            "location": "/autoscaling/index.html#enable-autoscaling", 
            "text": "For each newly created cluster, autoscaling is disabled by default but it can be enabled once the cluster is in a running state.    Autoscaling configuration is only available in the UI. It is currently not available in the CLI.    Steps   On the cluster details page, navigate to the  Autoscaling  tab.      Click the toggle button to enable autoscaling:        The toggle button turns green and you can see that \"Autoscaling is enabled\".      Define alerts  and then  define scaling policies . You can also  adjust the autoscaling settings .    If you decide to disable autoscaling, your previously defined alerts and policies will be preserved.", 
            "title": "Enable autoscaling"
        }, 
        {
            "location": "/autoscaling/index.html#defining-an-alert", 
            "text": "After you have enabled autoscaling, define a metric-based or time-based alert.", 
            "title": "Defining an alert"
        }, 
        {
            "location": "/autoscaling/index.html#define-a-metric-based-alert", 
            "text": "After  enabling autoscaling , perform the following steps to create a metric-based alert.     If you would like to change default thresholds for an Ambari metric, refer to  Modifying alerts  in Ambari documentation.   If you would like to create a custom Ambari alert, refer to  How to create a custom Ambari alert and use it for Cloudbreak autoscaling policies .   Steps   In the  Alert Configuration  section, select  Metric Based  alert type.         Provide the following information:     Parameter  Description      Enter alert name  Enter a unique name for the alert.    Choose metric type  Select the Ambari metric that should trigger the alert.    Alert status  Select the alert status that should trigger an alert for the selected metric. One of: OK, CRITICAL, WARNING.    Alert duration  Select the alert duration that should trigger an alert.       Click  +  to save the alert.      Once you have defined an alert,  create a scaling policy  that this metric should trigger.  Related links:  How to create a custom Ambari alert and use it for Cloudbreak autoscaling policies  (HCC)   Modifying alerts  (Hortonworks)", 
            "title": "Define a metric-based alert"
        }, 
        {
            "location": "/autoscaling/index.html#define-a-time-based-alert", 
            "text": "After  enabling autoscaling , perform the following steps to create a time-based alert.  Steps   In the  Alert Configuration  section, select the  Time Based  alert type.    Provide the following information:      Parameter  Description      Enter alert name.  Enter a unique name for the alert.    Select timezone.  Select your timezone.    Enter cron expression  Enter a cron expression that defines the frequency of the alert. Refer to  Cron expression generator .       Click  +  to save the alert.       Once you have defined an alert,  create a scaling policy  that this metric should trigger.", 
            "title": "Define a time-based alert"
        }, 
        {
            "location": "/autoscaling/index.html#create-a-scaling-policy", 
            "text": "After  enabling autoscaling  and  creating at least one alert , perform the following steps to create a scaling policy.  Steps    In the  Policy Configuration  section, provide the following information:     Parameter  Description      Enter policy name  Enter a unique name for the policy.    Select action  Select one of the following actions: Add (to add nodes to a host group) Remove (to delete nodes from a host group), or Set (to set the number of nodes in a host group to the chosen number).    Enter number or percentage  Enter a number defining how many or what percentage of nodes to add or remove. If the action selected is \"set\", this defines the number of nodes that a host group will be set to after scaling.    Select nodes of percent  Select \"nodes\" or \"percent\", depending on whether you want to scale to a specific number, or percent of current number of nodes.    Select host group  Select the host group to which to apply the scaling.    Choose an alert  Select the alert based on which the scaling should be applied.       Click  +  to save the alert.", 
            "title": "Create a scaling policy"
        }, 
        {
            "location": "/autoscaling/index.html#configure-autoscaling-settings", 
            "text": "After  enabling autoscaling , perform these steps to configure the auto scaling settings for your cluster.     Steps    In the  Cluster Scaling Configuration , provide the following information:      Setting  Description  Default Value      Cooldown time  After an auto scaling event occurs, the amount of time to wait before enforcing another scaling policy. This means that the scaling events scheduled during cooldown time are dropped.  30 minutes    Minimum Cluster Size  The minimum size allowed for the cluster. Auto scaling policies cannot scale the cluster below or above this size.  2 nodes    Maximum Cluster Size  The maximum size allowed for the cluster. Auto scaling policies cannot scale the cluster below or above this size.  100 nodes       Click  Save  to save the changes.", 
            "title": "Configure autoscaling settings"
        }, 
        {
            "location": "/external-db/index.html", 
            "text": "Using an external database for cluster services\n\n\nCloudbreak allows you to register an existing RDBMS instance as an \nexternal source\n to be used for a database for certain services. After you register the RDBMS with Cloudbreak, you can use it for multiple clusters. \n\n\nSupported databases\n\n\nIf you would like to use an external database for one of the components that support it, you may use the following database types and versions: \n\n\n\n\n\n\n\n\nComponent\n\n\nSupported databases\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmbari\n\n\nPostgreSQL, MySQL\n\n\nBy default, Ambari uses an embedded PostgreSQL instance.\n\n\n\n\n\n\n\n\n\n\nDruid\n\n\nPostgreSQL, MySQL\n\n\nYou must provide an external database.\n\n\n\n\n\n\n\n\n\n\nHive\n\n\nPostgreSQL, MySQL, Oracle 11, Oracle 12\n\n\nBy default, Cloudbreak installs a PostgreSQL instance on the Hive Metastore host.\n\n\n\n\n\n\n\n\n\n\nOozie\n\n\nPostgreSQL, MySQL, Oracle 11, Oracle 12\n\n\nYou must provide an external database.\n\n\n\n\n\n\n\n\n\n\nRanger\n\n\nPostgreSQL, MySQL, Oracle 11, Oracle 12\n\n\nYou must provide an external database.\n\n\n\n\n\n\n\n\n\n\nSuperset\n\n\nPostgreSQL, MySQL\n\n\nYou must provide an external database.\n\n\n\n\n\n\n\n\n\n\nOther\n\n\nPostgreSQL, MySQL, Oracle 11, Oracle 12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExternal database options\n\n\nCloudbreak includes the following external database options:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\nBlueprint requirements\n\n\nSteps\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nBuilt-in types\n\n\nCloudbreak includes a few built-in types: Hive, Druid, Ranger, Superset, and Oozie.\n\n\nUse a standard blueprint which does not include any JDBC  parameters. Cloudbreak automatically injects the JDBC property variables into the blueprint.\n\n\nSimply \nregister the database in the UI\n. After that, you can attach the database config to your clusters.\n\n\nRefer to \nExample 1\n\n\n\n\n\n\nOther types\n\n\nIn addition to the built-in types, Cloudbreak allows you to specify custom types. In the UI, this corresponds to the UI option is called \"Other\" \n \"Enter the type\".\n\n\nYou must provide a custom dynamic blueprint which includes RDBMS-specific variables. Refer to \nCreating a template blueprint\n.\n\n\nPrepare your custom blueprint first. Next, \nregister the database in the UI\n. After that, you can attach the database config to your clusters.\n\n\nRefer to \nExample 2\n\n\n\n\n\n\n\n\nDuring cluster create, Cloudbreak checks whether the JDBC properties are present in the blueprint:\n\n\n\n\nExample 1: Built-in type Hive\n\n\nIn this scenario, you start up with a standard blueprint, and Cloudbreak injects the JDBC properties into the blueprint.\n\n\n\n\n\n\nRegister an existing external database of \"Hive\" type (built-in type):\n\n\n\n\n\n\n\n\n\n\nProperty variable\n\n\nExample value\n\n\n\n\n\n\n\n\n\n\nrds.hive.connectionString\n\n\njdbc:postgresql://ec2-54-159-202-231.compute-1.amazonaws.com:5432/hive\n\n\n\n\n\n\nrds.hive.connectionDriver\n\n\norg.postgresql.Driver\n\n\n\n\n\n\nrds.hive.connectionUserName\n\n\nmyuser\n\n\n\n\n\n\nrds.hive.connectionPassword\n\n\nHadoop123!\n\n\n\n\n\n\nrds.hive.fancyName\n\n\nPostgreSQL\n\n\n\n\n\n\nrds.hive.databaseType\n\n\npostgres\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a cluster by using a standard blueprint (i.e. one without JDBC related variables) and by attaching the external Hive database configuration.  \n\n\n\n\n\n\nUpon cluster create, Hive JDBC properties will be injected into the blueprint according to the following template:\n\n\n...\n\"hive-site\": {\n\"properties\": {\n  \"javax.jdo.option.ConnectionURL\": \"{{{ rds.hive.connectionString }}}\",\n  \"javax.jdo.option.ConnectionDriverName\": \"{{{ rds.hive.connectionDriver }}}\",\n  \"javax.jdo.option.ConnectionUserName\": \"{{{ rds.hive.connectionUserName }}}\",\n  \"javax.jdo.option.ConnectionPassword\": \"{{{ rds.hive.connectionPassword }}}\"\n}\n  },\n  \"hive-env\" : {\n\"properties\" : {\n  \"hive_database\" : \"Existing {{{ rds.hive.fancyName}}} Database\",\n  \"hive_database_type\" : \"{{{ rds.hive.databaseType }}}\"\n}\n}\n...\n \n\n\n\n\n\n\nExample 2: Other type\n\n\nIn this scenario, you start up with a special blueprint including JDBC property variables, and Cloudbreak replaces JDBC-related property variables in the blueprint. \n\n\n\n\n\n\nPrepare a blueprint blueprint that includes property variables. Use \nmustache template\n syntax. For example:\n\n\n...\n\"test-site\": {\n  \"properties\": {\n   \"javax.jdo.option.ConnectionURL\":\"{{rds.test.connectionString}}\"\n  }\n...\n\n\n\n\n\n\nRegister an existing external database of some \"Other\" type. For example:\n\n\n\n\n\n\n\n\n\n\nProperty variable\n\n\nExample value\n\n\n\n\n\n\n\n\n\n\nrds.hive.connectionString\n\n\nec2-54-159-202-231.compute-1.amazonaws.com:5432/hive\n\n\n\n\n\n\nrds.hive.connectionDriver\n\n\norg.postgresql.Driver\n\n\n\n\n\n\nrds.hive.connectionUserName\n\n\nmyuser\n\n\n\n\n\n\nrds.hive.connectionPassword\n\n\nHadoop123!\n\n\n\n\n\n\nrds.hive.subprotocol\n\n\npostgres\n\n\n\n\n\n\nrds.hive.databaseEngine\n\n\nPOSTGRES\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a cluster by using your custom blueprint and by attaching the external database configuration.  \n\n\n\n\n\n\nUpon cluster create, Cloudbreak replaces JDBC-related property variables in the blueprint. \n\n\n\n\n\n\nRelated links\n\n\nMustache template syntax\n  \n\n\nCreating a template blueprint for RDMBS\n\n\nIn order to use an external RDBMS for some component other than the built-in components, you must include JDBC property variables in your blueprint. You must use \nmustache template\n syntax. See \nExample 2: Other type\n for an example configuration. \n\n\nRelated links\n\n\nCreating a template blueprint\n\n\nMustache template syntax\n (External)  \n\n\nRegister an external database\n\n\nYou must create the external RDBMS instance and database prior to registering it with Cloudbreak. Once you have it ready, you can:\n\n\n\n\nRegister it in Cloudbreak web UI or CLI.  \n\n\nUse it with one or more clusters. Once registered, the database will now show up in the list of available databases when creating a cluster under advanced \nExternal Sources\n \n \nConfigure Databases\n.  \n\n\n\n\nPrerequisites\n  \n\n\nIf you are planning to use an external MySQL or Oracle database, you must download the JDBC connector's JAR file and place it in a location available to the cluster host on which Ambari is installed. The steps below require that you provide the URL to the JDBC connector's JAR file. \n\n\n\n\nIf you are using your own \ncustom image\n, you may place the JDBC connector's JAR file directly on the machine as part of the image burning process. \n\n\n\n\nSteps\n \n\n\n\n\nFrom the navigation pane, select \nExternal Sources\n \n \nDatabase Configurations\n.  \n\n\nSelect \nRegister Database Configuration\n.    \n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter the name to use when registering this database to Cloudbreak. This is \nnot\n the database name.\n\n\n\n\n\n\nType\n\n\nSelect the service for which you would like to use the external database. If you selected \"Other\", you must provide a special blueprint.\n\n\n\n\n\n\nJDBC Connection\n\n\nSelect the database \ntype\n and enter the \nJDBC connection\n string (HOST:PORT/DB_NAME).\n\n\n\n\n\n\nConnector's JAR URL\n\n\n(MySQL and Oracle only) Provide a URL to the JDBC connector's JAR file. The JAR file must be hosted in a location accessible to the cluster host on which Ambari is installed. At cluster creation time, Cloudbreak places the JAR file in the /opts/jdbc-drivers directory. You do not need to provide the \"Connector's JAR URL if you are using a custom image and the JAR file was either manually placed on the VM as part of custom image burning or it was placed there by using a recipe.\n\n\n\n\n\n\nUsername\n\n\nEnter the JDBC connection username.\n\n\n\n\n\n\nPassword\n\n\nEnter the JDBC connection password.\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nTest Connection\n to validate and test the RDS connection information.  \n\n\n\n\nOnce your settings are validated and working, click \nREGISTER\n to save the configuration.  \n\n\nThe database will now show up on the list of available databases when creating a cluster under advanced \nExternal Sources\n \n \nConfigure Databases\n. You can select it and click \nAttach\n each time you would like to use it for a cluster.", 
            "title": "Register external database"
        }, 
        {
            "location": "/external-db/index.html#using-an-external-database-for-cluster-services", 
            "text": "Cloudbreak allows you to register an existing RDBMS instance as an  external source  to be used for a database for certain services. After you register the RDBMS with Cloudbreak, you can use it for multiple clusters.", 
            "title": "Using an external database for cluster services"
        }, 
        {
            "location": "/external-db/index.html#supported-databases", 
            "text": "If you would like to use an external database for one of the components that support it, you may use the following database types and versions:      Component  Supported databases  Description        Ambari  PostgreSQL, MySQL  By default, Ambari uses an embedded PostgreSQL instance.      Druid  PostgreSQL, MySQL  You must provide an external database.      Hive  PostgreSQL, MySQL, Oracle 11, Oracle 12  By default, Cloudbreak installs a PostgreSQL instance on the Hive Metastore host.      Oozie  PostgreSQL, MySQL, Oracle 11, Oracle 12  You must provide an external database.      Ranger  PostgreSQL, MySQL, Oracle 11, Oracle 12  You must provide an external database.      Superset  PostgreSQL, MySQL  You must provide an external database.      Other  PostgreSQL, MySQL, Oracle 11, Oracle 12", 
            "title": "Supported databases"
        }, 
        {
            "location": "/external-db/index.html#external-database-options", 
            "text": "Cloudbreak includes the following external database options:     Option  Description  Blueprint requirements  Steps  Example      Built-in types  Cloudbreak includes a few built-in types: Hive, Druid, Ranger, Superset, and Oozie.  Use a standard blueprint which does not include any JDBC  parameters. Cloudbreak automatically injects the JDBC property variables into the blueprint.  Simply  register the database in the UI . After that, you can attach the database config to your clusters.  Refer to  Example 1    Other types  In addition to the built-in types, Cloudbreak allows you to specify custom types. In the UI, this corresponds to the UI option is called \"Other\"   \"Enter the type\".  You must provide a custom dynamic blueprint which includes RDBMS-specific variables. Refer to  Creating a template blueprint .  Prepare your custom blueprint first. Next,  register the database in the UI . After that, you can attach the database config to your clusters.  Refer to  Example 2     During cluster create, Cloudbreak checks whether the JDBC properties are present in the blueprint:", 
            "title": "External database options"
        }, 
        {
            "location": "/external-db/index.html#example-1-built-in-type-hive", 
            "text": "In this scenario, you start up with a standard blueprint, and Cloudbreak injects the JDBC properties into the blueprint.    Register an existing external database of \"Hive\" type (built-in type):      Property variable  Example value      rds.hive.connectionString  jdbc:postgresql://ec2-54-159-202-231.compute-1.amazonaws.com:5432/hive    rds.hive.connectionDriver  org.postgresql.Driver    rds.hive.connectionUserName  myuser    rds.hive.connectionPassword  Hadoop123!    rds.hive.fancyName  PostgreSQL    rds.hive.databaseType  postgres       Create a cluster by using a standard blueprint (i.e. one without JDBC related variables) and by attaching the external Hive database configuration.      Upon cluster create, Hive JDBC properties will be injected into the blueprint according to the following template:  ...\n\"hive-site\": {\n\"properties\": {\n  \"javax.jdo.option.ConnectionURL\": \"{{{ rds.hive.connectionString }}}\",\n  \"javax.jdo.option.ConnectionDriverName\": \"{{{ rds.hive.connectionDriver }}}\",\n  \"javax.jdo.option.ConnectionUserName\": \"{{{ rds.hive.connectionUserName }}}\",\n  \"javax.jdo.option.ConnectionPassword\": \"{{{ rds.hive.connectionPassword }}}\"\n}\n  },\n  \"hive-env\" : {\n\"properties\" : {\n  \"hive_database\" : \"Existing {{{ rds.hive.fancyName}}} Database\",\n  \"hive_database_type\" : \"{{{ rds.hive.databaseType }}}\"\n}\n}\n...", 
            "title": "Example 1: Built-in type Hive"
        }, 
        {
            "location": "/external-db/index.html#example-2-other-type", 
            "text": "In this scenario, you start up with a special blueprint including JDBC property variables, and Cloudbreak replaces JDBC-related property variables in the blueprint.     Prepare a blueprint blueprint that includes property variables. Use  mustache template  syntax. For example:  ...\n\"test-site\": {\n  \"properties\": {\n   \"javax.jdo.option.ConnectionURL\":\"{{rds.test.connectionString}}\"\n  }\n...    Register an existing external database of some \"Other\" type. For example:      Property variable  Example value      rds.hive.connectionString  ec2-54-159-202-231.compute-1.amazonaws.com:5432/hive    rds.hive.connectionDriver  org.postgresql.Driver    rds.hive.connectionUserName  myuser    rds.hive.connectionPassword  Hadoop123!    rds.hive.subprotocol  postgres    rds.hive.databaseEngine  POSTGRES       Create a cluster by using your custom blueprint and by attaching the external database configuration.      Upon cluster create, Cloudbreak replaces JDBC-related property variables in the blueprint.     Related links  Mustache template syntax", 
            "title": "Example 2: Other type"
        }, 
        {
            "location": "/external-db/index.html#creating-a-template-blueprint-for-rdmbs", 
            "text": "In order to use an external RDBMS for some component other than the built-in components, you must include JDBC property variables in your blueprint. You must use  mustache template  syntax. See  Example 2: Other type  for an example configuration.   Related links  Creating a template blueprint  Mustache template syntax  (External)", 
            "title": "Creating a template blueprint for RDMBS"
        }, 
        {
            "location": "/external-db/index.html#register-an-external-database", 
            "text": "You must create the external RDBMS instance and database prior to registering it with Cloudbreak. Once you have it ready, you can:   Register it in Cloudbreak web UI or CLI.    Use it with one or more clusters. Once registered, the database will now show up in the list of available databases when creating a cluster under advanced  External Sources     Configure Databases .     Prerequisites     If you are planning to use an external MySQL or Oracle database, you must download the JDBC connector's JAR file and place it in a location available to the cluster host on which Ambari is installed. The steps below require that you provide the URL to the JDBC connector's JAR file.    If you are using your own  custom image , you may place the JDBC connector's JAR file directly on the machine as part of the image burning process.    Steps     From the navigation pane, select  External Sources     Database Configurations .    Select  Register Database Configuration .       Provide the following information:     Parameter  Description      Name  Enter the name to use when registering this database to Cloudbreak. This is  not  the database name.    Type  Select the service for which you would like to use the external database. If you selected \"Other\", you must provide a special blueprint.    JDBC Connection  Select the database  type  and enter the  JDBC connection  string (HOST:PORT/DB_NAME).    Connector's JAR URL  (MySQL and Oracle only) Provide a URL to the JDBC connector's JAR file. The JAR file must be hosted in a location accessible to the cluster host on which Ambari is installed. At cluster creation time, Cloudbreak places the JAR file in the /opts/jdbc-drivers directory. You do not need to provide the \"Connector's JAR URL if you are using a custom image and the JAR file was either manually placed on the VM as part of custom image burning or it was placed there by using a recipe.    Username  Enter the JDBC connection username.    Password  Enter the JDBC connection password.       Click  Test Connection  to validate and test the RDS connection information.     Once your settings are validated and working, click  REGISTER  to save the configuration.    The database will now show up on the list of available databases when creating a cluster under advanced  External Sources     Configure Databases . You can select it and click  Attach  each time you would like to use it for a cluster.", 
            "title": "Register an external database"
        }, 
        {
            "location": "/external-ldap/index.html", 
            "text": "Using an external authentication source for clusters\n\n\nCloudbreak allows you to register an existing LDAP/AD instance as an \nexternal source\n and use it for multiple clusters. You must create the LDAP/AD prior to registering it with Cloudbreak. Once you have it ready, the overall steps are:\n\n\n\n\nRegister an existing LDAP in Cloudbreak web UI or CLI.  \n\n\nOnce registered, the LDAP will now show up in the list of available authentication sources when creating a cluster under advanced \nExternal Sources\n \n \nConfigure Authentication\n.  \n\n\nPrepare a blueprint as described in \nPreparing a blueprint for LDAP/AD \n.  \n\n\nCreate a cluster by using the blueprint and by attaching the authentication source. Cloudbreak automatically injects the LDAP property variables into the blueprint. \n\n\n\n\nPreparing a blueprint for LDAP/AD\n\n\nIn order to use LDAP/AD for your cluster, you must provide a suitable cluster blueprint:\n\n\n\n\nThe blueprint must include one or more of the following supported components: Atlas, Hadoop, Hive LLAP, Ranger Admin, Ranger UserSync.  \n\n\nThe blueprint should not include any LDAP properties. Before injecting the properties, Cloudbreak checks if LDAP related properties already exist in the blueprint. If they exist, they are not injected.  \n\n\n\n\nDuring cluster creation the following properties will be injected in the blueprint:\n\n\n\n\nldap.connectionURL  \n\n\nldap.domain  \n\n\nldap.bindDn  \n\n\nldap.bindPassword  \n\n\nldap.userSearchBase  \n\n\nldap.userObjectClass  \n\n\nldap.userNameAttribute  \n\n\nldap.groupSearchBase  \n\n\nldap.groupObjectClass  \n\n\nldap.groupNameAttribute  \n\n\nldap.groupMemberAttribute  \n\n\nldap.directoryType  \n\n\nldap.directoryTypeShort  \n\n\n\n\nTheir values will be the values that you provided to Cloudbreak: \n\n\n\n\nRegister an authentication source\n\n\nCloudbreak allows you to register an existing LDAP/AD instance and use it for multiple clusters. You must create the LDAP/AD prior to registering it with Cloudbreak. Once you have it ready, you can:\n\n\n\n\nRegister an existing LDAP in Cloudbreak web UI or CLI.  \n\n\nUse it as an authentication source for your clusters. Once registered, the LDAP will now show up in the list of available authentication sources when creating a cluster under advanced \nExternal Sources\n \n \nConfigure Authentication\n.   \n\n\n\n\nSteps\n\n\n\n\nFrom the navigation pane, select \nExternal Sources\n \n \nAuthentication Configurations\n.  \n\n\nSelect \nRegister Authentication Source\n.     \n\n\n\n\nProvide the following parameters related to your existing LDAP/AD: \n\n\nGENERAL CONFIGURATION\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter a name for your LDAP.\n\n\ncb-ldap\n\n\n\n\n\n\nDirectory Type\n\n\nChoose whether your directory is \nLDAP\n or \nActive Directory\n.\n\n\nLDAP\n\n\n\n\n\n\nLDAP Server Connection\n\n\nSelect \nLDAP\n or \nLDAPS\n.\n\n\nLDAP\n\n\n\n\n\n\nServer Host\n\n\nEnter the hostname for the LDAP or AD server .\n\n\n10.0.3.128\n\n\n\n\n\n\nServer Port\n\n\nEnter the port.\n\n\n389\n\n\n\n\n\n\nLDAP Bind DN\n\n\nEnter the root Distinguished Name to search in the directory for users.\n\n\nCN=Administrator,CN=Users,DC=ad,DC=hdc,DC=com\n\n\n\n\n\n\nLDAP Bind Password\n\n\nEnter your root Distinguished Name password.\n\n\nMyPassword1234!\n\n\n\n\n\n\n\n\nUSER CONFIGURATION\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nLDAP User Search Base\n\n\nEnter your LDAP user search base. This defines the location in the directory from which the LDAP search begins.\n\n\nCN=Users,DC=ad,DC=hdc,DC=com\n\n\n\n\n\n\nLDAP User Name Attribute\n\n\nEnter the attribute for which to conduct a search on the user base.\n\n\nHDCaccountName\n\n\n\n\n\n\nLDAP User Object Class\n\n\nEnter the directory object class for users.\n\n\nperson\n\n\n\n\n\n\n\n\nGROUP CONFIGURATION\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nLDAP Group Search Base\n\n\nEnter your LDAP group search base. This defines the location in the directory from which the LDAP search begins.\n\n\nOU-scoDC=ad,DC=hdc,DC=com\n\n\n\n\n\n\nLDAP Admin Group\n\n\n(Optional) Enter your LDAP admin group, if needed.\n\n\n\n\n\n\n\n\nLDAP Group Name Attribute\n\n\nEnter the attribute for which to conduct a search on groups.\n\n\ncn\n\n\n\n\n\n\nLDAP Group Object Class\n\n\nEnter the directory object class for groups.\n\n\ngroup\n\n\n\n\n\n\nLDAP Group Member Attribute\n\n\nEnter the attribute on the group object class that represents members.\n\n\nmember\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nTest Connection\n to verify that the connection information that you entered is correct.\n\n\n\n\n\n\nClick \nREGISTER\n. \n\n\n\n\n\n\nThe LDAP will now show up on the list of available authentication sources when creating a cluster under advanced \nExternal Sources\n \n \nConfigure Authentication\n. It can be reused with multiple clusters. Just select it if you would like to use it for a given cluster.", 
            "title": "Register external authentication source"
        }, 
        {
            "location": "/external-ldap/index.html#using-an-external-authentication-source-for-clusters", 
            "text": "Cloudbreak allows you to register an existing LDAP/AD instance as an  external source  and use it for multiple clusters. You must create the LDAP/AD prior to registering it with Cloudbreak. Once you have it ready, the overall steps are:   Register an existing LDAP in Cloudbreak web UI or CLI.    Once registered, the LDAP will now show up in the list of available authentication sources when creating a cluster under advanced  External Sources     Configure Authentication .    Prepare a blueprint as described in  Preparing a blueprint for LDAP/AD  .    Create a cluster by using the blueprint and by attaching the authentication source. Cloudbreak automatically injects the LDAP property variables into the blueprint.", 
            "title": "Using an external authentication source for clusters"
        }, 
        {
            "location": "/external-ldap/index.html#preparing-a-blueprint-for-ldapad", 
            "text": "In order to use LDAP/AD for your cluster, you must provide a suitable cluster blueprint:   The blueprint must include one or more of the following supported components: Atlas, Hadoop, Hive LLAP, Ranger Admin, Ranger UserSync.    The blueprint should not include any LDAP properties. Before injecting the properties, Cloudbreak checks if LDAP related properties already exist in the blueprint. If they exist, they are not injected.     During cluster creation the following properties will be injected in the blueprint:   ldap.connectionURL    ldap.domain    ldap.bindDn    ldap.bindPassword    ldap.userSearchBase    ldap.userObjectClass    ldap.userNameAttribute    ldap.groupSearchBase    ldap.groupObjectClass    ldap.groupNameAttribute    ldap.groupMemberAttribute    ldap.directoryType    ldap.directoryTypeShort     Their values will be the values that you provided to Cloudbreak:", 
            "title": "Preparing a blueprint for LDAP/AD"
        }, 
        {
            "location": "/external-ldap/index.html#register-an-authentication-source", 
            "text": "Cloudbreak allows you to register an existing LDAP/AD instance and use it for multiple clusters. You must create the LDAP/AD prior to registering it with Cloudbreak. Once you have it ready, you can:   Register an existing LDAP in Cloudbreak web UI or CLI.    Use it as an authentication source for your clusters. Once registered, the LDAP will now show up in the list of available authentication sources when creating a cluster under advanced  External Sources     Configure Authentication .      Steps   From the navigation pane, select  External Sources     Authentication Configurations .    Select  Register Authentication Source .        Provide the following parameters related to your existing LDAP/AD:   GENERAL CONFIGURATION     Parameter  Description  Example      Name  Enter a name for your LDAP.  cb-ldap    Directory Type  Choose whether your directory is  LDAP  or  Active Directory .  LDAP    LDAP Server Connection  Select  LDAP  or  LDAPS .  LDAP    Server Host  Enter the hostname for the LDAP or AD server .  10.0.3.128    Server Port  Enter the port.  389    LDAP Bind DN  Enter the root Distinguished Name to search in the directory for users.  CN=Administrator,CN=Users,DC=ad,DC=hdc,DC=com    LDAP Bind Password  Enter your root Distinguished Name password.  MyPassword1234!     USER CONFIGURATION     Parameter  Description  Example      LDAP User Search Base  Enter your LDAP user search base. This defines the location in the directory from which the LDAP search begins.  CN=Users,DC=ad,DC=hdc,DC=com    LDAP User Name Attribute  Enter the attribute for which to conduct a search on the user base.  HDCaccountName    LDAP User Object Class  Enter the directory object class for users.  person     GROUP CONFIGURATION     Parameter  Description  Example      LDAP Group Search Base  Enter your LDAP group search base. This defines the location in the directory from which the LDAP search begins.  OU-scoDC=ad,DC=hdc,DC=com    LDAP Admin Group  (Optional) Enter your LDAP admin group, if needed.     LDAP Group Name Attribute  Enter the attribute for which to conduct a search on groups.  cn    LDAP Group Object Class  Enter the directory object class for groups.  group    LDAP Group Member Attribute  Enter the attribute on the group object class that represents members.  member       Click  Test Connection  to verify that the connection information that you entered is correct.    Click  REGISTER .     The LDAP will now show up on the list of available authentication sources when creating a cluster under advanced  External Sources     Configure Authentication . It can be reused with multiple clusters. Just select it if you would like to use it for a given cluster.", 
            "title": "Register an authentication source"
        }, 
        {
            "location": "/external-proxy/index.html", 
            "text": "Register a proxy\n\n\nCloudbreak allows you to save your existing proxy configuration information as an \nexternal source\n so that you can provide the proxy information to multiple clusters that you create with Cloudbreak. The steps are:       \n\n\n\n\nRegister your proxy in Cloudbreak web UI or CLI.   \n\n\nOnce the proxy has been registered with Cloudbreak, it will show up in the list of available proxies when creating a cluster under advanced \nExternal Sources\n \n \nConfigure Proxy\n.  \n\n\n\n\nSteps\n \n\n\n\n\nFrom the navigation pane, select \nExternal Sources\n \n \nProxy Configurations\n.  \n\n\nSelect \nRegister Proxy Configuration\n.    \n\n\n\n\nProvide the following information:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nName\n\n\nEnter the name to use when registering this database to Cloudbreak. This is \nnot\n the database name.\n\n\nmy-proxy\n\n\n\n\n\n\nDescription\n\n\nProvide description.\n\n\n\n\n\n\n\n\nProtocol\n\n\nSelect HTTP or HTTPS.\n\n\nHTTPS\n\n\n\n\n\n\nServer Host\n\n\nEnter the URL of your proxy server host.\n\n\n10.0.2.237\n\n\n\n\n\n\nServer Port\n\n\nEnter proxy server port.\n\n\n3128\n\n\n\n\n\n\nUsername\n\n\nEnter the username for the proxy.\n\n\ntestuser\n\n\n\n\n\n\nPassword\n\n\nEnter the password for the proxy.\n\n\nMyPassword123\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nREGISTER\n to save the configuration. \n\n\n\n\n\n\nThe proxy will now show up when creating a cluster under advanced \nExternal Sources\n \n \nConfigure Proxy\n. You can select it each time you would like to use it for a cluster.", 
            "title": "Register proxy configuration"
        }, 
        {
            "location": "/external-proxy/index.html#register-a-proxy", 
            "text": "Cloudbreak allows you to save your existing proxy configuration information as an  external source  so that you can provide the proxy information to multiple clusters that you create with Cloudbreak. The steps are:          Register your proxy in Cloudbreak web UI or CLI.     Once the proxy has been registered with Cloudbreak, it will show up in the list of available proxies when creating a cluster under advanced  External Sources     Configure Proxy .     Steps     From the navigation pane, select  External Sources     Proxy Configurations .    Select  Register Proxy Configuration .       Provide the following information:     Parameter  Description  Example      Name  Enter the name to use when registering this database to Cloudbreak. This is  not  the database name.  my-proxy    Description  Provide description.     Protocol  Select HTTP or HTTPS.  HTTPS    Server Host  Enter the URL of your proxy server host.  10.0.2.237    Server Port  Enter proxy server port.  3128    Username  Enter the username for the proxy.  testuser    Password  Enter the password for the proxy.  MyPassword123       Click  REGISTER  to save the configuration.     The proxy will now show up when creating a cluster under advanced  External Sources     Configure Proxy . You can select it each time you would like to use it for a cluster.", 
            "title": "Register a proxy"
        }, 
        {
            "location": "/hostnames/index.html", 
            "text": "Using custom hostnames based on DNS\n\n\nBy default, when Cloudbreak provisions cloud provider resources, your cloud provider assigns hostnames for your cluster nodes. Optionally, instead of using default hostnames, you can configure Cloudbreak to use custom hostnames based on DNS. To do that, follow the steps for configuring reverse Domain Name System (DNS) described below. \n\n\nWhen the cluster node machines are provisioned, they try to make a reverse DNS lookup (by querying of the DNS to determine the domain name associated with a specific IP address); if the reverse DNS lookup returns a valid value, then that value is set as hostname. This is why reverse DNS setup is required for using custom hostnames.   \n\n\nConfigure DNS on AWS\n\n\nOn AWS you have the following two options:\n\n\n\n\nUse \nRoute53\n as DNS provider.  \n\n\nSet up your own DNS server in your VPC  \n\n\n\n\nBoth options require you to have an existing VPC and attach a custom \nDHCP option\n to it.\n\n\nThe instructions provided in this section describe how to perform the steps in Amazon web consoles, but the steps can also be performed (and automated) in the AWS CLI.  \n\n\nConfigure DNS using Route53\n\n\nFollow these general steps to configure reverse DNS using \nRoute53\n. \n\n\nSteps\n\n\n\n\n\n\nCreate a new VPC or use your existing VPC\n:\n\n\n\n\nYou can create a new VPC from the Amazon VCP console (for example by using \nStart VPC Wizard\n):              \n\n\nCIDR block example: \n10.1.0.0/16\n \n\n\nSubnet's CIDR example: \n10.1.1.0/28\n  \n\n\n\n\n\n\nMake sure to:\n\n\nEnable DNS resolution for the VPC. You can do this by selecting the VPC, selecting \nActions \n Edit DNS resolution\n and choosing \nYes\n. \n\n\nEnable DNS hostnames for the VPC. You can do this by selecting the VPC, selecting \nActions \n Edit DNS hostnames\n and choosing \nYes\n.\n\n\n\n\n\n\n\n\n\n\nOptionally, you may want to set up an Internet Gateway for the VPC and add a default route to the routing table for the Internet Gateway.\nAdditionally, you may want to enable the \nAuto-assign Public IP\n option.\nThis way Cloudbreak would reach the cluster from outside of the VPC and the cluster would have internet access.\n\n\n\n\n\n\n\n\nCreate a DHCP options set\n: \n\n\nPerform this step from the Amazon VPC console. Select \nDHCP Options Sets\n from the left pane and click on \nCreate a DHCP options set\n. Make sure to:   \n\n\n\n\nSet the \nDomain name\n to a preferred domain, for example \ncloudbreak.beer\n\n\n\n\nSet the \nDomain name servers\n to \nAmazonProvidedDNS\n  \n\n\n   \n\n\n\n\n\n\nFor detailed steps, refer to \nAWS documentation\n. \n\n\n\n\n\n\nAssign the newly created DHCP options set to your VPC\n:\n\n\n\n\nFrom the Amazon VPC console, select \nYour VPCs\n from the left pane.\n\n\nSelect the VPC created earlier.  \n\n\nClick on \nActions \n Edit DHCP Options Set\n.  \n\n\nSelect the newly created DHCP option set.  \n\n\n\n\n\n\n\n\nConfigure your domain at Route53\n:\n\n\nPerform these steps from the Amazon Route53 console.\n\nFor general steps, refer to \nAWS documentation\n. \n\n\n\n\nSelect \nHosted zones\n from the left pane.    \n\n\n\n\nCreate a hosted zone by clicking on \nCreate Hosted Zone\n. Make sure to:   \n\n\n\n\nUse the same domain name as used previously with the DHCP options set (In the example this was \ncloudbreak.beer\n). \n\n\nSet the \nType\n to \nPrivate Hosted Zone for Amazon VPC\n.  \n\n\nSelect the VPC ID of the VPC to which you previously assigned the DHCP option.  \n\n\n\n\n\n\n\n\nAdd records for your hosted zone:\n\n\n\n\nSelect the hosted zone and choose \nGo to Record Sets\n\n\n\n\nClick \nCreate Record Set\n to create a record set. You must perform this step for every available IP, so that each IP can have a custom name (If you used the subnet example listed above, these IPs will be in the range of 10.1.1.4-14):\n\n\n\n\nType: select \nA\n\n\nName: for example \nb10\n\n\nValue: for example \n10.1.1.10\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nAfter performing this step for each IP, you should end up with an many records as IPs. For example:\n\n      \n\n\n\n\n\n\n\n\n\n\nCreate another hosted zone for reverse DNS lookup.\n\n\nPerform these steps from the Amazon Route53 console.  \n\n\n\n\nSelect \nHosted zones\n from the left pane.    \n\n\n\n\nCreate a hosted zone by clicking on \nCreate Hosted Zone\n. Make sure to: \n\n\n\n\n\n\nFor example, if you used the subnet example listed above, its \nDomain name\n should look like this (as reverse DNS lookups use the special domain \nin-addr.arpa\n):\n\n\n1.1.10.in-addr.arpa.\n\n\n\n\n\n\nSet the Type to \nPrivate Hosted Zone for Amazon VPC\n.\n\n\n\n\nSelect the VPC ID to which you previously assigned the DHCP option set.  \n\n\n\n\n\n\n\n\nAdd records for every created domain: \n\n\n\n\nType: select \nPTR\n \n\n\nName: This determines the first part of the IP, for example \n10\n\n\n\n\nValue: Enter the domain name that you set in the previous step, for example, \nb10\n\n\n\n\n\n\n\n\n\n\n\n\nAfter performing this step for each domain, you should end up with as many records as IPs. For example:\n\n      \n\n\n\n\n\n\n\n\n\n\nCreate the cluster in the VPC configured in the earlier steps\n and you will have the same hostnames set as the domain names.\n\n\n\n\nSince you don't have control the order over the IP addresses leased to the machines, the names may not be in order.\n\n\n\n\n\n\n\n\nConfigure DNS using custom DNS server\n\n\nFollow these general steps to configure reverse DNS using a custom DNS server.  \n\n\nSteps\n\n\n\n\n\n\nCreate a new VPC or use your existing VPC\n:\n\n\n\n\nYou can create a new VPC from the Amazon VCP console (for example by using \nStart VPC Wizard\n):              \n\n\nCIDR block example: \n10.1.0.0/16\n \n\n\nSubnet's CIDR example: \n10.1.1.0/28\n  \n\n\n\n\n\n\nMake sure to:\n\n\nEnable DNS resolution for the VPC. You can do this by selecting the VPC, selecting \nActions \n Edit DNS resolution\n and choosing \nYes\n. \n\n\nEnable DNS hostnames for the VPC. You can do this by selecting the VPC, selecting \nActions \n Edit DNS hostnames\n and choosing \nYes\n.\n\n\n\n\n\n\n\n\n\n\nOptionally, you may want to set up an Internet Gateway for the VPC and add a default route to the routing table for the Internet Gateway.\nAdditionally, you may want to enable the \nAuto-assign Public IP\n option.\nThis way Cloudbreak would reach the cluster from outside of the VPC and the cluster would have internet access.\n\n\n\n\n\n\n\n\nSet up DNS server in your VPC/subnet\n:\n\n\n\n\nIn the configuration ensure that you have DNS records and reverse DNS pointers for all IP address (for example 10.3.3.4-14)\n\n\n\n\nExample unbound configuration:\n\n\n\n    [root@ip-10-3-3-9 conf.d]# cat 00-cloudbreak.cloud.conf\n    server:\n        local-zone: \"cloudbreak.cloud.\" static\n        local-data: \"aww1.cloudbreak.cloud. IN A 10.3.3.4\"\n        local-data-ptr: \"10.3.3.4 aww1.cloudbreak.cloud.\"\n        local-data: \"aww2.cloudbreak.cloud. IN A 10.3.3.5\"\n        local-data-ptr: \"10.3.3.5 aww2.cloudbreak.cloud.\"\n        local-data: \"aww3.cloudbreak.cloud. IN A 10.3.3.6\"\n        local-data-ptr: \"10.3.3.6 aww3.cloudbreak.cloud.\"\n        local-data: \"aww4.cloudbreak.cloud. IN A 10.3.3.7\"\n        local-data-ptr: \"10.3.3.7 aww4.cloudbreak.cloud.\"\n        local-data: \"aww5.cloudbreak.cloud. IN A 10.3.3.8\"\n        local-data-ptr: \"10.3.3.8 aww5.cloudbreak.cloud.\"\n        local-data: \"aww6.cloudbreak.cloud. IN A 10.3.3.9\"\n        local-data-ptr: \"10.3.3.9 aww6.cloudbreak.cloud.\"\n        local-data: \"aww7.cloudbreak.cloud. IN A 10.3.3.10\"\n        local-data-ptr: \"10.3.3.10 aww7.cloudbreak.cloud.\"\n        local-data: \"aww8.cloudbreak.cloud. IN A 10.3.3.11\"\n        local-data-ptr: \"10.3.3.11 aww8.cloudbreak.cloud.\"\n        local-data: \"aww9.cloudbreak.cloud. IN A 10.3.3.12\"\n        local-data-ptr: \"10.3.3.12 aww9.cloudbreak.cloud.\"\n        local-data: \"aww10.cloudbreak.cloud. IN A 10.3.3.13\"\n        local-data-ptr: \"10.3.3.13 aww10.cloudbreak.cloud.\"\n        local-data: \"aww11.cloudbreak.cloud. IN A 10.3.3.14\"\n        local-data-ptr: \"10.3.3.14 aww11.cloudbreak.cloud.\"\n\n\n\n\n\n\n\n\n\n\n\nCreate a DHCP options set\n: \n\n\nPerform this step from the Amazon VPC console. Select \nDHCP Options Sets\n from the left pane and click on \nCreate a DHCP options set\n. Make sure to:   \n\n\n\n\nSet the \nDomain name\n to your preferred domain, for example \ncloudbreak.cloud\n\n\nSet \nDomain name servers\n to the previously created DNS server\n\n\n\n\nOptionally, set a \nName tag\n  \n\n\n  \n\n\n\n\n\n\nFor detailed steps, refer to \nAWS documentation\n. \n\n\n\n\n\n\nAssign the newly created DHCP options set to your VPC\n:\n\n\n\n\nFrom the Amazon VPC console, select \nYour VPCs\n from the left pane.\n\n\nSelect the VPC created earlier.  \n\n\nClick on \nActions \n Edit DHCP Options Set\n.  \n\n\nSelect the newly created DHCP option set.  \n\n\n\n\n\n\n\n\nCreate the cluster in the VPC configured in the preceding steps\n and you will have the same hostnames set as the domain names.\n\n\n\n\nSince you don't have control the order over the IP addresses leased to the machines, the names may not be in order.", 
            "title": "Use custom hostnames based on DNS"
        }, 
        {
            "location": "/hostnames/index.html#using-custom-hostnames-based-on-dns", 
            "text": "By default, when Cloudbreak provisions cloud provider resources, your cloud provider assigns hostnames for your cluster nodes. Optionally, instead of using default hostnames, you can configure Cloudbreak to use custom hostnames based on DNS. To do that, follow the steps for configuring reverse Domain Name System (DNS) described below.   When the cluster node machines are provisioned, they try to make a reverse DNS lookup (by querying of the DNS to determine the domain name associated with a specific IP address); if the reverse DNS lookup returns a valid value, then that value is set as hostname. This is why reverse DNS setup is required for using custom hostnames.", 
            "title": "Using custom hostnames based on DNS"
        }, 
        {
            "location": "/hostnames/index.html#configure-dns-on-aws", 
            "text": "On AWS you have the following two options:   Use  Route53  as DNS provider.    Set up your own DNS server in your VPC     Both options require you to have an existing VPC and attach a custom  DHCP option  to it.  The instructions provided in this section describe how to perform the steps in Amazon web consoles, but the steps can also be performed (and automated) in the AWS CLI.", 
            "title": "Configure DNS on AWS"
        }, 
        {
            "location": "/hostnames/index.html#configure-dns-using-route53", 
            "text": "Follow these general steps to configure reverse DNS using  Route53 .   Steps    Create a new VPC or use your existing VPC :   You can create a new VPC from the Amazon VCP console (for example by using  Start VPC Wizard ):                CIDR block example:  10.1.0.0/16    Subnet's CIDR example:  10.1.1.0/28       Make sure to:  Enable DNS resolution for the VPC. You can do this by selecting the VPC, selecting  Actions   Edit DNS resolution  and choosing  Yes .   Enable DNS hostnames for the VPC. You can do this by selecting the VPC, selecting  Actions   Edit DNS hostnames  and choosing  Yes .      Optionally, you may want to set up an Internet Gateway for the VPC and add a default route to the routing table for the Internet Gateway.\nAdditionally, you may want to enable the  Auto-assign Public IP  option.\nThis way Cloudbreak would reach the cluster from outside of the VPC and the cluster would have internet access.     Create a DHCP options set :   Perform this step from the Amazon VPC console. Select  DHCP Options Sets  from the left pane and click on  Create a DHCP options set . Make sure to:      Set the  Domain name  to a preferred domain, for example  cloudbreak.beer   Set the  Domain name servers  to  AmazonProvidedDNS            For detailed steps, refer to  AWS documentation .     Assign the newly created DHCP options set to your VPC :   From the Amazon VPC console, select  Your VPCs  from the left pane.  Select the VPC created earlier.    Click on  Actions   Edit DHCP Options Set .    Select the newly created DHCP option set.       Configure your domain at Route53 :  Perform these steps from the Amazon Route53 console. \nFor general steps, refer to  AWS documentation .    Select  Hosted zones  from the left pane.       Create a hosted zone by clicking on  Create Hosted Zone . Make sure to:      Use the same domain name as used previously with the DHCP options set (In the example this was  cloudbreak.beer ).   Set the  Type  to  Private Hosted Zone for Amazon VPC .    Select the VPC ID of the VPC to which you previously assigned the DHCP option.       Add records for your hosted zone:   Select the hosted zone and choose  Go to Record Sets   Click  Create Record Set  to create a record set. You must perform this step for every available IP, so that each IP can have a custom name (If you used the subnet example listed above, these IPs will be in the range of 10.1.1.4-14):   Type: select  A  Name: for example  b10  Value: for example  10.1.1.10          After performing this step for each IP, you should end up with an many records as IPs. For example: \n            Create another hosted zone for reverse DNS lookup.  Perform these steps from the Amazon Route53 console.     Select  Hosted zones  from the left pane.       Create a hosted zone by clicking on  Create Hosted Zone . Make sure to:     For example, if you used the subnet example listed above, its  Domain name  should look like this (as reverse DNS lookups use the special domain  in-addr.arpa ):  1.1.10.in-addr.arpa.    Set the Type to  Private Hosted Zone for Amazon VPC .   Select the VPC ID to which you previously assigned the DHCP option set.       Add records for every created domain:    Type: select  PTR    Name: This determines the first part of the IP, for example  10   Value: Enter the domain name that you set in the previous step, for example,  b10       After performing this step for each domain, you should end up with as many records as IPs. For example: \n            Create the cluster in the VPC configured in the earlier steps  and you will have the same hostnames set as the domain names.   Since you don't have control the order over the IP addresses leased to the machines, the names may not be in order.", 
            "title": "Configure DNS using Route53"
        }, 
        {
            "location": "/hostnames/index.html#configure-dns-using-custom-dns-server", 
            "text": "Follow these general steps to configure reverse DNS using a custom DNS server.    Steps    Create a new VPC or use your existing VPC :   You can create a new VPC from the Amazon VCP console (for example by using  Start VPC Wizard ):                CIDR block example:  10.1.0.0/16    Subnet's CIDR example:  10.1.1.0/28       Make sure to:  Enable DNS resolution for the VPC. You can do this by selecting the VPC, selecting  Actions   Edit DNS resolution  and choosing  Yes .   Enable DNS hostnames for the VPC. You can do this by selecting the VPC, selecting  Actions   Edit DNS hostnames  and choosing  Yes .      Optionally, you may want to set up an Internet Gateway for the VPC and add a default route to the routing table for the Internet Gateway.\nAdditionally, you may want to enable the  Auto-assign Public IP  option.\nThis way Cloudbreak would reach the cluster from outside of the VPC and the cluster would have internet access.     Set up DNS server in your VPC/subnet :   In the configuration ensure that you have DNS records and reverse DNS pointers for all IP address (for example 10.3.3.4-14)   Example unbound configuration:  \n    [root@ip-10-3-3-9 conf.d]# cat 00-cloudbreak.cloud.conf\n    server:\n        local-zone: \"cloudbreak.cloud.\" static\n        local-data: \"aww1.cloudbreak.cloud. IN A 10.3.3.4\"\n        local-data-ptr: \"10.3.3.4 aww1.cloudbreak.cloud.\"\n        local-data: \"aww2.cloudbreak.cloud. IN A 10.3.3.5\"\n        local-data-ptr: \"10.3.3.5 aww2.cloudbreak.cloud.\"\n        local-data: \"aww3.cloudbreak.cloud. IN A 10.3.3.6\"\n        local-data-ptr: \"10.3.3.6 aww3.cloudbreak.cloud.\"\n        local-data: \"aww4.cloudbreak.cloud. IN A 10.3.3.7\"\n        local-data-ptr: \"10.3.3.7 aww4.cloudbreak.cloud.\"\n        local-data: \"aww5.cloudbreak.cloud. IN A 10.3.3.8\"\n        local-data-ptr: \"10.3.3.8 aww5.cloudbreak.cloud.\"\n        local-data: \"aww6.cloudbreak.cloud. IN A 10.3.3.9\"\n        local-data-ptr: \"10.3.3.9 aww6.cloudbreak.cloud.\"\n        local-data: \"aww7.cloudbreak.cloud. IN A 10.3.3.10\"\n        local-data-ptr: \"10.3.3.10 aww7.cloudbreak.cloud.\"\n        local-data: \"aww8.cloudbreak.cloud. IN A 10.3.3.11\"\n        local-data-ptr: \"10.3.3.11 aww8.cloudbreak.cloud.\"\n        local-data: \"aww9.cloudbreak.cloud. IN A 10.3.3.12\"\n        local-data-ptr: \"10.3.3.12 aww9.cloudbreak.cloud.\"\n        local-data: \"aww10.cloudbreak.cloud. IN A 10.3.3.13\"\n        local-data-ptr: \"10.3.3.13 aww10.cloudbreak.cloud.\"\n        local-data: \"aww11.cloudbreak.cloud. IN A 10.3.3.14\"\n        local-data-ptr: \"10.3.3.14 aww11.cloudbreak.cloud.\"      Create a DHCP options set :   Perform this step from the Amazon VPC console. Select  DHCP Options Sets  from the left pane and click on  Create a DHCP options set . Make sure to:      Set the  Domain name  to your preferred domain, for example  cloudbreak.cloud  Set  Domain name servers  to the previously created DNS server   Optionally, set a  Name tag           For detailed steps, refer to  AWS documentation .     Assign the newly created DHCP options set to your VPC :   From the Amazon VPC console, select  Your VPCs  from the left pane.  Select the VPC created earlier.    Click on  Actions   Edit DHCP Options Set .    Select the newly created DHCP option set.       Create the cluster in the VPC configured in the preceding steps  and you will have the same hostnames set as the domain names.   Since you don't have control the order over the IP addresses leased to the machines, the names may not be in order.", 
            "title": "Configure DNS using custom DNS server"
        }, 
        {
            "location": "/cb-credentials/index.html", 
            "text": "Managing Cloudbreak credentials\n\n\nYou can view and manage Cloudbreak credentials in the \nCredentials\n tab by clicking \nCreate credential\n and providing required parameters. You must create at least one credential in order to be able to create a cluster. \n\n\nCreate Cloudbreak credential\n\n\nFor steps, refer to:\n\n\n\n\nCreate Cloudbreak credential on AWS\n  \n\n\nCreate Cloudbreak credential on Azure\n  \n\n\nCreate Cloudbreak credential on GCP\n \n\n\nCreate Cloudbreak credential on OpenStack\n\n\n\n\nModify an existing credential\n\n\nYou can modify an existing Cloudbreak credential by following these steps.\n\n\n\n\nThe value of the \"Name\" parameter cannot be changed.  \n\nThe values of sensitive parameters will not be displayed and you will have to reenter them.    \n\n\n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, select \nCredentials\n from the navigation pane.\n\n2.Click on \n  next to the credential that you want to edit.  \n\n\nWhen done making changes, click \nSave\n to save your changes.  \n\n\n\n\nSet a default credential\n\n\nIf using multiple Cloudbreak credentials, you can select one credential and use it as default for creating clusters. This default credential will be pre-selected in the create cluster wizard.\n\n\nSteps\n\n\n\n\nIn the Cloudbreak UI, select \nCredentials\n from the navigation pane.  \n\n\nClick \nSet as default\n next to the credential that you would like to set as default.  \n\n\nClick \nYes\n to confirm.", 
            "title": "Manage Cloudbreak credentials"
        }, 
        {
            "location": "/cb-credentials/index.html#managing-cloudbreak-credentials", 
            "text": "You can view and manage Cloudbreak credentials in the  Credentials  tab by clicking  Create credential  and providing required parameters. You must create at least one credential in order to be able to create a cluster.", 
            "title": "Managing Cloudbreak credentials"
        }, 
        {
            "location": "/cb-credentials/index.html#create-cloudbreak-credential", 
            "text": "For steps, refer to:   Create Cloudbreak credential on AWS     Create Cloudbreak credential on Azure     Create Cloudbreak credential on GCP    Create Cloudbreak credential on OpenStack", 
            "title": "Create Cloudbreak credential"
        }, 
        {
            "location": "/cb-credentials/index.html#modify-an-existing-credential", 
            "text": "You can modify an existing Cloudbreak credential by following these steps.   The value of the \"Name\" parameter cannot be changed.   \nThe values of sensitive parameters will not be displayed and you will have to reenter them.       Steps   In the Cloudbreak UI, select  Credentials  from the navigation pane. \n2.Click on    next to the credential that you want to edit.    When done making changes, click  Save  to save your changes.", 
            "title": "Modify an existing credential"
        }, 
        {
            "location": "/cb-credentials/index.html#set-a-default-credential", 
            "text": "If using multiple Cloudbreak credentials, you can select one credential and use it as default for creating clusters. This default credential will be pre-selected in the create cluster wizard.  Steps   In the Cloudbreak UI, select  Credentials  from the navigation pane.    Click  Set as default  next to the credential that you would like to set as default.    Click  Yes  to confirm.", 
            "title": "Set a default credential"
        }, 
        {
            "location": "/cb-migrate/index.html", 
            "text": "Moving a Cloudbreak instance\n\n\nTo transfer a Cloudbreak instance from one host to another, perform these tasks:\n\n\n\n\nIf you are using the embedded PostgreSQL database, \nback up current Cloudbreak database\n data  \n\n\nLaunch a new Cloudbreak instance and start Cloudbreak. Refer to \nLaunch Cloudbreak\n   \n\n\nIf you are using the embedded PostgreSQL database, \npopulate the new Cloudbreak instance database with the dump from the original Cloudbreak instance\n on the new host.  \n\n\nModify Cloudbreak Profile\n  \n\n\n\n\nBack up Cloudbreak database\n\n\nTo create a backup of the embedded PostgreSQL database, perform these steps.\n\n\nSteps\n \n\n\n\n\n\n\nOn your Cloudbreak host machine, execute the following  command to enter the container of the database:\n\n\ndocker exec -it cbreak_commondb_1 bash\n \nIf it is not running, start the database container by using the \ndocker start cbreak_commondb_1\n command.\n\n\n\n\n\n\nCreate three database dumps (cbdb, uaadb, periscopedb):  \n\n\npg_dump -Fc -U postgres cbdb \n cbdb.dump\npg_dump -Fc -U postgres uaadb \n uaadb.dump\npg_dump -Fc -U postgres periscopedb \n periscopedb.dump\n\n\n\n\n\n\nQuit from the container with shortcut \nCTRL+d\n.\n\n\n\n\n\n\nSave the previously created dumps to the host instance:               \n\n\ndocker cp cbreak_commondb_1:/cbdb.dump ./cbdb.dump\ndocker cp cbreak_commondb_1:/uaadb.dump ./uaadb.dump\ndocker cp cbreak_commondb_1:/periscopedb.dump ./periscopedb.dump\n\n\n\n\n\n\nPopulate database with dump from original Cloudbreak instance\n\n\nPerform these steps to populate databases with information from the Cloudbreak server.\n\n\nSteps\n \n\n\n\n\n\n\nCopy the saved database files from \nBack up Cloudbreak database\n to the new Cloudbreak server host.\n\n\n\n\n\n\nCopy the dump files into the database container with the following commands. Modify the location as necessary (The example below assumes that the files are in \n/tmp\n):\n\n\ndocker cp /tmp/cbdb.dump cbreak_commondb_1:/cbdb.dump\ndocker cp /tmp/uaadb.dump cbreak_commondb_1:/uaadb.dump\ndocker cp /tmp/periscopedb.dump cbreak_commondb_1:/periscopedb.dump\n\n\n\n\n\n\nExecute the following command to stop the container:\n\n\ndocker stop cbreak_identity_1\n\n\n\n\n\n\nExecute the following command to enter the container of the database:\n\n\ndocker exec -it cbreak_commondb_1 bash\n\n\n\n\n\n\nExecute the following commands:\n\n\npsql -U postgres\ndrop database uaadb;\ndrop database cbdb;\ndrop database periscopedb;\ncreate database uaadb;\ncreate database cbdb;\ncreate database periscopedb;\n\n\n\n\nIf you get \nERROR:  database \"uaadb\" is being accessed by other users\n error, ensure that    cbreak_identity_1 container is not running and then retry dropping uaadb.  \n\n\n\n\n\n\n\n\nExit the PostgreSQL interactive terminal.\n    \n\\q\n \n\n\n\n\n\n\nRestore the databases from the original backups:\n\n\npg_restore -U postgres -d periscopedb periscopedb.dump\npg_restore -U postgres -d cbdb cbdb.dump\npg_restore -U postgres -d uaadb uaadb.dump\n\n\n\n\n\n\nQuit from the container with the shortcut \nCTRL+d\n.     \n\n\n\n\n\n\nModify Cloudbreak Profile\n\n\nPerform these steps to ensure that your new Profile file is correctly set up. \n\n\nSteps\n \n\n\n\n\n\n\nEnsure that the following parameter values match in the origin and target Profile files and modify Profile file of the target environment if necessary:\n\n\nexport UAA_DEFAULT_USER_EMAIL=admin@example.com\nexport UAA_DEFAULT_SECRET=cbsecret\nexport UAA_DEFAULT_USER_PW=cbuser\n\n\n\n\n\n\nRestart Cloudbreak application by using the \ncbd restart\n command.  \n\n\nAfter performing these steps the migration is complete. To verify, log in to the UI of your new Cloudbreak instance and make sure that it contains the information from your old instance.", 
            "title": "Move Cloudbreak instance"
        }, 
        {
            "location": "/cb-migrate/index.html#moving-a-cloudbreak-instance", 
            "text": "To transfer a Cloudbreak instance from one host to another, perform these tasks:   If you are using the embedded PostgreSQL database,  back up current Cloudbreak database  data    Launch a new Cloudbreak instance and start Cloudbreak. Refer to  Launch Cloudbreak      If you are using the embedded PostgreSQL database,  populate the new Cloudbreak instance database with the dump from the original Cloudbreak instance  on the new host.    Modify Cloudbreak Profile", 
            "title": "Moving a Cloudbreak instance"
        }, 
        {
            "location": "/cb-migrate/index.html#back-up-cloudbreak-database", 
            "text": "To create a backup of the embedded PostgreSQL database, perform these steps.  Steps      On your Cloudbreak host machine, execute the following  command to enter the container of the database:  docker exec -it cbreak_commondb_1 bash  \nIf it is not running, start the database container by using the  docker start cbreak_commondb_1  command.    Create three database dumps (cbdb, uaadb, periscopedb):    pg_dump -Fc -U postgres cbdb   cbdb.dump\npg_dump -Fc -U postgres uaadb   uaadb.dump\npg_dump -Fc -U postgres periscopedb   periscopedb.dump    Quit from the container with shortcut  CTRL+d .    Save the previously created dumps to the host instance:                 docker cp cbreak_commondb_1:/cbdb.dump ./cbdb.dump\ndocker cp cbreak_commondb_1:/uaadb.dump ./uaadb.dump\ndocker cp cbreak_commondb_1:/periscopedb.dump ./periscopedb.dump", 
            "title": "Back up Cloudbreak database"
        }, 
        {
            "location": "/cb-migrate/index.html#populate-database-with-dump-from-original-cloudbreak-instance", 
            "text": "Perform these steps to populate databases with information from the Cloudbreak server.  Steps      Copy the saved database files from  Back up Cloudbreak database  to the new Cloudbreak server host.    Copy the dump files into the database container with the following commands. Modify the location as necessary (The example below assumes that the files are in  /tmp ):  docker cp /tmp/cbdb.dump cbreak_commondb_1:/cbdb.dump\ndocker cp /tmp/uaadb.dump cbreak_commondb_1:/uaadb.dump\ndocker cp /tmp/periscopedb.dump cbreak_commondb_1:/periscopedb.dump    Execute the following command to stop the container:  docker stop cbreak_identity_1    Execute the following command to enter the container of the database:  docker exec -it cbreak_commondb_1 bash    Execute the following commands:  psql -U postgres\ndrop database uaadb;\ndrop database cbdb;\ndrop database periscopedb;\ncreate database uaadb;\ncreate database cbdb;\ncreate database periscopedb;   If you get  ERROR:  database \"uaadb\" is being accessed by other users  error, ensure that    cbreak_identity_1 container is not running and then retry dropping uaadb.       Exit the PostgreSQL interactive terminal.\n     \\q      Restore the databases from the original backups:  pg_restore -U postgres -d periscopedb periscopedb.dump\npg_restore -U postgres -d cbdb cbdb.dump\npg_restore -U postgres -d uaadb uaadb.dump    Quit from the container with the shortcut  CTRL+d .", 
            "title": "Populate database with dump from original Cloudbreak instance"
        }, 
        {
            "location": "/cb-migrate/index.html#modify-cloudbreak-profile", 
            "text": "Perform these steps to ensure that your new Profile file is correctly set up.   Steps      Ensure that the following parameter values match in the origin and target Profile files and modify Profile file of the target environment if necessary:  export UAA_DEFAULT_USER_EMAIL=admin@example.com\nexport UAA_DEFAULT_SECRET=cbsecret\nexport UAA_DEFAULT_USER_PW=cbuser    Restart Cloudbreak application by using the  cbd restart  command.    After performing these steps the migration is complete. To verify, log in to the UI of your new Cloudbreak instance and make sure that it contains the information from your old instance.", 
            "title": "Modify Cloudbreak Profile"
        }, 
        {
            "location": "/cb-ldap/index.html", 
            "text": "Configuring Cloudbreak for LDAP/AD authentication\n\n\nBy default Cloudbreak uses an internal system as the user store for authentication (enabled by using \nCloudFoundry UAA\n). If you would like to configure LDAP or Active Directory (AD) external authentication, you need to:  \n\n\n\n\nCollect the \nfollowing information\n about your LDAP/AD setup    \n\n\nConfigure Cloudbreak\n to work with that LDAP/AD setup\n\n\n\n\nLDAP/AD information\n\n\nThe following table details the properties and values that you need to know about your LDAP/AD environment on order to use the LDAP/AD with Cloudbreak: \n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nbase\n\n\n\n\n\n\n\n\n\n\nurl\n\n\nThe LDAP url with port\n\n\nldap://10.0.3.128:389/\n\n\n\n\n\n\nuserDn\n\n\nEnter the root Distinguished Name to search in the directory for users.\n\n\ncn=Administrator,ou=srv,dc=hortonworks,dc=local\n\n\n\n\n\n\npassword\n\n\nEnter your root Distinguished Name password.\n\n\nMyPassword1234!\n\n\n\n\n\n\nsearchBase\n\n\nEnter your LDAP user search base. This defines the location in the directory from which the LDAP search begins.\n\n\nou=Users,dc=hortonworks,dc=local\n\n\n\n\n\n\nsearchFilter\n\n\nEnter the attribute for which to conduct a search on the user base.\n\n\nmail={0}\n\n\n\n\n\n\ngroups\n\n\n\n\n\n\n\n\n\n\nsearchBase\n\n\nEnter your LDAP group search base. This defines the location in the directory from which the LDAP search begins.\n\n\nou=Groups,dc=hortonworks,dc=local\n\n\n\n\n\n\ngroupSearchFilter\n\n\nEnter the attribute for which to conduct a search on the group base.\n\n\nmember={0}\n\n\n\n\n\n\n\n\nConfiguring Cloudbreak for LDAP/AD\n\n\nThere are two parts to configuring Cloudbreak for LDAP/AD:\n\n\n\n\nConfiguring LDAP/AD user authentication for Cloudbreak  \n\n\nConfiguring LDAP/AD group authorization for Cloudbreak  \n\n\n\n\nConfigure user authentication\n\n\nConfigure LDAP/AD user authentication for Cloudbreak by using these steps. \n\n\nSteps\n \n\n\n\n\nOn the Cloudbreak host, browse to \n/var/lib/cloudbreak-deployment\n.  \n\n\nCreate a new yml file. By default the name of this file should be \nuaa-changes.yml\n, but optionally it can be customized by setting the following in Profile: \nexport UAA_SETTINGS_FILE=\nsome-file-name\n.yml\n where \nsome-file-name\n is the name that you would like to use for this yml file.  \n\n\nIn the yml file enter the following using your \nLDAP/AD information\n. Next, save the file and restart Cloudbreak.  \n\n\n\n\nspring_profiles: postgresql,ldap\n\nldap:\n  profile:\n    file: ldap/ldap-search-and-bind.xml\n  base:\n    url: ldap://10.0.3.138:389\n    userDn: cn=Administrator,ou=srv,dc=hortonworks,dc=local\n    password: \u2019mypassword\u2019\n    searchBase: ou=Users,dc=hortonworks,dc=local\n    searchFilter: mail={0}\n  groups:\n    file: ldap/ldap-groups-map-to-scopes.xml\n    searchBase: ou=Groups,dc=hortonworks,dc=local\n    searchSubtree: false\n    maxSearchDepth: 1\n    groupSearchFilter: member={0}\n    autoAdd: true\n\n\n\n\nConfigure group authorization\n\n\nOnce user authentication is configured, you need to configure which group(s) can access Cloudbreak. Users (once authenticated) will be granted permission to access Cloudbreak and use the capabilities of Cloudbreak based on their group member. The following describes how to create (i.e. execute-and-map) a group authorization and how to remove (i.e. delete-mapping) an authorization. \n\n\nTo create a group authorization, execute the following (for example: to add \u201cAnalysts\u201d group):\n\n\ncbd util execute-ldap-mapping cn=Analysts,ou=Groups,dc=hortonworks,dc=local\n\n\n\nTo remove a group authorization, execute the following (for example: to remove \u201cAnalysts\u201d group):\n\n\ncbd util delete-ldap-mapping cn=Analysts,ou=Groups,dc=hortonworks,dc=local", 
            "title": "Configure LDAP/AD authentication for Cloudbreak"
        }, 
        {
            "location": "/cb-ldap/index.html#configuring-cloudbreak-for-ldapad-authentication", 
            "text": "By default Cloudbreak uses an internal system as the user store for authentication (enabled by using  CloudFoundry UAA ). If you would like to configure LDAP or Active Directory (AD) external authentication, you need to:     Collect the  following information  about your LDAP/AD setup      Configure Cloudbreak  to work with that LDAP/AD setup", 
            "title": "Configuring Cloudbreak for LDAP/AD authentication"
        }, 
        {
            "location": "/cb-ldap/index.html#ldapad-information", 
            "text": "The following table details the properties and values that you need to know about your LDAP/AD environment on order to use the LDAP/AD with Cloudbreak:      Parameter  Description  Example      base      url  The LDAP url with port  ldap://10.0.3.128:389/    userDn  Enter the root Distinguished Name to search in the directory for users.  cn=Administrator,ou=srv,dc=hortonworks,dc=local    password  Enter your root Distinguished Name password.  MyPassword1234!    searchBase  Enter your LDAP user search base. This defines the location in the directory from which the LDAP search begins.  ou=Users,dc=hortonworks,dc=local    searchFilter  Enter the attribute for which to conduct a search on the user base.  mail={0}    groups      searchBase  Enter your LDAP group search base. This defines the location in the directory from which the LDAP search begins.  ou=Groups,dc=hortonworks,dc=local    groupSearchFilter  Enter the attribute for which to conduct a search on the group base.  member={0}", 
            "title": "LDAP/AD information"
        }, 
        {
            "location": "/cb-ldap/index.html#configuring-cloudbreak-for-ldapad", 
            "text": "There are two parts to configuring Cloudbreak for LDAP/AD:   Configuring LDAP/AD user authentication for Cloudbreak    Configuring LDAP/AD group authorization for Cloudbreak", 
            "title": "Configuring Cloudbreak for LDAP/AD"
        }, 
        {
            "location": "/cb-ldap/index.html#configure-user-authentication", 
            "text": "Configure LDAP/AD user authentication for Cloudbreak by using these steps.   Steps     On the Cloudbreak host, browse to  /var/lib/cloudbreak-deployment .    Create a new yml file. By default the name of this file should be  uaa-changes.yml , but optionally it can be customized by setting the following in Profile:  export UAA_SETTINGS_FILE= some-file-name .yml  where  some-file-name  is the name that you would like to use for this yml file.    In the yml file enter the following using your  LDAP/AD information . Next, save the file and restart Cloudbreak.     spring_profiles: postgresql,ldap\n\nldap:\n  profile:\n    file: ldap/ldap-search-and-bind.xml\n  base:\n    url: ldap://10.0.3.138:389\n    userDn: cn=Administrator,ou=srv,dc=hortonworks,dc=local\n    password: \u2019mypassword\u2019\n    searchBase: ou=Users,dc=hortonworks,dc=local\n    searchFilter: mail={0}\n  groups:\n    file: ldap/ldap-groups-map-to-scopes.xml\n    searchBase: ou=Groups,dc=hortonworks,dc=local\n    searchSubtree: false\n    maxSearchDepth: 1\n    groupSearchFilter: member={0}\n    autoAdd: true", 
            "title": "Configure user authentication"
        }, 
        {
            "location": "/cb-ldap/index.html#configure-group-authorization", 
            "text": "Once user authentication is configured, you need to configure which group(s) can access Cloudbreak. Users (once authenticated) will be granted permission to access Cloudbreak and use the capabilities of Cloudbreak based on their group member. The following describes how to create (i.e. execute-and-map) a group authorization and how to remove (i.e. delete-mapping) an authorization.   To create a group authorization, execute the following (for example: to add \u201cAnalysts\u201d group):  cbd util execute-ldap-mapping cn=Analysts,ou=Groups,dc=hortonworks,dc=local  To remove a group authorization, execute the following (for example: to remove \u201cAnalysts\u201d group):  cbd util delete-ldap-mapping cn=Analysts,ou=Groups,dc=hortonworks,dc=local", 
            "title": "Configure group authorization"
        }, 
        {
            "location": "/security-cb-ssl/index.html", 
            "text": "Add SSL certificate for Cloudbreak web UI\n\n\nBy default Cloudbreak has been configured with a self-signed certificate for access via HTTPS. This is sufficient for many deployments such as trials, development, testing, or staging. However, for production deployments, a trusted certificate is preferred and can be configured in the controller. Follow these steps to configure the cloud controller to use your own trusted certificate. \n\n\nPrerequisites\n\n\nTo use your own certificate, you must have:\n\n\n\n\nA resolvable fully qualified domain name (FQDN) for the controller host IP address. For example, this can be set up in \nAmazon Route 53\n.  \n\n\nA valid SSL certificate for this fully qualified domain name. The certificate can be obtained from a number of certificate providers.  \n\n\n\n\nSteps\n\n\n\n\n\n\nSSH to the Cloudbreak host instance:\n\n\nssh -i mykeypair.pem cloudbreak@[CONTROLLER-IP-ADDRESS]\n\n\n\n\n\n\nMake sure that the target fully qualified domain name (FQDN) which you plan to use for Cloudbreak is resolvable:\n\n\nnslookup [TARGET-CONTROLLER-FQDN]\n\n\nFor example:\n\n\nnslookup hdcloud.example.com\n\n\n\n\n\n\nBrowse to the Cloudbreak deployment directory and edit the \nProfile\n file:\n\n\nvi /var/lib/cloudbreak-deployment/Profile\n\n\n\n\n\n\nReplace the value of the \nPUBLIC_IP\n variable with the \nTARGET-CONTROLLER-FQDN\n value:\n\n\nPUBLIC_IP=[TARGET-CONTROLLER-FQDN]\n\n\n\n\n\n\nCopy your private key and certificate files for the FQDN onto the Cloudbreak host. These files must be placed under \n/var/lib/cloudbreak-deployment/certs/traefik/\n directory.\n\n\n\n\nFile permissions for the private key and certificate files can be set to 600.\n\n\n\n\n\n\n\n\n\n\nFile\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nPRIV-KEY-LOCATION\n\n\n/var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.key\n\n\n\n\n\n\nCERT-LOCATION\n\n\n/var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.crt\n\n\n\n\n\n\n\n\n\n\n\n\nConfigure TLS details in your \nProfile\n by adding the following line at the end of the file.\n\n\n\n\nNotice that \nCERT-LOCATION\n and \nPRIV-KEY-LOCATION\n are file locations from Step 5, starting at the \n/certs/...\n path.\n\n\n\n\nexport CBD_TRAEFIK_TLS=\u201d[CERT-LOCATION],[PRIV-KEY-LOCATION]\u201d\n\n\nFor example:\n\n\nexport CBD_TRAEFIK_TLS=\"/certs/traefik/hdcloud.example.com.crt,/certs/traefik/hdcloud.example.com.key\"\n\n\n\n\n\n\nRestart Cloudbreak deployer:\n\n\ncbd restart\n\n\n\n\n\n\nUsing your web browser, access the Cloudbreak UI using the new resolvable fully qualified domain name.\n\n\n\n\n\n\nConfirm that the connection is SSL-protected and that the certificate used is the certificate that you provided to Cloudbreak.", 
            "title": "Add SSL certificate for Cloudbreak UI"
        }, 
        {
            "location": "/security-cb-ssl/index.html#add-ssl-certificate-for-cloudbreak-web-ui", 
            "text": "By default Cloudbreak has been configured with a self-signed certificate for access via HTTPS. This is sufficient for many deployments such as trials, development, testing, or staging. However, for production deployments, a trusted certificate is preferred and can be configured in the controller. Follow these steps to configure the cloud controller to use your own trusted certificate.   Prerequisites  To use your own certificate, you must have:   A resolvable fully qualified domain name (FQDN) for the controller host IP address. For example, this can be set up in  Amazon Route 53 .    A valid SSL certificate for this fully qualified domain name. The certificate can be obtained from a number of certificate providers.     Steps    SSH to the Cloudbreak host instance:  ssh -i mykeypair.pem cloudbreak@[CONTROLLER-IP-ADDRESS]    Make sure that the target fully qualified domain name (FQDN) which you plan to use for Cloudbreak is resolvable:  nslookup [TARGET-CONTROLLER-FQDN]  For example:  nslookup hdcloud.example.com    Browse to the Cloudbreak deployment directory and edit the  Profile  file:  vi /var/lib/cloudbreak-deployment/Profile    Replace the value of the  PUBLIC_IP  variable with the  TARGET-CONTROLLER-FQDN  value:  PUBLIC_IP=[TARGET-CONTROLLER-FQDN]    Copy your private key and certificate files for the FQDN onto the Cloudbreak host. These files must be placed under  /var/lib/cloudbreak-deployment/certs/traefik/  directory.   File permissions for the private key and certificate files can be set to 600.      File  Example      PRIV-KEY-LOCATION  /var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.key    CERT-LOCATION  /var/lib/cloudbreak-deployment/certs/traefik/hdcloud.example.com.crt       Configure TLS details in your  Profile  by adding the following line at the end of the file.   Notice that  CERT-LOCATION  and  PRIV-KEY-LOCATION  are file locations from Step 5, starting at the  /certs/...  path.   export CBD_TRAEFIK_TLS=\u201d[CERT-LOCATION],[PRIV-KEY-LOCATION]\u201d  For example:  export CBD_TRAEFIK_TLS=\"/certs/traefik/hdcloud.example.com.crt,/certs/traefik/hdcloud.example.com.key\"    Restart Cloudbreak deployer:  cbd restart    Using your web browser, access the Cloudbreak UI using the new resolvable fully qualified domain name.    Confirm that the connection is SSL-protected and that the certificate used is the certificate that you provided to Cloudbreak.", 
            "title": "Add SSL certificate for Cloudbreak web UI"
        }, 
        {
            "location": "/cb-proxy/index.html", 
            "text": "Configuring outbound internet access and proxy\n\n\nDepending on your enterprise requirements, you may have limited or restricted outbound network access and/or require the use of an internet proxy. Installing and configuring Cloudbreak, as well as creating cloud resources and clusters on those resources requires outbound network access to certain destinations, and in some cases must go through a proxy.\n\n\nThis section provides information on the outbound network destinations for Cloudbreak, and instructions on how to configure Cloudbreak to use a proxy for outbound access (if required).\n\n\n\n\n\n\n\n\nScenario\n\n\nDocumentation\n\n\n\n\n\n\n\n\n\n\nMy environment has limited outbound internet access\n\n\nRefer to \nOutbound network access destinations\n for information on network rules.\n\n\n\n\n\n\nMy environment requires use of a proxy for outbound internet access\n\n\nRefer to \nUsing a proxy\n for information on using a proxy with Cloudbreak.\n\n\n\n\n\n\n\n\nOutbound network access destinations\n\n\nTo install and configure Cloudbreak, you will need the following outbound destinations available:\n\n\n\n\n\n    \nDestination\n\n    \nDescription\n \n  \n\n  \n\n    \n*.docker.io\n\n    \nObtain the Docker images for Cloudbreak.\n \n  \n\n  \n\n    \nraw.githubusercontent.com\ngithub.com\ns3.amazonaws.com\n*.cloudfront.net\n\n    \nObtain Cloudbreak dependencies.\n \n  \n\n  \n\n  \ncloudbreak-imagecatalog.s3.amazonaws.com \n The default Cloudbreak image catalog used for VMs. Refer to \nCustom images\n for more information on image catalogs. \n\n  \n\n\n\n\n\nOnce Cloudbreak is installed and configured, you will need the following outbound destinations available in order to communicate with the cloud provider APIs to obtain cloud resources for clusters.\n\n\n\n\n\n    \nCloud provider\n\n    \nCloud provider API destinations\n \n  \n\n  \n\n  \nAmazon Web Services\n\n  \n*.amazonaws.com\n\n \n\n  \n Microsoft Azure \n\n  \n \n*.microsoftonline.com\n*.windows.net\n*.azure.com\n\n   \n\n   \n\n  \n Google Cloud Platform  \n  \n  \n \naccounts.google.com\n*.googleapis.com\n\n  \n\n\n\n\n\nTo install the cluster software, you can: \n\n\na) use the public hosted repositories provided by Hortonworks, or\n\nb) specify your own local hosted repositories when you create a cluster. \n\n\nIf you choose to (a) use the public hosted repositories, be sure to allow outbound access to the following destinations:\n\n\n\n\nprivate-repo-1.hortonworks.com  \n\n\npublic-repo-1.hortonworks.com  \n\n\n\n\nUsing a proxy\n\n\nIn some cases, your environment requires all internet traffic to go through an internet proxy. This section describes the following:\n\n\n\n\nHow to \nset up Cloudbreak to use a proxy\n  \n\n\nHow to \nconfigure your cluster hosts to use a proxy\n  \n\n\n\n\nSet up Cloudbreak to use a proxy\n\n\nUse these steps if you would like to set up Cloudbreak to use your proxy. \n\n\nSteps\n\n\n\n\n\n\nAfter downloading and installing Cloudbreak, configure the Docker daemon to use proxy by adding the following to the Docker service file:\n\n\nEnvironment=\"HTTP_PROXY=http://my-proxy-host:my-proxy-port\" \"NO_PROXY=localhost,127.0.0.1\"\n   \n\n\nFor example:\n\n\nvi /etc/systemd/system/docker.service -\n Environment=\"HTTP_PROXY=http://10.0.2.237:3128\" \"NO_PROXY=localhost,127.0.0.1\"\n\n\nFor more information refer to\n\nDocker docs\n.     \n\n\n\n\n\n\nEnsure that ports 9443 and 8443 are handled as SSL connections in the proxy config.   \n\n\n\n\n\n\nConfigure proxy settings in the Profile file by setting the following variables:  \n\n\n\n\n\n\nHTTP_PROXY_HOST=your-proxy-host\nHTTPS_PROXY_HOST=your-proxy-host\nPROXY_PORT=your-proxy-port\nPROXY_USER=your-proxy-user\nPROXY_PASSWORD=your-proxy-password\n#NON_PROXY_HOSTS\n#HTTPS_PROXYFORCLUSTERCONNECTION=false\n\n\n\nFor example:\n\n\nHTTP_PROXY_HOST=10.0.2.237\nHTTPS_PROXY_HOST=10.0.2.237\nPROXY_PORT=3128\nPROXY_USER=squid\nPROXY_PASSWORD=squid\n#NON_PROXY_HOSTS\n#HTTPS_PROXYFORCLUSTERCONNECTION=false\n\n\n\nSet up clusters to use a proxy\n\n\nUse the following guidelines to find out what steps to perform in order to set up your clusters to use a proxy:  \n\n\n\n\n\n\n\n\nWhat base image are you using?\n\n\nWhere are the platform repositories?\n\n\nWhat to do\n\n\n\n\n\n\n\n\n\n\nDefault\n\n\nPublic\n\n\nUse \nRegister a Proxy\n\n\n\n\n\n\nDefault\n\n\nLocal\n\n\nUse \nRegister a Proxy\n\n\n\n\n\n\nCustom\n\n\nPublic\n\n\nSet up the proxy on your custom image OR use \nRegister a Proxy\n.\n\n\n\n\n\n\nCustom\n\n\nLocal\n\n\nNot required. Skip this section.\n\n\n\n\n\n\n\n\nYou can define a proxy configuration as an external source in Cloudbreak web UI or CLI, and then (optionally) specify to configure that proxy configuration on the hosts that are part of the cluster during cluster create. Refer to \nRegister a Proxy\n for more information.  \n\n\nAdvanced proxy setup scenarios\n\n\nIn some cases, Cloudbreak using the proxy might vary depending on your Cloudbreak -\n cluster deployment. This section describes two scenarios:\n\n\n\n\nScenario 1\n: Cloudbreak needs to go through a proxy to access the Cloud provider APIs (and other public internet resources) but can talk to the cluster hosts directly.  \n\n\nScenario 2\n: Cloudbreak needs to go through a proxy to access the Cloud provider APIs (and other public internet resources) and the cluster hosts.  \n\n\n\n\nScenario 1\n\n\nIn this scenario, Cloudbreak can resolve and communicate with the Ambari Server in the cluster hosts directly. For example, this can be a scenario where Cloudbreak is deployed in the same VPC/VNet as the clusters and will not go through the proxy. However, Cloudbreak will communicate to the public Cloud Provider APIs via the proxy.\n\n\nTo configure this scenario, set this setting in your Profile file:\n\n\nHTTPS_PROXYFORCLUSTERCONNECTION = false\n\n\n\n \n\n\nScenario 2\n\n\nIn this scenario, Cloudbreak will connect to the Ambari Server through the configured proxy. For example, this can be a scenario where Cloudbreak is deployed to a different VPC/VNet than the cluster and must go through a proxy. Communication to the public cloud provider APIs also is via the proxy.\n\n\nTo configure this scenario, set this setting in your Profile file:\n\n\nHTTPS_PROXYFORCLUSTERCONNECTION = true", 
            "title": "Configure outbound internet access and proxy"
        }, 
        {
            "location": "/cb-proxy/index.html#configuring-outbound-internet-access-and-proxy", 
            "text": "Depending on your enterprise requirements, you may have limited or restricted outbound network access and/or require the use of an internet proxy. Installing and configuring Cloudbreak, as well as creating cloud resources and clusters on those resources requires outbound network access to certain destinations, and in some cases must go through a proxy.  This section provides information on the outbound network destinations for Cloudbreak, and instructions on how to configure Cloudbreak to use a proxy for outbound access (if required).     Scenario  Documentation      My environment has limited outbound internet access  Refer to  Outbound network access destinations  for information on network rules.    My environment requires use of a proxy for outbound internet access  Refer to  Using a proxy  for information on using a proxy with Cloudbreak.", 
            "title": "Configuring outbound internet access and proxy"
        }, 
        {
            "location": "/cb-proxy/index.html#outbound-network-access-destinations", 
            "text": "To install and configure Cloudbreak, you will need the following outbound destinations available:   \n     Destination \n     Description  \n   \n   \n     *.docker.io \n     Obtain the Docker images for Cloudbreak.  \n   \n   \n     raw.githubusercontent.com github.com s3.amazonaws.com *.cloudfront.net \n     Obtain Cloudbreak dependencies.  \n   \n   \n   cloudbreak-imagecatalog.s3.amazonaws.com   The default Cloudbreak image catalog used for VMs. Refer to  Custom images  for more information on image catalogs.  \n     Once Cloudbreak is installed and configured, you will need the following outbound destinations available in order to communicate with the cloud provider APIs to obtain cloud resources for clusters.   \n     Cloud provider \n     Cloud provider API destinations  \n   \n   \n   Amazon Web Services \n   *.amazonaws.com \n  \n    Microsoft Azure  \n     *.microsoftonline.com *.windows.net *.azure.com \n    \n    \n    Google Cloud Platform     \n     accounts.google.com *.googleapis.com \n     To install the cluster software, you can:   a) use the public hosted repositories provided by Hortonworks, or \nb) specify your own local hosted repositories when you create a cluster.   If you choose to (a) use the public hosted repositories, be sure to allow outbound access to the following destinations:   private-repo-1.hortonworks.com    public-repo-1.hortonworks.com", 
            "title": "Outbound network access destinations"
        }, 
        {
            "location": "/cb-proxy/index.html#using-a-proxy", 
            "text": "In some cases, your environment requires all internet traffic to go through an internet proxy. This section describes the following:   How to  set up Cloudbreak to use a proxy     How to  configure your cluster hosts to use a proxy", 
            "title": "Using a proxy"
        }, 
        {
            "location": "/cb-proxy/index.html#set-up-cloudbreak-to-use-a-proxy", 
            "text": "Use these steps if you would like to set up Cloudbreak to use your proxy.   Steps    After downloading and installing Cloudbreak, configure the Docker daemon to use proxy by adding the following to the Docker service file:  Environment=\"HTTP_PROXY=http://my-proxy-host:my-proxy-port\" \"NO_PROXY=localhost,127.0.0.1\"      For example:  vi /etc/systemd/system/docker.service -  Environment=\"HTTP_PROXY=http://10.0.2.237:3128\" \"NO_PROXY=localhost,127.0.0.1\"  For more information refer to Docker docs .         Ensure that ports 9443 and 8443 are handled as SSL connections in the proxy config.       Configure proxy settings in the Profile file by setting the following variables:      HTTP_PROXY_HOST=your-proxy-host\nHTTPS_PROXY_HOST=your-proxy-host\nPROXY_PORT=your-proxy-port\nPROXY_USER=your-proxy-user\nPROXY_PASSWORD=your-proxy-password\n#NON_PROXY_HOSTS\n#HTTPS_PROXYFORCLUSTERCONNECTION=false  For example:  HTTP_PROXY_HOST=10.0.2.237\nHTTPS_PROXY_HOST=10.0.2.237\nPROXY_PORT=3128\nPROXY_USER=squid\nPROXY_PASSWORD=squid\n#NON_PROXY_HOSTS\n#HTTPS_PROXYFORCLUSTERCONNECTION=false", 
            "title": "Set up Cloudbreak to use a proxy"
        }, 
        {
            "location": "/cb-proxy/index.html#set-up-clusters-to-use-a-proxy", 
            "text": "Use the following guidelines to find out what steps to perform in order to set up your clusters to use a proxy:       What base image are you using?  Where are the platform repositories?  What to do      Default  Public  Use  Register a Proxy    Default  Local  Use  Register a Proxy    Custom  Public  Set up the proxy on your custom image OR use  Register a Proxy .    Custom  Local  Not required. Skip this section.     You can define a proxy configuration as an external source in Cloudbreak web UI or CLI, and then (optionally) specify to configure that proxy configuration on the hosts that are part of the cluster during cluster create. Refer to  Register a Proxy  for more information.", 
            "title": "Set up clusters to use a proxy"
        }, 
        {
            "location": "/cb-proxy/index.html#advanced-proxy-setup-scenarios", 
            "text": "In some cases, Cloudbreak using the proxy might vary depending on your Cloudbreak -  cluster deployment. This section describes two scenarios:   Scenario 1 : Cloudbreak needs to go through a proxy to access the Cloud provider APIs (and other public internet resources) but can talk to the cluster hosts directly.    Scenario 2 : Cloudbreak needs to go through a proxy to access the Cloud provider APIs (and other public internet resources) and the cluster hosts.     Scenario 1  In this scenario, Cloudbreak can resolve and communicate with the Ambari Server in the cluster hosts directly. For example, this can be a scenario where Cloudbreak is deployed in the same VPC/VNet as the clusters and will not go through the proxy. However, Cloudbreak will communicate to the public Cloud Provider APIs via the proxy.  To configure this scenario, set this setting in your Profile file:  HTTPS_PROXYFORCLUSTERCONNECTION = false     Scenario 2  In this scenario, Cloudbreak will connect to the Ambari Server through the configured proxy. For example, this can be a scenario where Cloudbreak is deployed to a different VPC/VNet than the cluster and must go through a proxy. Communication to the public cloud provider APIs also is via the proxy.  To configure this scenario, set this setting in your Profile file:  HTTPS_PROXYFORCLUSTERCONNECTION = true", 
            "title": "Advanced proxy setup scenarios"
        }, 
        {
            "location": "/security-cb-inbound/index.html", 
            "text": "Restrict inbound access to clusters\n\n\nWe recommend that after launching Cloudbreak you set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file.\nWhen you launch a cluster, and Cloudbreak proposes security groups, this CIDR will be used\nfor the Cloudbreak to the Cluster master node (i.e. the host with Ambari Server) with this IP. This limits access\nfrom Cloudbreak to this cluster name for ports 9443 and 22 for Cloudbreak communication and management of the cluster.\n\n\nSteps\n \n\n\n\n\n\n\nSet CB_DEFAULT_GATEWAY_CIDR to the CIDR address range which is used by Cloudbreak to communicate with the cluster:\n\n\nexport CB_DEFAULT_GATEWAY_CIDR=14.15.16.17/32\n\n\nOr, if your Cloudbreak communicates with the cluster through multiple addresses, set multiple addresses separated with a comma:\n\n\nexport CB_DEFAULT_GATEWAY_CIDR=14.15.16.17/32,18.17.16.15/32\n\n\n\n\n\n\nIf Cloudbreak has already been started, restart it using \ncbd restart\n.     \n\n\n\n\n\n\nWhen CB_DEFAULT_GATEWAY_CIDR is set, two additional rules are added to your Ambari node security group: (1) port 9443 open to your Cloudbreak IP, and (2) port 22 open to your Cloudbreak IP. You can view and edit these default rules in the create cluster wizard.", 
            "title": "Restrict inbound access to clusters"
        }, 
        {
            "location": "/security-cb-inbound/index.html#restrict-inbound-access-to-clusters", 
            "text": "We recommend that after launching Cloudbreak you set CB_DEFAULT_GATEWAY_CIDR in your Cloudbreak's Profile file.\nWhen you launch a cluster, and Cloudbreak proposes security groups, this CIDR will be used\nfor the Cloudbreak to the Cluster master node (i.e. the host with Ambari Server) with this IP. This limits access\nfrom Cloudbreak to this cluster name for ports 9443 and 22 for Cloudbreak communication and management of the cluster.  Steps      Set CB_DEFAULT_GATEWAY_CIDR to the CIDR address range which is used by Cloudbreak to communicate with the cluster:  export CB_DEFAULT_GATEWAY_CIDR=14.15.16.17/32  Or, if your Cloudbreak communicates with the cluster through multiple addresses, set multiple addresses separated with a comma:  export CB_DEFAULT_GATEWAY_CIDR=14.15.16.17/32,18.17.16.15/32    If Cloudbreak has already been started, restart it using  cbd restart .         When CB_DEFAULT_GATEWAY_CIDR is set, two additional rules are added to your Ambari node security group: (1) port 9443 open to your Cloudbreak IP, and (2) port 22 open to your Cloudbreak IP. You can view and edit these default rules in the create cluster wizard.", 
            "title": "Restrict inbound access to clusters"
        }, 
        {
            "location": "/security-cb-customdom/index.html", 
            "text": "Configure sccess from custom domains\n\n\nCloudbreak deployer, which uses UAA as an identity provider, supports multitenancy. In UAA, multitenancy is managed through identity zones. An identity zone is accessed through a unique subdomain. For example, if the standard UAA responds to \nhttps://uaa.10.244.0.34.xip.io\n, a zone on this UAA can be accessed through a unique subdomain \nhttps://testzone1.uaa.10.244.0.34.xip.io\n.\n\n\nIf you want to use a custom domain for your identity or deployment, add the \nUAA_ZONE_DOMAIN\n line to your \nProfile\n:\n\n\nexport UAA_ZONE_DOMAIN=my-subdomain.example.com\n\n\n\nThis variable is necessary for UAA to identify which zone provider should handle the requests that arrive to that domain.", 
            "title": "Configure access from custom domains"
        }, 
        {
            "location": "/security-cb-customdom/index.html#configure-sccess-from-custom-domains", 
            "text": "Cloudbreak deployer, which uses UAA as an identity provider, supports multitenancy. In UAA, multitenancy is managed through identity zones. An identity zone is accessed through a unique subdomain. For example, if the standard UAA responds to  https://uaa.10.244.0.34.xip.io , a zone on this UAA can be accessed through a unique subdomain  https://testzone1.uaa.10.244.0.34.xip.io .  If you want to use a custom domain for your identity or deployment, add the  UAA_ZONE_DOMAIN  line to your  Profile :  export UAA_ZONE_DOMAIN=my-subdomain.example.com  This variable is necessary for UAA to identify which zone provider should handle the requests that arrive to that domain.", 
            "title": "Configure sccess from custom domains"
        }, 
        {
            "location": "/cb-db/index.html", 
            "text": "Configuring external Cloudbreak database\n\n\nBy default, Cloudbreak uses an embedded PostgreSQL database to persist data related to Cloudbreak configuration, setup, and so on. For a production Cloudbreak deployment, you must configure an external database. \n\n\nSupported databases\n\n\nAn embedded PostgreSQL 9.6.1 database is used by Cloudbreak by default. If you would like to\nuse an external database for Cloudbreak, you may use the following supported database types and versions: \n\n\n\n\n\n\n\n\nDatabase type\n\n\nSupported version\n\n\n\n\n\n\n\n\n\n\nExternal PostgreSQL\n\n\n9.6.1 or above\n\n\n\n\n\n\nExternal MySQL\n\n\nNot supported\n\n\n\n\n\n\nExternal MariaDB\n\n\nNot supported\n\n\n\n\n\n\nExternal Oracle\n\n\nNot supported\n\n\n\n\n\n\nExternal SQL Server\n\n\nNot supported\n\n\n\n\n\n\n\n\nConfigure external Cloudbreak database\n\n\nThe following section describes how to use Cloudbreak with an existing external database, other than\nthe embedded PostgreSQL database instance that Cloudbreak uses by default. To configure an external PostgreSQL database for Cloudbreak, perform these steps. \n\n\nSteps\n\n\n\n\n\n\nOn your Cloudbreak host machine, set the following environment variables according to the settings of your external database: \n\n\nexport DATABASE_HOST=my.database.host\nexport DATABASE_PORT=5432\nexport DATABASE_USERNAME=admin\nexport DATABASE_PASSWORD=Admin123!\n\n\n\n\n\n\n\nOn your external database, create three databases: \ncbdb, uaadb, periscopedb\n. You can create these databases using the \ncreatedb\n utility with the following commands:\n\n\ncreatedb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME cbdb\ncreatedb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME uaadb\ncreatedb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME periscopedb\n\n\nFor more information refer to the \nPostgreSQL documentation\n. \n\nAlternatively, you can log in to the management interface of your external database and execute \ncreate database\n commands directly. \n\n\n\n\n\n\nSet the following variables in your Cloudbreak Profile file. Modify the database parameters according to your external database.\n\n\nexport DATABASE_HOST=my.database.host\nexport DATABASE_PORT=5432\nexport DATABASE_USERNAME=admin\nexport DATABASE_PASSWORD=Admin123!\n\n\nexport CB_DB_PORT_5432_TCP_ADDR=$DATABASE_HOST\nexport CB_DB_PORT_5432_TCP_PORT=$DATABASE_PORT\nexport CB_DB_ENV_USER=$DATABASE_USERNAME\nexport CB_DB_ENV_PASS=$DATABASE_PASSWORD\nexport CB_DB_ENV_DB=cbdb\n\n\nexport PERISCOPE_DB_TCP_ADDR=$DATABASE_HOST\nexport PERISCOPE_DB_TCP_PORT=$DATABASE_PORT\nexport PERISCOPE_DB_USER=$DATABASE_USERNAME\nexport PERISCOPE_DB_PASS=$DATABASE_PASSWORD\nexport PERISCOPE_DB_NAME=periscopedb\nexport PERISCOPE_DB_SCHEMA_NAME=public\n\n\nexport IDENTITY_DB_URL=$DATABASE_HOST:$DATABASE_PORT\nexport IDENTITY_DB_USER=$DATABASE_USERNAME\nexport IDENTITY_DB_PASS=$DATABASE_PASSWORD\nexport IDENTITY_DB_NAME=uaadb\n\n\n\n\n\n\nRestart Cloudbreak application by using the \ncbd restart\n command. \n\n\n\n\n\n\nAfter performing these steps, your external database will be used for Cloudbreak instead of the built-in database. \n\n\n      \n\nData migration\n  \n\n If you want to migrate your existing data (such as  blueprints, recipes, and so on) from the embedded database to the external one, then after completing these steps, you should also  \ncreate a backup\n of your original database and then \nrestore\n it in the external database.", 
            "title": "Configure external Cloudbreak database"
        }, 
        {
            "location": "/cb-db/index.html#configuring-external-cloudbreak-database", 
            "text": "By default, Cloudbreak uses an embedded PostgreSQL database to persist data related to Cloudbreak configuration, setup, and so on. For a production Cloudbreak deployment, you must configure an external database.", 
            "title": "Configuring external Cloudbreak database"
        }, 
        {
            "location": "/cb-db/index.html#supported-databases", 
            "text": "An embedded PostgreSQL 9.6.1 database is used by Cloudbreak by default. If you would like to\nuse an external database for Cloudbreak, you may use the following supported database types and versions:      Database type  Supported version      External PostgreSQL  9.6.1 or above    External MySQL  Not supported    External MariaDB  Not supported    External Oracle  Not supported    External SQL Server  Not supported", 
            "title": "Supported databases"
        }, 
        {
            "location": "/cb-db/index.html#configure-external-cloudbreak-database", 
            "text": "The following section describes how to use Cloudbreak with an existing external database, other than\nthe embedded PostgreSQL database instance that Cloudbreak uses by default. To configure an external PostgreSQL database for Cloudbreak, perform these steps.   Steps    On your Cloudbreak host machine, set the following environment variables according to the settings of your external database:   export DATABASE_HOST=my.database.host\nexport DATABASE_PORT=5432\nexport DATABASE_USERNAME=admin\nexport DATABASE_PASSWORD=Admin123!    On your external database, create three databases:  cbdb, uaadb, periscopedb . You can create these databases using the  createdb  utility with the following commands:  createdb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME cbdb\ncreatedb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME uaadb\ncreatedb -h $DATABASE_HOST -p $DATABASE_PORT -U $DATABASE_USERNAME periscopedb  For more information refer to the  PostgreSQL documentation .  \nAlternatively, you can log in to the management interface of your external database and execute  create database  commands directly.     Set the following variables in your Cloudbreak Profile file. Modify the database parameters according to your external database.  export DATABASE_HOST=my.database.host\nexport DATABASE_PORT=5432\nexport DATABASE_USERNAME=admin\nexport DATABASE_PASSWORD=Admin123!  export CB_DB_PORT_5432_TCP_ADDR=$DATABASE_HOST\nexport CB_DB_PORT_5432_TCP_PORT=$DATABASE_PORT\nexport CB_DB_ENV_USER=$DATABASE_USERNAME\nexport CB_DB_ENV_PASS=$DATABASE_PASSWORD\nexport CB_DB_ENV_DB=cbdb  export PERISCOPE_DB_TCP_ADDR=$DATABASE_HOST\nexport PERISCOPE_DB_TCP_PORT=$DATABASE_PORT\nexport PERISCOPE_DB_USER=$DATABASE_USERNAME\nexport PERISCOPE_DB_PASS=$DATABASE_PASSWORD\nexport PERISCOPE_DB_NAME=periscopedb\nexport PERISCOPE_DB_SCHEMA_NAME=public  export IDENTITY_DB_URL=$DATABASE_HOST:$DATABASE_PORT\nexport IDENTITY_DB_USER=$DATABASE_USERNAME\nexport IDENTITY_DB_PASS=$DATABASE_PASSWORD\nexport IDENTITY_DB_NAME=uaadb    Restart Cloudbreak application by using the  cbd restart  command.     After performing these steps, your external database will be used for Cloudbreak instead of the built-in database.          Data migration     If you want to migrate your existing data (such as  blueprints, recipes, and so on) from the embedded database to the external one, then after completing these steps, you should also   create a backup  of your original database and then  restore  it in the external database.", 
            "title": "Configure external Cloudbreak database"
        }, 
        {
            "location": "/cb-disable-provider/index.html", 
            "text": "Disable providers\n\n\nIf you are planning to use Cloudbreak with a specific cloud provider or a specific set of cloud providers, you may want to disable the remaining providers. For example, if you are planning to use Cloudbreak with Azure only, you may want to disable AWS, Google Cloud, and OpenStack. \n\n\nSteps\n\n\n\n\n\n\nNavigate to the Cloudbreak deployment directory and edit Profile. For example:\n\n\ncd /var/lib/cloudbreak-deployment/\nvi Profile\n\n\n\n\n\n\nAdd the following entry, setting it to the provider that you  would like to see. For example, if you would like to see Azure only, set this to \"AZURE\":\n\n\nexport CB_ENABLEDPLATFORMS=AZURE\n\n\nAccepted values are:\n\n\n\n\nAZURE\n\n\nAWS\n\n\nGCP\n\n\nOPENSTACK\n\n\n\n\nAny combination of platforms can be used; for example if you would like to see AWS and OpenStack, then use:\n\n\nexport CB_ENABLEDPLATFORMS=AWS,OPENSTACK\n\n\nIf you want to reverse the change and see all providers, then either delete CB_ENABLEDPLATFORMS from the Profile or add the following: \n\n\nexport CB_ENABLEDPLATFORMS=AZURE,AWS,GCP,OPENSTACK\n\n\n\n\n\n\nRestart Cloudbreak by using \ncbd restart\n.", 
            "title": "Disable providers"
        }, 
        {
            "location": "/cb-disable-provider/index.html#disable-providers", 
            "text": "If you are planning to use Cloudbreak with a specific cloud provider or a specific set of cloud providers, you may want to disable the remaining providers. For example, if you are planning to use Cloudbreak with Azure only, you may want to disable AWS, Google Cloud, and OpenStack.   Steps    Navigate to the Cloudbreak deployment directory and edit Profile. For example:  cd /var/lib/cloudbreak-deployment/\nvi Profile    Add the following entry, setting it to the provider that you  would like to see. For example, if you would like to see Azure only, set this to \"AZURE\":  export CB_ENABLEDPLATFORMS=AZURE  Accepted values are:   AZURE  AWS  GCP  OPENSTACK   Any combination of platforms can be used; for example if you would like to see AWS and OpenStack, then use:  export CB_ENABLEDPLATFORMS=AWS,OPENSTACK  If you want to reverse the change and see all providers, then either delete CB_ENABLEDPLATFORMS from the Profile or add the following:   export CB_ENABLEDPLATFORMS=AZURE,AWS,GCP,OPENSTACK    Restart Cloudbreak by using  cbd restart .", 
            "title": "Disable providers"
        }, 
        {
            "location": "/cb-ports/index.html", 
            "text": "Change default Cloudbreak ports\n\n\nBy default, Cloudbreak uses ports 80 (HTTP) and 443 (HTTPS) to access the Cloudbreak server (for the web UI and for the CLI). To change these port numbers, you must edit the Profile file on your Cloudbreak host. \n\n\nCloudbreak should not be running when you change the port numbers. Edit Profile either before you start Cloudbreak the first time or stop Cloudbreak before editing the file.\n\n\nSteps\n  \n\n\n\n\n\n\nNavigate to the Cloudbreak deployment directory (typically \n/var/lib/cloudbreak-deployment\n) and open the Profile file with a text editor. \n\n\n\n\n\n\nAdd one or both of the following parameters, setting them to the port numbers that you want to use:\n\n\nexport PUBLIC_HTTP_PORT=111\nexport PUBLIC_HTTPS_PORT=222\n\n\n\n\n\n\nStart or restart Cloudbreak by using \ncbd start\n or \ncbd restart\n. \n\n\n\n\n\n\nThis change affects Cloudbreak CLI configuration. When \nconfiguring the CLI\n, you must provide these ports as part of the server URL. For example: \n\n\ncb configure --server http://cb.server.address:111 --username  test@hortonworks.com\ncb configure --server https://cb.server.address:222 --username  test@hortonworks.com", 
            "title": "Change default Cloudbreak ports"
        }, 
        {
            "location": "/cb-ports/index.html#change-default-cloudbreak-ports", 
            "text": "By default, Cloudbreak uses ports 80 (HTTP) and 443 (HTTPS) to access the Cloudbreak server (for the web UI and for the CLI). To change these port numbers, you must edit the Profile file on your Cloudbreak host.   Cloudbreak should not be running when you change the port numbers. Edit Profile either before you start Cloudbreak the first time or stop Cloudbreak before editing the file.  Steps       Navigate to the Cloudbreak deployment directory (typically  /var/lib/cloudbreak-deployment ) and open the Profile file with a text editor.     Add one or both of the following parameters, setting them to the port numbers that you want to use:  export PUBLIC_HTTP_PORT=111\nexport PUBLIC_HTTPS_PORT=222    Start or restart Cloudbreak by using  cbd start  or  cbd restart .     This change affects Cloudbreak CLI configuration. When  configuring the CLI , you must provide these ports as part of the server URL. For example:   cb configure --server http://cb.server.address:111 --username  test@hortonworks.com\ncb configure --server https://cb.server.address:222 --username  test@hortonworks.com", 
            "title": "Change default Cloudbreak ports"
        }, 
        {
            "location": "/cb-email/index.html", 
            "text": "Configure SMTP email notifications\n\n\nIf you want to configure email notification, configure SMTP parameters in your \nProfile\n. \n\n\n\n\nIn order to use this configuration, your email server must use SMTP. \n\n\n\n\nThe default values of the SMTP parameters are:\n\n\nexport CLOUDBREAK_SMTP_SENDER_USERNAME=\nexport CLOUDBREAK_SMTP_SENDER_PASSWORD=\nexport CLOUDBREAK_SMTP_SENDER_HOST=\nexport CLOUDBREAK_SMTP_SENDER_PORT=25\nexport CLOUDBREAK_SMTP_SENDER_FROM=\nexport CLOUDBREAK_SMTP_AUTH=true\nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true\nexport CLOUDBREAK_SMTP_TYPE=smtp\n\n\n\n\nFor example: \n\n\nexport CLOUDBREAK_SMTP_SENDER_USERNAME='myemail@gmail.com'  \nexport CLOUDBREAK_SMTP_SENDER_PASSWORD='Mypassword123'  \nexport CLOUDBREAK_SMTP_SENDER_HOST='smtp.gmail.com'  \nexport CLOUDBREAK_SMTP_SENDER_PORT=25  \nexport CLOUDBREAK_SMTP_SENDER_FROM='myemail@gmail.com'  \nexport CLOUDBREAK_SMTP_AUTH=true  \nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true  \nexport CLOUDBREAK_SMTP_TYPE=smtp  \n\n\n\n\n\n\nThe example assumes that you are using gmail. You should use the settings appropriate for your SMTP server.\n\n\n\n\nIf your SMTP server uses SMTPS, you must set the protocol in your \nProfile\n to smtps:\n\n\nexport CLOUDBREAK_SMTP_TYPE=smtps", 
            "title": "Set up email notifications"
        }, 
        {
            "location": "/cb-email/index.html#configure-smtp-email-notifications", 
            "text": "If you want to configure email notification, configure SMTP parameters in your  Profile .    In order to use this configuration, your email server must use SMTP.    The default values of the SMTP parameters are:  export CLOUDBREAK_SMTP_SENDER_USERNAME=\nexport CLOUDBREAK_SMTP_SENDER_PASSWORD=\nexport CLOUDBREAK_SMTP_SENDER_HOST=\nexport CLOUDBREAK_SMTP_SENDER_PORT=25\nexport CLOUDBREAK_SMTP_SENDER_FROM=\nexport CLOUDBREAK_SMTP_AUTH=true\nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true\nexport CLOUDBREAK_SMTP_TYPE=smtp  For example:   export CLOUDBREAK_SMTP_SENDER_USERNAME='myemail@gmail.com'  \nexport CLOUDBREAK_SMTP_SENDER_PASSWORD='Mypassword123'  \nexport CLOUDBREAK_SMTP_SENDER_HOST='smtp.gmail.com'  \nexport CLOUDBREAK_SMTP_SENDER_PORT=25  \nexport CLOUDBREAK_SMTP_SENDER_FROM='myemail@gmail.com'  \nexport CLOUDBREAK_SMTP_AUTH=true  \nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true  \nexport CLOUDBREAK_SMTP_TYPE=smtp     The example assumes that you are using gmail. You should use the settings appropriate for your SMTP server.   If your SMTP server uses SMTPS, you must set the protocol in your  Profile  to smtps:  export CLOUDBREAK_SMTP_TYPE=smtps", 
            "title": "Configure SMTP email notifications"
        }, 
        {
            "location": "/cb-profile/index.html", 
            "text": "Customizing Cloudbreak Profile file\n\n\nCloudbreak deployer configuration is based on environment variables.  \n\n\nDuring startup, Cloudbreak deployer tries to determine the underlying infrastructure and then sets required environment variables with appropriate default values. If these environment variables are not sufficient for your use case, you can set additional environment variables in your \nProfile\n file. \n\n\nSet Profile variables\n\n\nTo set environment variables relevant for Cloudbreak Deployer, add them to a file called \nProfile\n located in the Cloudbreak deployment directory (typically \n/var/lib/cloudbreak-deployment\n).\n\n\nThe \nProfile\n file is sourced, so you can use the usual syntax to set configuration values:\n\n\nexport MY_VAR=some_value\nexport MY_OTHER_VAR=another_value \n\n\n\n\nAfter changing a property, you must regenerate the config file and restart the application by using \ncbd restart\n.\n\n\nCheck available Profile variables\n\n\nTo see all available environment variables with their default values, use:\n\n\ncbd env show\n\n\n\n\nCreate environment-specific Profile files\n\n\nIf you would like to use a different versions of Cloudbreak for prod and qa profile, you must create two environment specific configurations that can be sourced. For example:\n\n\n\n\nProfile.prod  \n\n\nProfile.qa   \n\n\n\n\nFor example, to create and use a prod profile, you need to:\n\n\n\n\nCreate a file called \nProfile.prod\n  \n\n\nWrite the environment-specific \nexport DOCKER_TAG_CLOUDBREAK=0.3.99\n into \nProfile.prod\n to specify Docker image.  \n\n\nSet the environment variable: \nCBD_DEFAULT_PROFILE=prod\n  \n\n\n\n\nTo use the prod specific profile once, set:  \n\n\nCBD_DEFAULT_PROFILE=prod cbd some_commands\n\n\n\nTo permanently use the prod profile, set \nexport CBD_DEFAULT_PROFILE=prod\n in your \n.bash_profile\n.\n\n\nSecure the Profile file\n\n\nBefore starting Cloudbreak for the first time, configure the Profile file as directed below. Changes are applied during startup so a restart (\ncbd restart\n) is required after each change.\n\n\n\n\n\n\nExecute the following command in the directory where you want to store Cloudbreak-related files:\n\n\n\necho export PUBLIC_IP=[the ip or hostname to bind] \n Profile\n\n\n\n\n\n\n\nAfter you have a base Profile file, add the following custom properties to it:\n\n\n\nexport UAA_DEFAULT_SECRET='[custom secret]'\nexport UAA_DEFAULT_USER_EMAIL='[default admin email address]'\nexport UAA_DEFAULT_USER_PW='[default admin password]'\nexport UAA_DEFAULT_USER_FIRSTNAME='[default admin first name]'\nexport UAA_DEFAULT_USER_LASTNAME='[default admin last name]'\n\n\n\nCloudbreak has additional secrets which by default inherit their values from \nUAA_DEFAULT_SECRET\n. Instead of using the default, you can define different values in the Profile for each of these service clients:\n\n\n\nexport UAA_CLOUDBREAK_SECRET='[cloudbreak secret]'\nexport UAA_PERISCOPE_SECRET='[auto scaling secret]'\nexport UAA_ULUWATU_SECRET='[web ui secret]'\nexport UAA_SULTANS_SECRET='[authenticator secret]'\n\n\n\nYou can change these secrets at any time, except \nUAA_CLOUDBREAK_SECRET\n which is used to encrypt sensitive information at database level. \n\n\nUAA_DEFAULT_USER_PW\n is stored in plain text format, but if \nUAA_DEFAULT_USER_PW\n is missing from the Profile, it gets a default value. Because default password is not an option, if you set an empty password explicitly in the Profile Cloudbreak deployer will ask for password all the time when it is needed for the operation.\n\n\n\nexport UAA_DEFAULT_USER_PW=''\n\n\n\nIn this case, Cloudbreak deployer wouldn't be able to add the default user, so you have to do it manually by executing the following command:\n\n\n\ncbd util add-default-user\n\n\n\n\n\n\n\nFor more information about setting environment variables in Profile, refer to \nSet Profile variables\n.\n\n\nConfiguring Consul\n\n\nCloudbreak uses \nConsul\n for DNS resolution. All Cloudbreak related services are registered as someservice.service.consul.\n\n\nConsul\u2019s built-in DNS server is able to fallback on another DNS server. This option is called \n-recursor\n. Cloudbreak deployer first tries to discover the DNS settings of the host by looking for nameserver entry in the \n/etc/resolv.conf\n file. If it finds one, consul will use it as a recursor. Otherwise, it will use \n8.8.8.8\n.\n\n\nFor a full list of available consul config options, refer to \nConsul documentation\n.\n\n\nTo pass any additional Consul configuration, define the \nDOCKER_CONSUL_OPTIONS\n variable in the Profile file.", 
            "title": "Modify settings in Cloudbreak Profile"
        }, 
        {
            "location": "/cb-profile/index.html#customizing-cloudbreak-profile-file", 
            "text": "Cloudbreak deployer configuration is based on environment variables.    During startup, Cloudbreak deployer tries to determine the underlying infrastructure and then sets required environment variables with appropriate default values. If these environment variables are not sufficient for your use case, you can set additional environment variables in your  Profile  file.", 
            "title": "Customizing Cloudbreak Profile file"
        }, 
        {
            "location": "/cb-profile/index.html#set-profile-variables", 
            "text": "To set environment variables relevant for Cloudbreak Deployer, add them to a file called  Profile  located in the Cloudbreak deployment directory (typically  /var/lib/cloudbreak-deployment ).  The  Profile  file is sourced, so you can use the usual syntax to set configuration values:  export MY_VAR=some_value\nexport MY_OTHER_VAR=another_value   After changing a property, you must regenerate the config file and restart the application by using  cbd restart .", 
            "title": "Set Profile variables"
        }, 
        {
            "location": "/cb-profile/index.html#check-available-profile-variables", 
            "text": "To see all available environment variables with their default values, use:  cbd env show", 
            "title": "Check available Profile variables"
        }, 
        {
            "location": "/cb-profile/index.html#create-environment-specific-profile-files", 
            "text": "If you would like to use a different versions of Cloudbreak for prod and qa profile, you must create two environment specific configurations that can be sourced. For example:   Profile.prod    Profile.qa      For example, to create and use a prod profile, you need to:   Create a file called  Profile.prod     Write the environment-specific  export DOCKER_TAG_CLOUDBREAK=0.3.99  into  Profile.prod  to specify Docker image.    Set the environment variable:  CBD_DEFAULT_PROFILE=prod      To use the prod specific profile once, set:    CBD_DEFAULT_PROFILE=prod cbd some_commands  To permanently use the prod profile, set  export CBD_DEFAULT_PROFILE=prod  in your  .bash_profile .", 
            "title": "Create environment-specific Profile files"
        }, 
        {
            "location": "/cb-profile/index.html#secure-the-profile-file", 
            "text": "Before starting Cloudbreak for the first time, configure the Profile file as directed below. Changes are applied during startup so a restart ( cbd restart ) is required after each change.    Execute the following command in the directory where you want to store Cloudbreak-related files:  \necho export PUBLIC_IP=[the ip or hostname to bind]   Profile    After you have a base Profile file, add the following custom properties to it:  \nexport UAA_DEFAULT_SECRET='[custom secret]'\nexport UAA_DEFAULT_USER_EMAIL='[default admin email address]'\nexport UAA_DEFAULT_USER_PW='[default admin password]'\nexport UAA_DEFAULT_USER_FIRSTNAME='[default admin first name]'\nexport UAA_DEFAULT_USER_LASTNAME='[default admin last name]'  Cloudbreak has additional secrets which by default inherit their values from  UAA_DEFAULT_SECRET . Instead of using the default, you can define different values in the Profile for each of these service clients:  \nexport UAA_CLOUDBREAK_SECRET='[cloudbreak secret]'\nexport UAA_PERISCOPE_SECRET='[auto scaling secret]'\nexport UAA_ULUWATU_SECRET='[web ui secret]'\nexport UAA_SULTANS_SECRET='[authenticator secret]'  You can change these secrets at any time, except  UAA_CLOUDBREAK_SECRET  which is used to encrypt sensitive information at database level.   UAA_DEFAULT_USER_PW  is stored in plain text format, but if  UAA_DEFAULT_USER_PW  is missing from the Profile, it gets a default value. Because default password is not an option, if you set an empty password explicitly in the Profile Cloudbreak deployer will ask for password all the time when it is needed for the operation.  \nexport UAA_DEFAULT_USER_PW=''  In this case, Cloudbreak deployer wouldn't be able to add the default user, so you have to do it manually by executing the following command:  \ncbd util add-default-user    For more information about setting environment variables in Profile, refer to  Set Profile variables .", 
            "title": "Secure the Profile file"
        }, 
        {
            "location": "/cb-profile/index.html#configuring-consul", 
            "text": "Cloudbreak uses  Consul  for DNS resolution. All Cloudbreak related services are registered as someservice.service.consul.  Consul\u2019s built-in DNS server is able to fallback on another DNS server. This option is called  -recursor . Cloudbreak deployer first tries to discover the DNS settings of the host by looking for nameserver entry in the  /etc/resolv.conf  file. If it finds one, consul will use it as a recursor. Otherwise, it will use  8.8.8.8 .  For a full list of available consul config options, refer to  Consul documentation .  To pass any additional Consul configuration, define the  DOCKER_CONSUL_OPTIONS  variable in the Profile file.", 
            "title": "Configuring Consul"
        }, 
        {
            "location": "/os-image-import/index.html", 
            "text": "Manually import HDP and HDF images to OpenStack\n\n\nAn OpenStack administrator can perform these steps to add the Cloudbreak deployer image to your OpenStack deployment. Perform these steps for each image. \n\n\n\n\nImporting prewarmed and base HDP and HDF images is no longer required, because if these images are not imported manually, Cloudbreak will import them once you attempt to create a cluster.\n\n\n\n\nThe following images can be imported:\n\n\n\n\n\n\n\n\nImage\n\n\nOperating system\n\n\nLocation\n\n\n\n\n\n\n\n\n\n\nPrewarmed HDP 2.6 image\n\n\ncentos7\n\n\nhttp://public-repo-1.hortonworks.com/HDP/cloudbreak/cb-hdp-26-1805171052.img\n\n\n\n\n\n\nPrewarmed HDF 3.1 image\n\n\ncentos7\n\n\nhttp://public-repo-1.hortonworks.com/HDP/cloudbreak/cb-hdp-31-1805251001.img\n\n\n\n\n\n\nBase image\n\n\ncentos7\n\n\nhttp://public-repo-1.hortonworks.com/HDP/cloudbreak/cb-hdp--1806131251.img\n\n\n\n\n\n\nBase image\n\n\nubuntu\n\n\nhttp://public-repo-1.hortonworks.com/HDP/cloudbreak/cb-hdp--1805252042.img\n\n\n\n\n\n\n\n\nSteps\n\n\n\n\n\n\nDownload the image to your local machine. For example: \n\n\ncurl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/cb-hdp-26-1805171052.img\n\n\n\n\n\n\nSet the following environment variables for the OpenStack image import:\n\n\nexport CB_LATEST_IMAGE=cb-hdp-26-1805171052.img\nexport CB_LATEST_IMAGE_NAME=cb-hdp-26-1805171052.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name\n\n\n\n\n\n\nImport the new image into your OpenStack:\n\n\nglance image-create --name \"$CB_LATEST_IMAGE_NAME\" --file \"$CB_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress\n\n\n\n\n\n\nAfter performing the import, you should be able to see the Cloudbreak image among your OpenStack images.", 
            "title": "Manually import HDP and HDF images to OpenStack"
        }, 
        {
            "location": "/os-image-import/index.html#manually-import-hdp-and-hdf-images-to-openstack", 
            "text": "An OpenStack administrator can perform these steps to add the Cloudbreak deployer image to your OpenStack deployment. Perform these steps for each image.    Importing prewarmed and base HDP and HDF images is no longer required, because if these images are not imported manually, Cloudbreak will import them once you attempt to create a cluster.   The following images can be imported:     Image  Operating system  Location      Prewarmed HDP 2.6 image  centos7  http://public-repo-1.hortonworks.com/HDP/cloudbreak/cb-hdp-26-1805171052.img    Prewarmed HDF 3.1 image  centos7  http://public-repo-1.hortonworks.com/HDP/cloudbreak/cb-hdp-31-1805251001.img    Base image  centos7  http://public-repo-1.hortonworks.com/HDP/cloudbreak/cb-hdp--1806131251.img    Base image  ubuntu  http://public-repo-1.hortonworks.com/HDP/cloudbreak/cb-hdp--1805252042.img     Steps    Download the image to your local machine. For example:   curl -O https://public-repo-1.hortonworks.com/HDP/cloudbreak/cb-hdp-26-1805171052.img    Set the following environment variables for the OpenStack image import:  export CB_LATEST_IMAGE=cb-hdp-26-1805171052.img\nexport CB_LATEST_IMAGE_NAME=cb-hdp-26-1805171052.img\nexport OS_USERNAME=your_os_user_name\nexport OS_AUTH_URL=your_authentication_url\nexport OS_TENANT_NAME=your_os_tenant_name    Import the new image into your OpenStack:  glance image-create --name \"$CB_LATEST_IMAGE_NAME\" --file \"$CB_LATEST_IMAGE\" --disk-format qcow2 --container-format bare --progress    After performing the import, you should be able to see the Cloudbreak image among your OpenStack images.", 
            "title": "Manually import HDP and HDF images to OpenStack"
        }, 
        {
            "location": "/hdf/index.html", 
            "text": "Creating HDF clusters\n\n\nIn general, the create cluster wizard offers prescriptive default settings that help you configure your HDF clusters properly; however, there are a few additional configuration requirements that you should be aware of. \n\n\nCreating HDF Flow Management clusters\n\n\nWhen creating a Flow Management cluster from the default blueprint, make sure to do the following:\n\n\n\n\nOn the \nHardware and Storage\n page, place the \nAmbari Server\n on the \"Services\" host group. \n\n\nOn the \nNetwork\n page, open the required ports:        \n\n\nOpen \n9091\n TCP port on the NiFi host group. This port is used by \nNiFi web UI\n; Without it, you will be unable to access the NiFi web UI.   \n\n\nOpen \n61443\n TCP port on the Services host group. This port is used by \nNiFi Registry\n.       \n\n\n\n\n\n\nYou should either use your \nexisting LDAP\n or enable \nKerberos\n.      \n\n\nIf using \nLDAP\n, you must first \nregister it as an external authentication source\n in the Cloudbreak web UI.   \n\n\nIf using \nKerberos\n, you can either use your own Kerberos (for production) or select for Cloudbreak to create a test KDC (for evaluation only).  \n\n\n\n\n\n\nWhen creating a HDF cluster with LDAP, on the \nSecurity\n page:\n\n\nYou must must specify a \nCluster User\n that is a valid user in the LDAP. This is a limitation with NiFi/NiFi Registry. Only one login provider can be configured for those components, and if an LDAP is supplied, then the login provider is set to LDAP; Consequently, this requires that the initial admin be in the given LDAP.  \n\n\nWhile the passwords don't need to match between the cluster user and the same-named LDAP user, any other components that are used by that cluster user would require the password entered for the cluster user, not the same-named LDAP user.        \n\n\n\n\n\n\nWhen creating the NiFi Registry controller service in NiFi, the internal hostname must be used, \ne.g. https://ip-1-2-3-4.us-west-2.compute.internal:61443\n   \n\n\nAlthough Cloudbreak allows cluster scaling (including autoscaling), scaling is not supported by NiFi:       \n\n\nDownscaling NiFi clusters is not supported\n - as it can result in data loss when a node is removed that has not yet processed all the data on that node.       \n\n\nThere is a known issue related to upscaling; It is listed in the \nKnown Issues\n. \n\n\n\n\n\n\n\n\nTroubleshooting HDF Flow Management cluster creation\n\n\nNiFi returns the following error when the Flow Management cluster is set up with an LDAP: \n\n\nCaused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'authorizer': FactoryBean threw exception on object creation; nested exception is org.apache.nifi.authorization.exception.AuthorizerCreationException: org.apache.nifi.authorization.exception.AuthorizerCreationException: Unable to locate initial admin admin to seed policies\n        at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:175)\n        at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.getObjectFromFactoryBean(FactoryBeanRegistrySupport.java:103)\n        at org.springframework.beans.factory.support.AbstractBeanFactory.getObjectForBeanInstance(AbstractBeanFactory.java:1634)\n        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:317)\n        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197)\n        at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:351)\n        ... 90 common frames omitted\nCaused by: org.apache.nifi.authorization.exception.AuthorizerCreationException: org.apache.nifi.authorization.exception.AuthorizerCreationException: Unable to locate initial admin admin to seed policies\n        at org.apache.nifi.authorization.FileAccessPolicyProvider.onConfigured(FileAccessPolicyProvider.java:231)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.nifi.authorization.AccessPolicyProviderInvocationHandler.invoke(AccessPolicyProviderInvocationHandler.java:46)\n        at com.sun.proxy.$Proxy72.onConfigured(Unknown Source)\n        at org.apache.nifi.authorization.AuthorizerFactoryBean.getObject(AuthorizerFactoryBean.java:143)\n        at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:168)\n        ... 95 common frames omitted\nCaused by: org.apache.nifi.authorization.exception.AuthorizerCreationException: Unable to locate initial admin admin to seed policies\n        at org.apache.nifi.authorization.FileAccessPolicyProvider.populateInitialAdmin(FileAccessPolicyProvider.java:566)\n        at org.apache.nifi.authorization.FileAccessPolicyProvider.load(FileAccessPolicyProvider.java:509)\n        at org.apache.nifi.authorization.FileAccessPolicyProvider.onConfigured(FileAccessPolicyProvider.java:222)\n        ... 103 common frames omitted\n\n\n\nCause\n:\n\n\nWhen creating a HDF cluster with LDAP, on the \nSecurity\n page of the create cluster wizard you specified some \nCluster User\n that is not a valid user in the LDAP. \n\n\nSolution\n:\n\n\nWhen creating a HDF cluster with LDAP, on the \nSecurity\n page of the create cluster wizard, specify a \nCluster User\n that is a valid user in the LDAP. \n\n\nCreating HDF Messaging Management clusters\n\n\nWhen creating a Messaging Management cluster from the default blueprint, make sure to do the following:\n\n\n\n\nOn the \nHardware and Storage\n page, place the \nAmbari Server\n on the \"Services\" host group.  \n\n\nWhen creating a cluster, open \n3000\n TCP port on the Services host group for Grafana.", 
            "title": "Creating an HDF cluster"
        }, 
        {
            "location": "/hdf/index.html#creating-hdf-clusters", 
            "text": "In general, the create cluster wizard offers prescriptive default settings that help you configure your HDF clusters properly; however, there are a few additional configuration requirements that you should be aware of.", 
            "title": "Creating HDF clusters"
        }, 
        {
            "location": "/hdf/index.html#creating-hdf-flow-management-clusters", 
            "text": "When creating a Flow Management cluster from the default blueprint, make sure to do the following:   On the  Hardware and Storage  page, place the  Ambari Server  on the \"Services\" host group.   On the  Network  page, open the required ports:          Open  9091  TCP port on the NiFi host group. This port is used by  NiFi web UI ; Without it, you will be unable to access the NiFi web UI.     Open  61443  TCP port on the Services host group. This port is used by  NiFi Registry .           You should either use your  existing LDAP  or enable  Kerberos .        If using  LDAP , you must first  register it as an external authentication source  in the Cloudbreak web UI.     If using  Kerberos , you can either use your own Kerberos (for production) or select for Cloudbreak to create a test KDC (for evaluation only).      When creating a HDF cluster with LDAP, on the  Security  page:  You must must specify a  Cluster User  that is a valid user in the LDAP. This is a limitation with NiFi/NiFi Registry. Only one login provider can be configured for those components, and if an LDAP is supplied, then the login provider is set to LDAP; Consequently, this requires that the initial admin be in the given LDAP.    While the passwords don't need to match between the cluster user and the same-named LDAP user, any other components that are used by that cluster user would require the password entered for the cluster user, not the same-named LDAP user.            When creating the NiFi Registry controller service in NiFi, the internal hostname must be used,  e.g. https://ip-1-2-3-4.us-west-2.compute.internal:61443      Although Cloudbreak allows cluster scaling (including autoscaling), scaling is not supported by NiFi:         Downscaling NiFi clusters is not supported  - as it can result in data loss when a node is removed that has not yet processed all the data on that node.         There is a known issue related to upscaling; It is listed in the  Known Issues .", 
            "title": "Creating HDF Flow Management clusters"
        }, 
        {
            "location": "/hdf/index.html#troubleshooting-hdf-flow-management-cluster-creation", 
            "text": "NiFi returns the following error when the Flow Management cluster is set up with an LDAP:   Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'authorizer': FactoryBean threw exception on object creation; nested exception is org.apache.nifi.authorization.exception.AuthorizerCreationException: org.apache.nifi.authorization.exception.AuthorizerCreationException: Unable to locate initial admin admin to seed policies\n        at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:175)\n        at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.getObjectFromFactoryBean(FactoryBeanRegistrySupport.java:103)\n        at org.springframework.beans.factory.support.AbstractBeanFactory.getObjectForBeanInstance(AbstractBeanFactory.java:1634)\n        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:317)\n        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197)\n        at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:351)\n        ... 90 common frames omitted\nCaused by: org.apache.nifi.authorization.exception.AuthorizerCreationException: org.apache.nifi.authorization.exception.AuthorizerCreationException: Unable to locate initial admin admin to seed policies\n        at org.apache.nifi.authorization.FileAccessPolicyProvider.onConfigured(FileAccessPolicyProvider.java:231)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.nifi.authorization.AccessPolicyProviderInvocationHandler.invoke(AccessPolicyProviderInvocationHandler.java:46)\n        at com.sun.proxy.$Proxy72.onConfigured(Unknown Source)\n        at org.apache.nifi.authorization.AuthorizerFactoryBean.getObject(AuthorizerFactoryBean.java:143)\n        at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:168)\n        ... 95 common frames omitted\nCaused by: org.apache.nifi.authorization.exception.AuthorizerCreationException: Unable to locate initial admin admin to seed policies\n        at org.apache.nifi.authorization.FileAccessPolicyProvider.populateInitialAdmin(FileAccessPolicyProvider.java:566)\n        at org.apache.nifi.authorization.FileAccessPolicyProvider.load(FileAccessPolicyProvider.java:509)\n        at org.apache.nifi.authorization.FileAccessPolicyProvider.onConfigured(FileAccessPolicyProvider.java:222)\n        ... 103 common frames omitted  Cause :  When creating a HDF cluster with LDAP, on the  Security  page of the create cluster wizard you specified some  Cluster User  that is not a valid user in the LDAP.   Solution :  When creating a HDF cluster with LDAP, on the  Security  page of the create cluster wizard, specify a  Cluster User  that is a valid user in the LDAP.", 
            "title": "Troubleshooting HDF Flow Management cluster creation"
        }, 
        {
            "location": "/hdf/index.html#creating-hdf-messaging-management-clusters", 
            "text": "When creating a Messaging Management cluster from the default blueprint, make sure to do the following:   On the  Hardware and Storage  page, place the  Ambari Server  on the \"Services\" host group.    When creating a cluster, open  3000  TCP port on the Services host group for Grafana.", 
            "title": "Creating HDF Messaging Management clusters"
        }, 
        {
            "location": "/data-lake/index.html", 
            "text": "Setting up a data lake\n\n\n\n\nThis feature is technical preview.  \n\n\n\n\nA \ndata lake\n provides a way for you to centrally apply and enforce authentication, authorization, and audit policies across multiple ephemeral workload clusters. \"Attaching\" your workload cluster to the data lake instance allows the attached cluster workloads to access data and run in the security context provided by the data lake. \n\n\nWhile workloads are temporary, the security policies around your data schema are long-running and shared for all workloads. As your workloads come and go, the data lake instance lives on, providing consistent and available security policy definitions that are available for current and future ephemeral workloads. All information related to schema (Hive), security policies (Ranger) and audit (Ranger) is stored on external locations (external databases and cloud storage).\n\n\nThis is illustrated in the following diagram: \n\n\n\n\nOnce you\u2019ve created a data lake instance, you have an option to attach it to one or more ephemeral clusters. This allows you to apply the authentication, authorization, and audit across multiple workload clusters.\n\n\nThe following table explains basic data lake terminology: \n\n\n\n\n\n\n\n\nTerm\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nData lake\n\n\nRuns Ranger, which is used for configuring authorization. policies and is used for audit capture. Runs Hive Metastore, which is used for data schema.\n\n\n\n\n\n\nWorkload clusters\n\n\nThe clusters that get attached to the data lake to run workloads. This is where you run workloads such as Hive via \nJDBC\n.\n\n\n\n\n\n\n\n\nThe following table explains the components of a data lake: \n\n\n\n\n\n\n\n\n\n\nComponent\n\n\nTechnology\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSchema\n\n\nApache Hive\n\n\nProvides Hive schema (tables, views, and so on). If you have two or more workloads accessing the same Hive data, you need to share schema across these workloads.\n\n\n\n\n\n\nPolicy\n\n\nApache Ranger\n\n\nDefines security policies around Hive schema. If you have two or more users accessing the same data, you need security policies to be consistently available and enforced.\n\n\n\n\n\n\nAudit\n\n\nApache Ranger\n\n\nAudits user access and captures data access activity for the workloads.\n\n\n\n\n\n\nDirectory \n\n\nLDAP/AD\n\n\nProvides an authentication source for users and a definition of groups for authorization.\n\n\n\n\n\n\nGateway\n\n\nApache Knox\n\n\nSupports a single workload endpoint that can be protected with SSL and enabled for authentication to access to resources.\n\n\n\n\n\n\n\n\nOverview of data lake setup\n\n\nSetting up a data lake involves the following steps:\n\n\n\n\n\n\n\n\nStep\n\n\nWhere to perform\n\n\n\n\n\n\n\n\n\n\nMeet the prerequisites\n:\nCreate two external databases (one for Hive metastore and one for Ranger)\nCreate an external authentication source for LDAP/AD\nPrepare a cloud storage location (depending on your cloud provider, this should be, on Amazon S3, Azure's ADLS or WASB, or GCS) for default Hive warehouse directory and Ranger audit logs)\n\n\nYou must create these resources on your own, outside of Cloudbreak. You may use one database instance and create two databases.\n\n\n\n\n\n\nRegister the two databases and LDAP\n\n\nIn the Cloudbreak web UI \n External Sources\n\n\n\n\n\n\nCreate a data lake\n\n\nIn the Cloudbreak web UI \n Create cluster\n\n\n\n\n\n\nCreate clusters attached to the data lake\n\n\nIn the Cloudbreak web UI \n Create cluster\n\n\n\n\n\n\n\n\nPrerequisites\n\n\nYou must perform the following prerequisites \noutside of Cloudbreak\n: \n\n\n\n\nSet up two external database instances, one for the HIVE component, and one for the RANGER component. For supported databases, refer to \nUsing an external database\n.     \n\n\nCreate an LDAP instance and set up your users inside the LDAP.  \n\n\nPrepare a cloud storage location for default Hive warehouse directory and Ranger audit logs.  \n\n\n\n\nIn the steps that follow, you will be required to provide the information related to these external resources. \n\n\nRegister the databases and LDAP in the Cloudbreak web UI\n\n\nPrior to creating a data lake, you must register the following resources in the Cloudbreak web UI:\n\n\nSteps\n \n\n\n\n\n\n\nRegister each of your two RDS instances created as part of the prerequisites in the Cloudbreak web UI, under \nExternal Sources\n \n \nDatabase Configurations\n. For instructions, refer  \nUsing an external database\n.    \n\n\n\n\nWhen registering the database for Hive, select \nType \n Hive\n.  \n\n\nWhen registering the database for Ranger, select \nType \n Ranger\n.  \n\n\n\n\n\n\n\n\nRegister your LDAP (created as part of the prerequisites) in the Cloudbreak web UI, under \nExternal Sources \n Authentication Configurations\n. For instructions, refer to \nRegister an authentication source\n.  \n\n\n\n\n\n\nAs an outcome of this step, you should have two external databases and one authentication source registered in the Cloudbreak web UI.  \n\n\nCreate a data lake\n\n\nCreate a data lake by using the create cluster wizard. Among other information, make sure to provide the information listed in the steps below. \n\n\nSteps\n \n\n\n\n\nNavigate to the advanced version of the create cluster wizard.  \n\n\n\n\nOn the \nGeneral Configuration\n page:  \n\n\n\n\nUnder \nCluster Name\n, provide a name for your data lake. \n\n\nUnder \nCluster Type\n, choose the \nDATA LAKE\n blueprint:\n\n\n\n\n  \n\n\n\n\n\n\nOn the \nCloud Storage\n page:  \n\n\n\n\nUnder \nCloud Storage\n, configure access to cloud storage via the method available for your cloud provider.  \n\n\nUnder \nStorage Locations\n, provide an existing location within your cloud storage account that can be used for Hive warehouse directory and Ranger audit logs.  \n\n\n\n\n\n\nNote\n\n\nThe storage location must exist prior to data lake provisioning. If the storage location does not exist then Ranger is installed properly, but it may not work.\n\n\n\n\n  \n\n\n\n\n\n\nOn the \nExternal Sources\n page, select the previously registered Ranger database, Hive database and LDAP:\n\n\n \n\n\n\n\n\n\nOn the \nGateway Configuration\n page, the gateway is enabled by default with Ambari exposed through the gateway. You should also enable Ranger by selecting the Ranger service and clicking \nExpose\n. \n\n\n    \n\n\n\n\n\n\nOn the \nSecurity\n page:\n\n\n\n\nUnder \nPassword\n, provide a strong password for your cluster. For example \u201cSomeRandomChars123!\u201d is a strong password. A strong password is required for the default Ranger admin, which - among other cluster components like Ambari - will use this password. \n\n\nSelect an SSH key.\n\n\n\n\n\n\n\n\nClick \nCREATE CLUSTER\n to initiate data lake creation.       \n\n\n\n\n\n\nAs an outcome of this step, you should have a running data lake.\n\n\nCreate attached HDP clusters\n\n\nOnce your data lake is running, you can start creating clusters attached to the data lake. Follow these general steps to create an cluster attached to a data lake. \n\n\nIn general, once you've selected the data lake that the cluster should be using, the cluster wizard should provide you with the cluster settings that should be used for the attached cluster.    \n\n\nSteps\n \n\n\n\n\n\n\nIn the Cloudbreak web UI, click on the cluster tile representing your data lake.  \n\n\n\n\n\n\nFrom the \nACTIONS\n menu, select \nCREATE ATTACHED CLUSTER\n.    \n\n\n \n\n\n\n\n\n\nIn general, the cluster wizard should provide you with the cluster settings that should be used for the attached cluster. Still, make sure to do the following:\n\n\n\n\nUnder \nRegion\n and \nAvailability Zone\n, select the same location where your data lake is running.   \n\n\nSelect one of the three default blueprints.  \n\n\nOn the \nCloud Storage\n page, enter the same cloud storage location that your data lake is using.  \n\n\nOn the \nExternal Sources\n page, the LDAP, and Ranger and Hive databases that you attached to the data lake should be attached to your cluster.      \n\n\nOn the \nNetwork\n page, select the same VPC and subnet where the data lake is running.  \n\n\n\n\n\n\n\n\nClick on \nCREATE CLUSTER\n to initiate cluster creation. \n\n\n\n\n\n\nAs an outcome of this step, you should have a running cluster attached to the data lake. Access your attached clusters and run your workloads as normal.", 
            "title": "Setting up a data lake"
        }, 
        {
            "location": "/data-lake/index.html#setting-up-a-data-lake", 
            "text": "This feature is technical preview.     A  data lake  provides a way for you to centrally apply and enforce authentication, authorization, and audit policies across multiple ephemeral workload clusters. \"Attaching\" your workload cluster to the data lake instance allows the attached cluster workloads to access data and run in the security context provided by the data lake.   While workloads are temporary, the security policies around your data schema are long-running and shared for all workloads. As your workloads come and go, the data lake instance lives on, providing consistent and available security policy definitions that are available for current and future ephemeral workloads. All information related to schema (Hive), security policies (Ranger) and audit (Ranger) is stored on external locations (external databases and cloud storage).  This is illustrated in the following diagram:    Once you\u2019ve created a data lake instance, you have an option to attach it to one or more ephemeral clusters. This allows you to apply the authentication, authorization, and audit across multiple workload clusters.  The following table explains basic data lake terminology:      Term  Description      Data lake  Runs Ranger, which is used for configuring authorization. policies and is used for audit capture. Runs Hive Metastore, which is used for data schema.    Workload clusters  The clusters that get attached to the data lake to run workloads. This is where you run workloads such as Hive via  JDBC .     The following table explains the components of a data lake:       Component  Technology  Description      Schema  Apache Hive  Provides Hive schema (tables, views, and so on). If you have two or more workloads accessing the same Hive data, you need to share schema across these workloads.    Policy  Apache Ranger  Defines security policies around Hive schema. If you have two or more users accessing the same data, you need security policies to be consistently available and enforced.    Audit  Apache Ranger  Audits user access and captures data access activity for the workloads.    Directory   LDAP/AD  Provides an authentication source for users and a definition of groups for authorization.    Gateway  Apache Knox  Supports a single workload endpoint that can be protected with SSL and enabled for authentication to access to resources.", 
            "title": "Setting up a data lake"
        }, 
        {
            "location": "/data-lake/index.html#overview-of-data-lake-setup", 
            "text": "Setting up a data lake involves the following steps:     Step  Where to perform      Meet the prerequisites : Create two external databases (one for Hive metastore and one for Ranger) Create an external authentication source for LDAP/AD Prepare a cloud storage location (depending on your cloud provider, this should be, on Amazon S3, Azure's ADLS or WASB, or GCS) for default Hive warehouse directory and Ranger audit logs)  You must create these resources on your own, outside of Cloudbreak. You may use one database instance and create two databases.    Register the two databases and LDAP  In the Cloudbreak web UI   External Sources    Create a data lake  In the Cloudbreak web UI   Create cluster    Create clusters attached to the data lake  In the Cloudbreak web UI   Create cluster", 
            "title": "Overview of data lake setup"
        }, 
        {
            "location": "/data-lake/index.html#prerequisites", 
            "text": "You must perform the following prerequisites  outside of Cloudbreak :    Set up two external database instances, one for the HIVE component, and one for the RANGER component. For supported databases, refer to  Using an external database .       Create an LDAP instance and set up your users inside the LDAP.    Prepare a cloud storage location for default Hive warehouse directory and Ranger audit logs.     In the steps that follow, you will be required to provide the information related to these external resources.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/data-lake/index.html#register-the-databases-and-ldap-in-the-cloudbreak-web-ui", 
            "text": "Prior to creating a data lake, you must register the following resources in the Cloudbreak web UI:  Steps      Register each of your two RDS instances created as part of the prerequisites in the Cloudbreak web UI, under  External Sources     Database Configurations . For instructions, refer   Using an external database .       When registering the database for Hive, select  Type   Hive .    When registering the database for Ranger, select  Type   Ranger .       Register your LDAP (created as part of the prerequisites) in the Cloudbreak web UI, under  External Sources   Authentication Configurations . For instructions, refer to  Register an authentication source .      As an outcome of this step, you should have two external databases and one authentication source registered in the Cloudbreak web UI.", 
            "title": "Register the databases and LDAP in the Cloudbreak web UI"
        }, 
        {
            "location": "/data-lake/index.html#create-a-data-lake", 
            "text": "Create a data lake by using the create cluster wizard. Among other information, make sure to provide the information listed in the steps below.   Steps     Navigate to the advanced version of the create cluster wizard.     On the  General Configuration  page:     Under  Cluster Name , provide a name for your data lake.   Under  Cluster Type , choose the  DATA LAKE  blueprint:         On the  Cloud Storage  page:     Under  Cloud Storage , configure access to cloud storage via the method available for your cloud provider.    Under  Storage Locations , provide an existing location within your cloud storage account that can be used for Hive warehouse directory and Ranger audit logs.      Note  The storage location must exist prior to data lake provisioning. If the storage location does not exist then Ranger is installed properly, but it may not work.         On the  External Sources  page, select the previously registered Ranger database, Hive database and LDAP:       On the  Gateway Configuration  page, the gateway is enabled by default with Ambari exposed through the gateway. You should also enable Ranger by selecting the Ranger service and clicking  Expose .           On the  Security  page:   Under  Password , provide a strong password for your cluster. For example \u201cSomeRandomChars123!\u201d is a strong password. A strong password is required for the default Ranger admin, which - among other cluster components like Ambari - will use this password.   Select an SSH key.     Click  CREATE CLUSTER  to initiate data lake creation.           As an outcome of this step, you should have a running data lake.", 
            "title": "Create a data lake"
        }, 
        {
            "location": "/data-lake/index.html#create-attached-hdp-clusters", 
            "text": "Once your data lake is running, you can start creating clusters attached to the data lake. Follow these general steps to create an cluster attached to a data lake.   In general, once you've selected the data lake that the cluster should be using, the cluster wizard should provide you with the cluster settings that should be used for the attached cluster.      Steps      In the Cloudbreak web UI, click on the cluster tile representing your data lake.      From the  ACTIONS  menu, select  CREATE ATTACHED CLUSTER .           In general, the cluster wizard should provide you with the cluster settings that should be used for the attached cluster. Still, make sure to do the following:   Under  Region  and  Availability Zone , select the same location where your data lake is running.     Select one of the three default blueprints.    On the  Cloud Storage  page, enter the same cloud storage location that your data lake is using.    On the  External Sources  page, the LDAP, and Ranger and Hive databases that you attached to the data lake should be attached to your cluster.        On the  Network  page, select the same VPC and subnet where the data lake is running.       Click on  CREATE CLUSTER  to initiate cluster creation.     As an outcome of this step, you should have a running cluster attached to the data lake. Access your attached clusters and run your workloads as normal.", 
            "title": "Create attached HDP clusters"
        }, 
        {
            "location": "/clusters-access/index.html", 
            "text": "Accessing a cluster\n\n\nThe following section describes how to access the various services in the cluster.\n\n\nCloudbreak user accounts\n\n\nThe following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:\n\n\n\n\n\n\n\n\nComponent\n\n\nMethod\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak\n\n\nWeb UI, CLI\n\n\nAccess with the username and password provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCloudbreak\n\n\nSSH to VM\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.\n\n\n\n\n\n\nCluster\n\n\nSSH to VMs\n\n\nAccess as the \"cloudbreak\" user with the SSH key provided during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nAmbari web UI\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\nCluster\n\n\nWeb UIs for specific cluster services\n\n\nAccess with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.\n\n\n\n\n\n\n\n\nFinding cluster information in the web UI\n\n\nOnce your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions. \n\n\n \n\n\nThe information presented includes:\n\n\n\n\nCluster summary\n  \n\n\nCluster information\n     \n\n\nEvent history\n  \n\n\n\n\n\n  \nTips\n\n  \n\n  \n Access cluster actions such as resize and sync by clicking on \nACTIONS\n.\n\n  \n Access Ambari web UI by clicking on the link in the \nCLUSTER INFORMATION\n section.\n\n\n View public IP addresses for all cluster instances in the \nHARDWARE\n section. Click on the links to view the instances in the cloud console.\n\n\n The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".\n \n\n\n\n\n\n\n\n\nCluster summary\n\n\nThe summary bar includes the following information about your cluster:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster Name\n\n\nThe name that you selected for your cluster is displayed at the top of the page. Below it is the name of the cluster blueprint.\n\n\n\n\n\n\nTime Remaining\n\n\nIf you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.\n\n\n\n\n\n\nCloud Provider\n\n\nThe logo of the cloud provider on which the cluster is running.\n\n\n\n\n\n\nCredential\n\n\nThe name of the credential used to create the cluster.\n\n\n\n\n\n\nStatus\n\n\nCurrent status. When a cluster is healthy, the status is \nRunning\n.\n\n\n\n\n\n\nNodes\n\n\nThe current number of cluster nodes, including the master node.\n\n\n\n\n\n\nUptime\n\n\nThe amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.\n\n\n\n\n\n\nCreated\n\n\nThe date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.\n\n\n\n\n\n\n\n\nCluster information\n\n\nThe following information is available on the cluster details page: \n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCluster User\n\n\nThe name of the cluster user that you created when creating the cluster.\n\n\n\n\n\n\nSSH Username\n\n\nThe SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".\n\n\n\n\n\n\nAmbari URL\n\n\nLink to the Ambari web UI.\n\n\n\n\n\n\nRegion\n\n\nThe region in which the cluster is running in the cloud provider infrastructure.\n\n\n\n\n\n\nAvailability Zone\n\n\nThe availability zone within the region in which the cluster is running.\n\n\n\n\n\n\nBlueprint\n\n\nThe name of the blueprint selected under \"Cluster Type\" to create this cluster.\n\n\n\n\n\n\nCreated With\n\n\nThe version of Cloudbreak used to create this cluster.\n\n\n\n\n\n\nAmbari Version\n\n\nThe Ambari version which this cluster is currently running.\n\n\n\n\n\n\nHDP/HDF Version\n\n\nThe HDP or HDF version which this cluster is currently running.\n\n\n\n\n\n\nAuthentication Source\n\n\nIf you are using an external authentication source (LDAP/AD) for your cluster, you can see it here. Refer to \nUsing an external authentication source\n.\n\n\n\n\n\n\n\n\nBelow this, you will see additional tabs that you can click on in order to see their content:\n\n\n\n\n\n\n\n\nItem\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nHardware\n\n\nThis section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs.\n\n\n\n\n\n\nCloud storage\n\n\nIf you configured any cloud storage options, you will see them listed here.\n\n\n\n\n\n\nTags\n\n\nThis section lists keys and values of the user-defined tags, in the same order as you added them.\n\n\n\n\n\n\nGateway\n\n\nThis section is available when gateway is configured for a cluster. It includes the gateway URL to Ambari and the URLs for the service UIs.\n\n\n\n\n\n\nRecipes\n\n\nThis section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type. Refer to \nUsing custom scripts (recipes)\n.\n\n\n\n\n\n\nExternal databases\n\n\nIf you are using an external database for your cluster, you can see it here. Refer to \nusing an external database\n.\n\n\n\n\n\n\nRepository details\n\n\nThis section includes Ambari and HDP/HDF repository information, as you provided it in the \"Base Images\" section when creating a cluster.\n\n\n\n\n\n\nImage details\n\n\nThis section includes information about the base image that was used for the Cloudbreak instance.\n\n\n\n\n\n\nNetwork\n\n\nThis section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.\n\n\n\n\n\n\nSecurity\n\n\nThis section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.\n\n\n\n\n\n\nAutoscaling\n\n\nThis section includes configuration options related to autoscaling. Refer to \nConfiguring autoscaling\n.\n\n\n\n\n\n\n\n\nEvent history\n\n\nThe Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:\n\n\n\nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM\n\n\n\nAccessing cluster via SSH\n\n\nIf you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster. \n\n\n\n\nIn order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.  \n\n\nYou can find the cluster instance public IP addresses on the cluster details page.  \n\n\nWhen accessing instances via SSH use the \ncloudbreak\n user. \n\n\n\n\nOn Mac OS, you can use the following syntax to SSH to the VM:\n\nssh -i \"privatekey.pem\" cloudbreak@publicIP\n\nFor example:\n\nssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132\n\n\nOn Windows, you can SSH using an SSH client such as PuTTY.\n\n\nAccess Ambari\n\n\nYou can access Ambari web UI by clicking on the links provided in the \nCluster Information\n \n \nAmbari URL\n.\n\n\nSteps\n\n\n\n\n\n\nFrom the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.\n\n\n\n\n\n\nFind the Ambari URL in the \nCluster Information\n section. This URL is available once the Ambari cluster creation process has completed.  \n\n\n\n\n\n\nClick on the \nAmbari URL\n link.\n\n\n\n\n\n\nThe first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.\n\n\n\n\n\n\n\n\nBrowser\n\n\nSteps\n\n\n\n\n\n\n\n\n\n\nFirefox\n\n\nClick \nAdvanced\n \n Click \nAdd Exception...\n \n Click \nConfirm Security Exception\n\n\n\n\n\n\nSafari\n\n\nClick \nContinue\n\n\n\n\n\n\nChrome\n\n\nClick \nAdvanced\n \n Click \nProceed...\n\n\n\n\n\n\n\n\n\n\n\n\nView cluster blueprints\n\n\nCloudbreak includes a useful option to view blueprints of a future cluster (from the create cluster wizard) or an existing cluster (from cluster details):\n\n\n\n\n\n\nTo view a cluster blueprint from the create cluster wizard, on the last page of the wizard, select \nShow blueprint\n.    \n\n\n\n\n\n\nTo view a cluster blueprint for an existing cluster, navigate to cluster details, and from the \nACTIONS\n menu, select \nShow blueprint\n.", 
            "title": "Accessing a cluster"
        }, 
        {
            "location": "/clusters-access/index.html#accessing-a-cluster", 
            "text": "The following section describes how to access the various services in the cluster.", 
            "title": "Accessing a cluster"
        }, 
        {
            "location": "/clusters-access/index.html#cloudbreak-user-accounts", 
            "text": "The following table describes what credentials to use to access Cloudbreak and Cloudbreak-managed clusters:     Component  Method  Description      Cloudbreak  Web UI, CLI  Access with the username and password provided when launching Cloudbreak on the cloud provider.    Cloudbreak  SSH to VM  Access as the \"cloudbreak\" user with the SSH key provided when launching Cloudbreak on the cloud provider.    Cluster  SSH to VMs  Access as the \"cloudbreak\" user with the SSH key provided during cluster creation.    Cluster  Ambari web UI  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.    Cluster  Web UIs for specific cluster services  Access with the credentials provided in the \u201cCluster User\u201d parameter during cluster creation.", 
            "title": "Cloudbreak user accounts"
        }, 
        {
            "location": "/clusters-access/index.html#finding-cluster-information-in-the-web-ui", 
            "text": "Once your cluster is up and running, click on the tile representing your cluster in the Cloudbreak UI to access information related the cluster and access cluster actions.      The information presented includes:   Cluster summary     Cluster information        Event history      \n   Tips \n   \n    Access cluster actions such as resize and sync by clicking on  ACTIONS . \n    Access Ambari web UI by clicking on the link in the  CLUSTER INFORMATION  section.   View public IP addresses for all cluster instances in the  HARDWARE  section. Click on the links to view the instances in the cloud console.   The SSH user that you must use when accessing cluster VMs is \"cloudbreak\".", 
            "title": "Finding cluster information in the web UI"
        }, 
        {
            "location": "/clusters-access/index.html#cluster-summary", 
            "text": "The summary bar includes the following information about your cluster:     Item  Description      Cluster Name  The name that you selected for your cluster is displayed at the top of the page. Below it is the name of the cluster blueprint.    Time Remaining  If you enabled lifetime management for your cluster, the clock next to the cluster name indicates the amount of time that your cluster will run before it gets terminated. Note that the time remaining counter does not stop when you stop the cluster.    Cloud Provider  The logo of the cloud provider on which the cluster is running.    Credential  The name of the credential used to create the cluster.    Status  Current status. When a cluster is healthy, the status is  Running .    Nodes  The current number of cluster nodes, including the master node.    Uptime  The amount of time (HH:MM) that the cluster has been in the running state since it was started. Each time you stop and restart the cluster, the running time is reset to 0.    Created  The date when the cluster was created. The date format is Mon DD, YYYY. For example: Oct 27, 2017.", 
            "title": "Cluster summary"
        }, 
        {
            "location": "/clusters-access/index.html#cluster-information", 
            "text": "The following information is available on the cluster details page:      Item  Description      Cluster User  The name of the cluster user that you created when creating the cluster.    SSH Username  The SSH user which you must use when accessing cluster VMs via SSH. The SSH user is always \"cloudbreak\".    Ambari URL  Link to the Ambari web UI.    Region  The region in which the cluster is running in the cloud provider infrastructure.    Availability Zone  The availability zone within the region in which the cluster is running.    Blueprint  The name of the blueprint selected under \"Cluster Type\" to create this cluster.    Created With  The version of Cloudbreak used to create this cluster.    Ambari Version  The Ambari version which this cluster is currently running.    HDP/HDF Version  The HDP or HDF version which this cluster is currently running.    Authentication Source  If you are using an external authentication source (LDAP/AD) for your cluster, you can see it here. Refer to  Using an external authentication source .     Below this, you will see additional tabs that you can click on in order to see their content:     Item  Description      Hardware  This section includes information about your cluster instances: instance names, instance IDs, instance types, their status, fully qualified domain names (FQDNs), and private and public IPs.    Cloud storage  If you configured any cloud storage options, you will see them listed here.    Tags  This section lists keys and values of the user-defined tags, in the same order as you added them.    Gateway  This section is available when gateway is configured for a cluster. It includes the gateway URL to Ambari and the URLs for the service UIs.    Recipes  This section includes recipe-related information. For each recipe, you can see the host group on which a recipe was executed, recipe name, and recipe type. Refer to  Using custom scripts (recipes) .    External databases  If you are using an external database for your cluster, you can see it here. Refer to  using an external database .    Repository details  This section includes Ambari and HDP/HDF repository information, as you provided it in the \"Base Images\" section when creating a cluster.    Image details  This section includes information about the base image that was used for the Cloudbreak instance.    Network  This section includes information about the names of the network and subnet in which the cluster is running and the links to related cloud provider console.    Security  This section is only available if you have enabled Kerberos security. It provides you with the details of your Kerberos configuration.    Autoscaling  This section includes configuration options related to autoscaling. Refer to  Configuring autoscaling .", 
            "title": "Cluster information"
        }, 
        {
            "location": "/clusters-access/index.html#event-history", 
            "text": "The Event History section shows you events logged for the cluster, with the most recent event at the top. For example, after your cluster has been created, the following messages will be written to the log:  \nAmbari cluster built; Ambari ip:34.215.103.66\n10/26/2017, 9:41:58 AM\nBuilding Ambari cluster; Ambari ip:34.215.103.66\n10/26/2017, 9:30:20 AM\nStarting Ambari cluster services\n10/26/2017, 9:27:12 AM\nSetting up infrastructure metadata\n10/26/2017, 9:27:11 AM\nBootstrapping infrastructure cluster\n10/26/2017, 9:26:38 AM\nInfrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nBilling started, Infrastructure successfully provisioned\n10/26/2017, 9:26:37 AM\nInfrastructure metadata collection finished\n10/26/2017, 9:25:39 AM\nInfrastructure creation took 194 seconds\n10/26/2017, 9:25:37 AM\nCreating infrastructure\n10/26/2017, 9:22:22 AM\nSetting up HDP image\n10/26/2017, 9:22:21 AM", 
            "title": "Event history"
        }, 
        {
            "location": "/clusters-access/index.html#accessing-cluster-via-ssh", 
            "text": "If you plan to access the cluster via the command line clients, SSH into the master node instance in the cluster.    In order to use SSH, you must generate an SSH key pair or use an existing SSH key pair.    You can find the cluster instance public IP addresses on the cluster details page.    When accessing instances via SSH use the  cloudbreak  user.    On Mac OS, you can use the following syntax to SSH to the VM: ssh -i \"privatekey.pem\" cloudbreak@publicIP \nFor example: ssh -i \"dominika-kp.pem\" cloudbreak@p52.25.169.132  On Windows, you can SSH using an SSH client such as PuTTY.", 
            "title": "Accessing cluster via SSH"
        }, 
        {
            "location": "/clusters-access/index.html#access-ambari", 
            "text": "You can access Ambari web UI by clicking on the links provided in the  Cluster Information     Ambari URL .  Steps    From the cluster dashboard, click on the tile representing your cluster to navigate to cluster details.    Find the Ambari URL in the  Cluster Information  section. This URL is available once the Ambari cluster creation process has completed.      Click on the  Ambari URL  link.    The first time you access the server, your browser will attempt to confirm that the SSL Certificate is valid. Since Cloudbreak automatically generates a self-signed certificate, your browser will warn you about an Untrusted Connection and ask you to confirm a Security Exception. Depending on your browser, perform the steps below to proceed.     Browser  Steps      Firefox  Click  Advanced    Click  Add Exception...    Click  Confirm Security Exception    Safari  Click  Continue    Chrome  Click  Advanced    Click  Proceed...", 
            "title": "Access Ambari"
        }, 
        {
            "location": "/clusters-access/index.html#view-cluster-blueprints", 
            "text": "Cloudbreak includes a useful option to view blueprints of a future cluster (from the create cluster wizard) or an existing cluster (from cluster details):    To view a cluster blueprint from the create cluster wizard, on the last page of the wizard, select  Show blueprint .        To view a cluster blueprint for an existing cluster, navigate to cluster details, and from the  ACTIONS  menu, select  Show blueprint .", 
            "title": "View cluster blueprints"
        }, 
        {
            "location": "/clusters-manage/index.html", 
            "text": "Managing and monitoring clusters\n\n\nYou can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner: \n\n\n \n\n\n\n  \nTips\n\n  \n\n  \nTo add or remove nodes from your cluster click \nACTIONS>Resize\n.\n\n  \nTo synchronize your cluster with the cloud provider account click \nACTIONS>Sync\n.\n\n  \nTo temporarily stop your cluster click \nSTOP\n.\n\n  \nTo terminate your cluster click \nTERMINATE\n.\n\n\n\n\n\n\n\n\n\nRetry a cluster\n\n\nWhen stack provisioning or cluster creation failure occurs, the \"retry\" option allows you to resume the process from the last failed step. \n\n\nIn some cases the cause of a failed stack provisioning or cluster creation can be eliminated by simply retrying the process. For example, in case of a temporary network outage, a retry should be successful. In other cases, a manual modification is required before a retry can succeed. For example, if you are using a custom image but some configuration is missing, causing the process to fail, you must log in to the machine and fix the issue; Only after that you can retry the the process.\n\n\nOnly failed stack or cluster creations can be retried. A retry can be initiated any number of times on a failed creation process. \n\n\nTo retry provisioning a failed stack or cluster, follow these steps.  \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nRetry\n. \n\n\nOnly failed stack or cluster creations can be retried, so the option is only available in these two cases.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nThe operation continues from the last failed step. \n\n\n\n\n\n\nResize a cluster\n\n\nTo resize a cluster, follow these steps.\n\n\n\n\nCluster resizing is not supported for HDF clusters.\n\nTo configure automated cluster scaling, refer to \nConfigure autoscaling\n.   \n\n\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nResize\n. The cluster resize dialog is displayed.\n\n\n\n\n\n\nUsing the +/- controls, adjust the number of nodes for a chosen host group. \n\n\n\n\nYou can only modify one host group at a time. \n\nIt is not possible to resize the Ambari server host group.     \n\n\n\n\n\n\n\n\nClick \nYes\n to confirm the scale-up/scale-down.\n\n\nWhile nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\". \n\n\n\n\n\n\nSynchronize a cluster\n\n\nUse the \nsync\n option if you:  \n\n\n\n\nMade changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.  \n\n\nManually changed service status in Ambari (for example, restarted services).   \n\n\n\n\nTo synchronize your cluster with the cloud provider, follow these steps. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nActions\n and select \nSync\n.\n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\". \n\n\n\n\n\n\nStop a cluster\n\n\nCloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Cloudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nStop\n to stop a currently running cluster.  \n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\n\n\n\n\nYour cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a \nStart\n option to restart your cluster. \n\n\n\n\n\n\nWhen a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.  \n\n\nRestart a cluster\n\n\nIf your cluster is in the \"Stopped\" state, you can restart the cluster by following these steps.\n\n\nSteps\n\n\n\n\n\n\nclick \nStart\n. This option is only available when the cluster has been stopped. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nYour cluster status changes to \"Start in progress\" and then to \"Running\". \n\n\n\n\n\n\nTerminate a cluster\n\n\nTo terminate a cluster managed by Cloudbreak, use the option available from the Cloudbreak UI. \n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nClick \nYes\n to confirm.\n\n\nAll cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved. \n\n\n\n\n\n\nForce terminate a cluster\n\n\nCluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the \nTerminate\n \n \nForce terminate\n option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.\n\n\nSteps\n\n\n\n\n\n\nBrowse to the cluster details.\n\n\n\n\n\n\nClick \nTerminate\n. \n\n\n\n\n\n\nCheck  \nForce terminate\n.\n\n\n\n\n\n\nClick \nYes\n to confirm. \n\n\nWhen terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.\n\n\n\n\n\n\nThis deletes the cluster tile from the UI.  \n\n\n\n\n\n\nLog in to your cloud provider account and \nmanually delete\n any resources that failed to be deleted.\n\n\n\n\n\n\nView cluster history\n\n\nFrom the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.\n\n\nTo generate a report, follow these steps.\n\n\nSteps\n\n\n\n\n\n\nFrom the Cloudbreak UI navigation menu, select \nHistory\n.\n\n\n\n\n\n\nOn the History page, select the range of dates and click \nShow History\n to generate a tabular report for the selected period.\n\n\n\n\n\n\nHistory report content\n\n\nEach entry in the report represents one cluster instance group. For each entry, the report includes the following information:\n\n\n\n\nCreated\n - The date when your cluster was created (YYYY-MM-DD).\n\n\nProvider\n - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.\n\n\nCluster Name\n - The name that you selected for the cluster.  \n\n\nInstance Group\n - The name of the host group.   \n\n\nInstance Count\n - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.\n\n\nInstance Type\n - Provider-specific VM type of the cluster instances. \n\n\nRegion\n - The AWS region in which your cluster is/was running.\n\n\nAvailability Zone\n - The availability zone in which your cluster is/was running.      \n\n\nRunning Time (hours)\n - The sum of the running times for all the nodes in the instance group.\n\n\n\n\nThe \nAGGREGATE RUNNING TIME\n is the sum of the Running Times, adjusted for the selected time range.\n\n\nTo learn about how your cloud provider bills you for the VMs, refer to their documentation:\n\n\n\n\nAWS\n      \n\n\nAzure\n     \n\n\nGCP", 
            "title": "Managing clusters"
        }, 
        {
            "location": "/clusters-manage/index.html#managing-and-monitoring-clusters", 
            "text": "You can manage monitor your clusters from the Cloudbreak UI. To do that, click on the tile representing the cluster that you want to access. The actions available for your cluster are listed in the top right corner:      \n   Tips \n   \n   To add or remove nodes from your cluster click  ACTIONS>Resize . \n   To synchronize your cluster with the cloud provider account click  ACTIONS>Sync . \n   To temporarily stop your cluster click  STOP . \n   To terminate your cluster click  TERMINATE .", 
            "title": "Managing and monitoring clusters"
        }, 
        {
            "location": "/clusters-manage/index.html#retry-a-cluster", 
            "text": "When stack provisioning or cluster creation failure occurs, the \"retry\" option allows you to resume the process from the last failed step.   In some cases the cause of a failed stack provisioning or cluster creation can be eliminated by simply retrying the process. For example, in case of a temporary network outage, a retry should be successful. In other cases, a manual modification is required before a retry can succeed. For example, if you are using a custom image but some configuration is missing, causing the process to fail, you must log in to the machine and fix the issue; Only after that you can retry the the process.  Only failed stack or cluster creations can be retried. A retry can be initiated any number of times on a failed creation process.   To retry provisioning a failed stack or cluster, follow these steps.    Steps    Browse to the cluster details.    Click  Actions  and select  Retry .   Only failed stack or cluster creations can be retried, so the option is only available in these two cases.      Click  Yes  to confirm.   The operation continues from the last failed step.", 
            "title": "Retry a cluster"
        }, 
        {
            "location": "/clusters-manage/index.html#resize-a-cluster", 
            "text": "To resize a cluster, follow these steps.   Cluster resizing is not supported for HDF clusters. \nTo configure automated cluster scaling, refer to  Configure autoscaling .      Steps    Browse to the cluster details.    Click  Actions  and select  Resize . The cluster resize dialog is displayed.    Using the +/- controls, adjust the number of nodes for a chosen host group.    You can only modify one host group at a time.  \nIt is not possible to resize the Ambari server host group.          Click  Yes  to confirm the scale-up/scale-down.  While nodes are being added or removed, cluster status changes to \"Update In Progress\". Once the operation has completed, cluster status changes back to \"Running\".", 
            "title": "Resize a cluster"
        }, 
        {
            "location": "/clusters-manage/index.html#synchronize-a-cluster", 
            "text": "Use the  sync  option if you:     Made changes on your cloud provider side (for example, deleted cluster VMs) and you would like to synchronize Cloudbreak with the cloud provider.    Manually changed service status in Ambari (for example, restarted services).      To synchronize your cluster with the cloud provider, follow these steps.   Steps    Browse to the cluster details.    Click  Actions  and select  Sync .    Click  Yes  to confirm.  Your cluster infrastructure is synchronized based on changes on the cloud provider. The updates are written to the \"Event History\".", 
            "title": "Synchronize a cluster"
        }, 
        {
            "location": "/clusters-manage/index.html#stop-a-cluster", 
            "text": "Cloudbreak supports stopping and restarting clusters. To stop and restart a cluster managed by Cloudbreak, use the options available from the Cloudbreak UI.   Steps    Browse to the cluster details.    Click  Stop  to stop a currently running cluster.      Click  Yes  to confirm.     Your cluster status changes to \"Stopping in progress\" and then to \"Stopped\". Once stopping the infrastructure has completed, you will see a  Start  option to restart your cluster.     When a cluster is in the \"stopped\" state, you are not charged for the VMs, but you are charged for external storage.", 
            "title": "Stop a cluster"
        }, 
        {
            "location": "/clusters-manage/index.html#restart-a-cluster", 
            "text": "If your cluster is in the \"Stopped\" state, you can restart the cluster by following these steps.  Steps    click  Start . This option is only available when the cluster has been stopped.     Click  Yes  to confirm.  Your cluster status changes to \"Start in progress\" and then to \"Running\".", 
            "title": "Restart a cluster"
        }, 
        {
            "location": "/clusters-manage/index.html#terminate-a-cluster", 
            "text": "To terminate a cluster managed by Cloudbreak, use the option available from the Cloudbreak UI.   Steps    Browse to the cluster details.    Click  Terminate .     Click  Yes  to confirm.  All cluster-related resources will be deleted, unless the resources (such as networks and subnets) existed prior to cluster creation or are used by other VMs in which case they will be preserved.", 
            "title": "Terminate a cluster"
        }, 
        {
            "location": "/clusters-manage/index.html#force-terminate-a-cluster", 
            "text": "Cluster deletion may fail if Cloudbreak is unable to delete one or more of the cloud resources that were part of your cluster infrastructure. In such as case, you can use the  Terminate     Force terminate  option to remove the cluster entry from the Cloudbreak web UI, but you must also check your cloud provider account to see if there are any resources that must be deleted manually.  Steps    Browse to the cluster details.    Click  Terminate .     Check   Force terminate .    Click  Yes  to confirm.   When terminating a cluster with Kerberos enabled, you have an option to disable Kerberos prior to cluster termination. This option removes any cluster-related principals from the KDC.    This deletes the cluster tile from the UI.      Log in to your cloud provider account and  manually delete  any resources that failed to be deleted.", 
            "title": "Force terminate a cluster"
        }, 
        {
            "location": "/clusters-manage/index.html#view-cluster-history", 
            "text": "From the navigation menu in the Cloudbreak UI, you can access the History page that allows you to generate a report showing basic information related to the clusters that were running within the specified range of dates.  To generate a report, follow these steps.  Steps    From the Cloudbreak UI navigation menu, select  History .    On the History page, select the range of dates and click  Show History  to generate a tabular report for the selected period.", 
            "title": "View cluster history"
        }, 
        {
            "location": "/clusters-manage/index.html#history-report-content", 
            "text": "Each entry in the report represents one cluster instance group. For each entry, the report includes the following information:   Created  - The date when your cluster was created (YYYY-MM-DD).  Provider  - The name of the cloud provider (AWS, Azure, Google, or OpenStack) on which the cluster instances are/were running.  Cluster Name  - The name that you selected for the cluster.    Instance Group  - The name of the host group.     Instance Count  - The number of nodes in the host group. This number may be a decimal if a cluster has been resized.  Instance Type  - Provider-specific VM type of the cluster instances.   Region  - The AWS region in which your cluster is/was running.  Availability Zone  - The availability zone in which your cluster is/was running.        Running Time (hours)  - The sum of the running times for all the nodes in the instance group.   The  AGGREGATE RUNNING TIME  is the sum of the Running Times, adjusted for the selected time range.  To learn about how your cloud provider bills you for the VMs, refer to their documentation:   AWS         Azure        GCP", 
            "title": "History report content"
        }, 
        {
            "location": "/hive/index.html", 
            "text": "Access Hive via JDBC\n\n\nHive can be accessed via JDBC through the \ngateway\n that is automatically installed and configured in your cluster. If your cluster configuration includes Hive LLAP, then Hive LLAP is configured with the gateway; otherwise, HiveServer2 is configured. In either case, the transport mode is \u201chttp\u201d and the gateway path to Hive is \n\"${CLUSTER_NAME}/${TOPOLOGY_NAME}/hive\"\n (for example \"test-cluster/db-proxy/hive\").\n\n\nBefore you can start using Hive JDBC, you must download the SSL certificate to your truststore. After downloading the SSL certificate, the Hive JDBC endpoint is:\n\n\njdbc:hive2://{GATEWAY_HOST}:8443/;ssl=true;sslTrustStore=gateway.jks;trustStorePassword={GATEWAY_JKS_PASSWORD};transportMode=http;httpPath={CLUSTER_NAME}/{TOPOLOGY_NAME}/hive\n\n\n\nDownload SSL Certificate\n\n\nBy default, the gateway has been configured with a \nself-signed certificate\n to protect the Hive endpoint via SSL. Therefore, in order to use Hive via JDBC or Beeline client, you \nmust download the SSL certificate\n from the \ngateway\n and add it to your truststore.\n\n\nSteps\n \n\n\nOn \nLinux or OSX\n, you can download the self-signed SSL certificate by using the following commands:\n\n\nexport GATEWAY_HOST=IP_OF_GATEWAY_NODE\nexport GATEWAY_JKS_PASSWORD=GATEWAY_PASSWORD\nopenssl s_client -servername ${GATEWAY_HOST} -connect ${GATEWAY_HOST}:8443 -showcerts \n/dev/null | openssl x509 -outform PEM \n gateway.pem\nkeytool -import -alias gateway-identity -file gateway.pem -keystore gateway.jks -storepass ${GATEWAY_JKS_PASSWORD}\n\n\n\nWhere:\n\nGATEWAY_HOST - Set this to the IP address of the instance on which gateway is running (Ambari server node).\n\nGATEWAY_JKS_PASSWORD - Create a password for the truststore that will hold the \nself-signed certificate\n. The password must be at least 6 characters long.\n\n\nFor example:\n\n\nexport GATEWAY_HOST=2-52-86-252-73\nexport GATEWAY_JKS_PASSWORD=Hadoop123!\nopenssl s_client -servername ${GATEWAY_HOST} -connect ${GATEWAY_HOST}:8443 -showcerts \n/dev/null | openssl x509 -outform PEM \n gateway.pem\nkeytool -import -alias gateway-identity -file gateway.pem -keystore gateway.jks -storepass ${GATEWAY_JKS_PASSWORD}\n\n\n\nAfter executing these commands, \ngateway.pem\n and \ngateway.jks\n files will be downloaded onto your computer to the location where you ran the commands.\n\n\nExamples\n\n\nHere are two examples of using tools to connect to Hive via JDBC:\n\n\n\n\nSQL Workbench/J\n  \n\n\nTableau\n  \n\n\n\n\nExample: SQL Workbench/J\n\n\nSQL Workbench/J\n is a cross-platform SQL tool that can be used to access database systems. In this example, we will provide a high-level overview of the steps required to setup SQL Workbench to access Hive via JDBC.\n\n\nPrerequisite:\n \nDownload the SSL Certificate\n \n\n\nStep 1: Install SQL Workbench and Hive JDBC Driver\n\n\n\n\nDownload and install \nSQL Workbench\n. Refer to \nhttp://www.sql-workbench.net/getting-started.html\n. \n\n\nDownload the \nHortonworks JDBC Driver for Apache Hive\n from \nhttps://hortonworks.com/downloads/#addons\n. Next, extract the driver package.  \n\n\n\n\nStep 2: Configure SQL Workbench with Hive JDBC Driver\n\n\n\n\nLaunch SQL Workbench. \n\n\nThe \nSelect Connection Profile\n window should be open by default. If it is not, you can open it from \nFile\n \n \nConnect window\n.  \n\n\nClick \nManage Drivers\n. The \nManage drivers\n window will appear.\n\n\nClick \n to create a new driver, and enter the \nName\n: \u201cHortonworks Hive JDBC\u201d.\n\n\nClick \n and then browse to the Hortonworks JDBC Driver for Apache Hive package that you downloaded earlier. Next, select the JDBC Driver JAR files in the package.\n\n\nWhen prompted, select the \u201ccom.simba.hive.jdbc41.HS2Driver\u201d driver.  \n\n\nFor the \nSample URL\n, enter: \njdbc:hive2://${GATEWAY_HOST}:8443/\n  \n\n\n\n\nAfter performing these steps, your configuration should look similar to: \n\n\n \n\n\n\n\n\n\nClick \nOK\n to save the driver.  \n\n\n\n\n\n\nStep 2: Create a Connection to Hive\n\n\n\n\nFrom the \nSelect Connection Profile\n window, select the \u201cHortonworks Hive JDBC\" from the \nDriver\n dropdown.  \n\n\nFor \nURL\n , enter the URL to the cluster instance where gateway is installed, such as \njdbc:hive2://52.52.98.57:8443/\n (where \n52.52.98.57\n is the public hostname of your gateway node).  \n\n\nFor \nUsername\n and \nPassword\n, enter the credentials that you created when creating your cluster.\n\n\n\n\nClick \nExtended Properties\n and add the following properties:\n\n\n\n\n\n\n\n\nProperty\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nssl\n\n\n1\n\n\n\n\n\n\ntransportMode\n\n\nhttp\n\n\n\n\n\n\nhttpPath\n\n\nProvide \n\"${CLUSTER_NAME}/${TOPOLOGY_NAME}/hive\"\n. For example \nhive-test/db-proxy/hive\n\n\n\n\n\n\nsslTrustStore\n\n\nEnter the path to the \ngateway.jks\n file. This file was generated when you \ndownloaded the SSL certificate\n.\n\n\n\n\n\n\ntrustStorePassword\n\n\nEnter the GATEWAY_JKS_PASSWORD that you specified when you \ndownloaded the SSL certificate\n.\n\n\n\n\n\n\n\n\nAfter performing these steps, your configuration should look similar to:\n\n\n \n\n\n\n\n\n\nClick \nOK\n to save the properties.\n\n\n\n\nClick \nTest\n to confirm a connection can be established.\n\n\nClick \nOK\n to make the connection and start using SQL Workbench to query Hive.\n\n\n\n\nExample: Tableau\n\n\nTableau\n is a business intelligence tool for interacting with and visualizing data via SQL. Connecting Tableau to Hive requires the use of an ODBC driver. In this example, we will provide high-level steps required to set up Tableau to access Hive.\n\n\nPrerequisite:\n \nDownload the SSL Certificate\n  \n\n\nStep 1: Install ODBC Driver\n\n\n\n\nDownload the \nHortonworks ODBC Driver for Apache Hive\n from \nhttps://hortonworks.com/downloads/#addons\n.    2. Next, extract and install the driver.  \n\n\n\n\nStep 2: Launch Tableau and Connect to Hive\n\n\n\n\nLaunch Tableau. If you do not already have Tableau, you can download a trial version from \nhttps://www.tableau.com/trial/download-tableau\n.\n\n\n\n\nIn Tableau, create a connection to a \u201cHortonworks Hadoop Hive\u201d server. Enter the following:\n\n\n\n\n\n\n\n\nProperty\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nServer\n\n\nEnter the public hostname of your controller node instance.\n\n\n\n\n\n\nPort\n\n\n8443\n\n\n\n\n\n\nType\n\n\nHiveServer2\n\n\n\n\n\n\nAuthentication\n\n\nUsername and Password\n\n\n\n\n\n\nTransport\n\n\nHTTP\n\n\n\n\n\n\nUsername\n\n\nEnter the cluster username created when creating your cluster\n\n\n\n\n\n\nPassword\n\n\nEnter the cluster password created when creating your cluster\n\n\n\n\n\n\nHTTP Path\n\n\nProvide \n\"${CLUSTER_NAME}/${TOPOLOGY_NAME}/hive\"\n. For example \nhive-test/db-proxy/hive\n\n\n\n\n\n\n\n\n\n\n\n\nCheck the \nRequire SSL\n checkbox.  \n\n\n\n\nClick on the text underneath the checkbox to \nadd a configuration file\n link.  \n\n\n\n\nSpecify to use a custom SSL certificate, and then browse to the SSL certificate \ngateway.pem\n file that was generated when you \ndownloaded the SSL certificate\n as a prerequisite. \n\n\n \n\n\n\n\n\n\nClick \nOK\n.  \n\n\nAfter performing these steps, your configuration should look similar to:\n\n\n \n\n\n\n\n\n\nClick \nSign In\n and you will be connected to Hive.", 
            "title": "Accessing Hive via JDBC"
        }, 
        {
            "location": "/hive/index.html#access-hive-via-jdbc", 
            "text": "Hive can be accessed via JDBC through the  gateway  that is automatically installed and configured in your cluster. If your cluster configuration includes Hive LLAP, then Hive LLAP is configured with the gateway; otherwise, HiveServer2 is configured. In either case, the transport mode is \u201chttp\u201d and the gateway path to Hive is  \"${CLUSTER_NAME}/${TOPOLOGY_NAME}/hive\"  (for example \"test-cluster/db-proxy/hive\").  Before you can start using Hive JDBC, you must download the SSL certificate to your truststore. After downloading the SSL certificate, the Hive JDBC endpoint is:  jdbc:hive2://{GATEWAY_HOST}:8443/;ssl=true;sslTrustStore=gateway.jks;trustStorePassword={GATEWAY_JKS_PASSWORD};transportMode=http;httpPath={CLUSTER_NAME}/{TOPOLOGY_NAME}/hive", 
            "title": "Access Hive via JDBC"
        }, 
        {
            "location": "/hive/index.html#download-ssl-certificate", 
            "text": "By default, the gateway has been configured with a  self-signed certificate  to protect the Hive endpoint via SSL. Therefore, in order to use Hive via JDBC or Beeline client, you  must download the SSL certificate  from the  gateway  and add it to your truststore.  Steps    On  Linux or OSX , you can download the self-signed SSL certificate by using the following commands:  export GATEWAY_HOST=IP_OF_GATEWAY_NODE\nexport GATEWAY_JKS_PASSWORD=GATEWAY_PASSWORD\nopenssl s_client -servername ${GATEWAY_HOST} -connect ${GATEWAY_HOST}:8443 -showcerts  /dev/null | openssl x509 -outform PEM   gateway.pem\nkeytool -import -alias gateway-identity -file gateway.pem -keystore gateway.jks -storepass ${GATEWAY_JKS_PASSWORD}  Where: \nGATEWAY_HOST - Set this to the IP address of the instance on which gateway is running (Ambari server node). \nGATEWAY_JKS_PASSWORD - Create a password for the truststore that will hold the  self-signed certificate . The password must be at least 6 characters long.  For example:  export GATEWAY_HOST=2-52-86-252-73\nexport GATEWAY_JKS_PASSWORD=Hadoop123!\nopenssl s_client -servername ${GATEWAY_HOST} -connect ${GATEWAY_HOST}:8443 -showcerts  /dev/null | openssl x509 -outform PEM   gateway.pem\nkeytool -import -alias gateway-identity -file gateway.pem -keystore gateway.jks -storepass ${GATEWAY_JKS_PASSWORD}  After executing these commands,  gateway.pem  and  gateway.jks  files will be downloaded onto your computer to the location where you ran the commands.", 
            "title": "Download SSL Certificate"
        }, 
        {
            "location": "/hive/index.html#examples", 
            "text": "Here are two examples of using tools to connect to Hive via JDBC:   SQL Workbench/J     Tableau", 
            "title": "Examples"
        }, 
        {
            "location": "/hive/index.html#example-sql-workbenchj", 
            "text": "SQL Workbench/J  is a cross-platform SQL tool that can be used to access database systems. In this example, we will provide a high-level overview of the steps required to setup SQL Workbench to access Hive via JDBC.  Prerequisite:   Download the SSL Certificate    Step 1: Install SQL Workbench and Hive JDBC Driver   Download and install  SQL Workbench . Refer to  http://www.sql-workbench.net/getting-started.html .   Download the  Hortonworks JDBC Driver for Apache Hive  from  https://hortonworks.com/downloads/#addons . Next, extract the driver package.     Step 2: Configure SQL Workbench with Hive JDBC Driver   Launch SQL Workbench.   The  Select Connection Profile  window should be open by default. If it is not, you can open it from  File     Connect window .    Click  Manage Drivers . The  Manage drivers  window will appear.  Click   to create a new driver, and enter the  Name : \u201cHortonworks Hive JDBC\u201d.  Click   and then browse to the Hortonworks JDBC Driver for Apache Hive package that you downloaded earlier. Next, select the JDBC Driver JAR files in the package.  When prompted, select the \u201ccom.simba.hive.jdbc41.HS2Driver\u201d driver.    For the  Sample URL , enter:  jdbc:hive2://${GATEWAY_HOST}:8443/      After performing these steps, your configuration should look similar to:        Click  OK  to save the driver.      Step 2: Create a Connection to Hive   From the  Select Connection Profile  window, select the \u201cHortonworks Hive JDBC\" from the  Driver  dropdown.    For  URL  , enter the URL to the cluster instance where gateway is installed, such as  jdbc:hive2://52.52.98.57:8443/  (where  52.52.98.57  is the public hostname of your gateway node).    For  Username  and  Password , enter the credentials that you created when creating your cluster.   Click  Extended Properties  and add the following properties:     Property  Value      ssl  1    transportMode  http    httpPath  Provide  \"${CLUSTER_NAME}/${TOPOLOGY_NAME}/hive\" . For example  hive-test/db-proxy/hive    sslTrustStore  Enter the path to the  gateway.jks  file. This file was generated when you  downloaded the SSL certificate .    trustStorePassword  Enter the GATEWAY_JKS_PASSWORD that you specified when you  downloaded the SSL certificate .     After performing these steps, your configuration should look similar to:       Click  OK  to save the properties.   Click  Test  to confirm a connection can be established.  Click  OK  to make the connection and start using SQL Workbench to query Hive.", 
            "title": "Example: SQL Workbench/J"
        }, 
        {
            "location": "/hive/index.html#example-tableau", 
            "text": "Tableau  is a business intelligence tool for interacting with and visualizing data via SQL. Connecting Tableau to Hive requires the use of an ODBC driver. In this example, we will provide high-level steps required to set up Tableau to access Hive.  Prerequisite:   Download the SSL Certificate     Step 1: Install ODBC Driver   Download the  Hortonworks ODBC Driver for Apache Hive  from  https://hortonworks.com/downloads/#addons .    2. Next, extract and install the driver.     Step 2: Launch Tableau and Connect to Hive   Launch Tableau. If you do not already have Tableau, you can download a trial version from  https://www.tableau.com/trial/download-tableau .   In Tableau, create a connection to a \u201cHortonworks Hadoop Hive\u201d server. Enter the following:     Property  Value      Server  Enter the public hostname of your controller node instance.    Port  8443    Type  HiveServer2    Authentication  Username and Password    Transport  HTTP    Username  Enter the cluster username created when creating your cluster    Password  Enter the cluster password created when creating your cluster    HTTP Path  Provide  \"${CLUSTER_NAME}/${TOPOLOGY_NAME}/hive\" . For example  hive-test/db-proxy/hive       Check the  Require SSL  checkbox.     Click on the text underneath the checkbox to  add a configuration file  link.     Specify to use a custom SSL certificate, and then browse to the SSL certificate  gateway.pem  file that was generated when you  downloaded the SSL certificate  as a prerequisite.        Click  OK .    After performing these steps, your configuration should look similar to:       Click  Sign In  and you will be connected to Hive.", 
            "title": "Example: Tableau"
        }, 
        {
            "location": "/cli-install/index.html", 
            "text": "Installing Cloudbreak CLI\n\n\nThe Cloudbreak Command Line Interface (CLI) is a tool to help you manage your Cloudbreak cluster instances. This tool can be used to interact with Cloudbreak for automating cluster creation, management, monitoring, and termination. \n\n\nThe CLI is available for Linux, Mac OS X, and Windows. \n\n\nInstall the CLI\n\n\nAfter you have launched Cloudbreak, the CLI is available for download from that Cloudbreak instance.\n\n\nSteps\n\n\n\n\nBrowse to your Cloudbreak instance and log in to the Cloudbreak web UI.  \n\n\nSelect \nDownload CLI\n from the navigation pane. \n\n\nSelect your operating system. The CLI is available for Linux, Mac OS X, and Windows.   \n\n\nDownload the selected bundle to your local machine.  \n\n\nExtract the bundle.  \n\n\nYou can optionally add \ncb\n to your system path.\n\n\n\n\nRun the executable to verify the CLI: \n\n\ncb --version\n\n\n\n\n\n\nConfigure the CLI\n\n\nOnce you have installed the CLI, you need to configure the CLI to work with Cloudbreak.\n\n\nSteps\n\n\n\n\n\n\nUse the \ncb configure\n command to set up the CLI configuration file. The configuration options are:  \n\n\n\n\n--server\n server address [$CB_SERVER_ADDRESS]  \n\n\n--username\n user name (e-mail address) [$CB_USER_NAME]  \n\n\n--password\n password [$CB_PASSWORD]  \n\n\n\n\nThe password configuration is optional. If you do not provide the password, no password is stored in the CLI configuration file. Therefore, you will need to provide the password with each command you execute or via an environment variable.\n\n\nFor example:\n\n\ncb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com\n\n\n\n\n\n\nThe CLI configuration file will be saved at \n~/.cb/config\n. The content will look similar to the following:\n\n\ndefault:\n  username: admin@hortonworks.com\n  server: https://ec2-11-111-111-11.compute-1.amazonaws.com\n\n\n\n\n\n\nRun any command to verify that you can connect to the Cloudbreak instance via CLI. For example:\n\n\ncb cluster list\n  \n\n\n\n\n\n\n\n    \nConfiguration precedence\n\n    \n\n    The CLI can look for configuration options from different locations. You can optionally\n    pass the configuration options on each command or from environment variables. The following\n    order is used for the CLI to look for configuration options: \nCommand line\n, \nEnvironment variables\n\n    and the \nConfiguration file\n.\n    \n\n\n\n\n\nAdd multiple configurations\n\n\nIf you are using multiple profiles for multiple environments, you can configure them using the \ncb configure\n command and passing the name of your environment-specific profile file using the \n--profile\n parameter. After running the command, the configuration will be added as a new entry to the \nconfig\n file. For example, running the following command \ncb configure --server https://192.167.65.4 --username test@hortonworks.com --profile staging\n will add the \"staging\" entry:\n\n\ndefault:\n  username: admin@hortonworks.com\n  server: https://192.167.65.4\nstaging:\n  username: test@hortonworks.com\n  server: https://192.167.65.4  \n\n\n\n\nFor example:\n\n\n#cb configure --server https://192.167.65.4 --username test@hortonworks.com --profile staging\nINFO:  [writeConfigToFile] dir already exists: /Users/rkovacs/.cb\nINFO:  [writeConfigToFile] writing credentials to file: /Users/rkovacs/.cb/config\n# cat /Users/rkovacs/.cb/config\ndefault:\n  username: admin@example.com\n  server: https://192.167.65.4\n  output: table\nstaging:\n  username: test@hortonworks.com\n  server: https://192.167.65.4\n\n\n\nConfigure default output\n\n\nBy default, JSON format is used in command output. For example, if you run \ncb list-clusters\n without specifying output type, the output will be JSON. If you would like to change default output, add it to the config file. For example:\n\n\ndefault:\n  username: admin@hortonworks.com\n  server: https://192.167.65.4\n  output: table\n\n\n\nConfigure CLI autocomplete\n\n\nThe CLI includes an autocomplete option. Before you can use this option, you must download and source one of the following files:\n\n\n\n\nFor \nbash\n: bash_autocomplete  \n\n\nFor \nzsh\n: zsh_autocomplete  \n\n\n\n\nThese two files are located in \nhttps://github.com/hortonworks/cb-cli/tree/master/autocomplete/\n  \n\n\nOnce you've sourced the file, type the CLI commands as usual and use the \nTab\n key to access the autocomplete feature.  \n\n\n\n\nNext: Get Started with CLI", 
            "title": "Install the CLI"
        }, 
        {
            "location": "/cli-install/index.html#installing-cloudbreak-cli", 
            "text": "The Cloudbreak Command Line Interface (CLI) is a tool to help you manage your Cloudbreak cluster instances. This tool can be used to interact with Cloudbreak for automating cluster creation, management, monitoring, and termination.   The CLI is available for Linux, Mac OS X, and Windows.", 
            "title": "Installing Cloudbreak CLI"
        }, 
        {
            "location": "/cli-install/index.html#install-the-cli", 
            "text": "After you have launched Cloudbreak, the CLI is available for download from that Cloudbreak instance.  Steps   Browse to your Cloudbreak instance and log in to the Cloudbreak web UI.    Select  Download CLI  from the navigation pane.   Select your operating system. The CLI is available for Linux, Mac OS X, and Windows.     Download the selected bundle to your local machine.    Extract the bundle.    You can optionally add  cb  to your system path.   Run the executable to verify the CLI:   cb --version", 
            "title": "Install the CLI"
        }, 
        {
            "location": "/cli-install/index.html#configure-the-cli", 
            "text": "Once you have installed the CLI, you need to configure the CLI to work with Cloudbreak.  Steps    Use the  cb configure  command to set up the CLI configuration file. The configuration options are:     --server  server address [$CB_SERVER_ADDRESS]    --username  user name (e-mail address) [$CB_USER_NAME]    --password  password [$CB_PASSWORD]     The password configuration is optional. If you do not provide the password, no password is stored in the CLI configuration file. Therefore, you will need to provide the password with each command you execute or via an environment variable.  For example:  cb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com    The CLI configuration file will be saved at  ~/.cb/config . The content will look similar to the following:  default:\n  username: admin@hortonworks.com\n  server: https://ec2-11-111-111-11.compute-1.amazonaws.com    Run any command to verify that you can connect to the Cloudbreak instance via CLI. For example:  cb cluster list       \n     Configuration precedence \n     \n    The CLI can look for configuration options from different locations. You can optionally\n    pass the configuration options on each command or from environment variables. The following\n    order is used for the CLI to look for configuration options:  Command line ,  Environment variables \n    and the  Configuration file .", 
            "title": "Configure the CLI"
        }, 
        {
            "location": "/cli-install/index.html#add-multiple-configurations", 
            "text": "If you are using multiple profiles for multiple environments, you can configure them using the  cb configure  command and passing the name of your environment-specific profile file using the  --profile  parameter. After running the command, the configuration will be added as a new entry to the  config  file. For example, running the following command  cb configure --server https://192.167.65.4 --username test@hortonworks.com --profile staging  will add the \"staging\" entry:  default:\n  username: admin@hortonworks.com\n  server: https://192.167.65.4\nstaging:\n  username: test@hortonworks.com\n  server: https://192.167.65.4    For example:  #cb configure --server https://192.167.65.4 --username test@hortonworks.com --profile staging\nINFO:  [writeConfigToFile] dir already exists: /Users/rkovacs/.cb\nINFO:  [writeConfigToFile] writing credentials to file: /Users/rkovacs/.cb/config\n# cat /Users/rkovacs/.cb/config\ndefault:\n  username: admin@example.com\n  server: https://192.167.65.4\n  output: table\nstaging:\n  username: test@hortonworks.com\n  server: https://192.167.65.4", 
            "title": "Add multiple configurations"
        }, 
        {
            "location": "/cli-install/index.html#configure-default-output", 
            "text": "By default, JSON format is used in command output. For example, if you run  cb list-clusters  without specifying output type, the output will be JSON. If you would like to change default output, add it to the config file. For example:  default:\n  username: admin@hortonworks.com\n  server: https://192.167.65.4\n  output: table", 
            "title": "Configure default output"
        }, 
        {
            "location": "/cli-install/index.html#configure-cli-autocomplete", 
            "text": "The CLI includes an autocomplete option. Before you can use this option, you must download and source one of the following files:   For  bash : bash_autocomplete    For  zsh : zsh_autocomplete     These two files are located in  https://github.com/hortonworks/cb-cli/tree/master/autocomplete/     Once you've sourced the file, type the CLI commands as usual and use the  Tab  key to access the autocomplete feature.     Next: Get Started with CLI", 
            "title": "Configure CLI autocomplete"
        }, 
        {
            "location": "/cli-get-started/index.html", 
            "text": "Getting started with the CLI\n\n\nGet started with the CLI\n\n\nAfter \ninstalling\n and \nconfiguring\n the CLI, you can use it to perform the same tasks as are available in the Cloudbreak UI: create and manage clusters, credentials, blueprints, and recipes.\n\n\nSteps\n\n\n\n\n\n\nBefore you start using the CLI, familiarize yourself with Cloudbreak concepts and the Cloudbreak web UI. \n\n\n\n\n\n\nIf you haven't already, create at least one Cloudbreak credential by using Cloudbreak UI or the \ncredential create\n command. \n\n\n\n\n\n\nTo create a cluster, you must first generate a JSON skeleton. Although it is possible to generate it by using the \ncluster generate-template\n command, it is easiest to obtain it from the Cloudbreak UI, as described in \nObtain cluster JSON template from the UI\n.\n\n\n\n\n\n\nSave the template in the JSON format and edit it if needed.\n\n\n\n\n\n\nOnce your JSON file is ready, you can use it to create a cluster via the \ncluster create\n command.\n\n\n\n\n\n\nOnce your cluster is running, use can use the CLI to manage and monitor your cluster. For a full list of commands, refer to \nCLI reference\n.    \n\n\n\n\n\n\nObtain cluster JSON template from the UI\n\n\nThe simplest way to obtain a valid JSON template for your cluster is to get it from the Cloudbreak UI. You can do this in two ways:\n\n\nFrom create cluster\n\n\nOnce you've provided all the cluster parameters, on the last page of the create cluster wizard, click \nShow CLI Command\n to obtain the JSON template:\n\n\n    \n\n\nClick \nCopy the JSON\n to copy the content and then use a text editor to edit and save it. \n\n\nFrom cluster details\n\n\nYou can obtain the JSON template for a cluster from the cluster details page by selecting \nActions\n \n \nShow CLI Command\n. This option is available for all clusters that have been initiated, so the cluster does not need to be in the running state to obtain this information. In fact, this option is useful when troubleshooting cluster failures.  \n\n\n   \n\n\nClick \nCopy the JSON\n to copy the content and then use a text editor to edit and save it. \n\n\n \n\n\nObtain CLI command from the UI\n\n\nCloudbreak web UI includes an option in the UI which allows you to generate the  \ncreate\n command for resources such as credentials, blueprints, clusters, and recipes. This option is available when creating a resource and for existing resources, from the resource details page.   \n\n\nFrom Create Resource\n\n\nWhen creating a resource (credential, blueprint, cluster, or recipe), provide all information and then click \nShow CLI Command\n. The UI will display the \ncreate\n CLI command for the resource.\n\n\nFrom Resource Details\n\n\nNavigate to credential, blueprint, cluster, or recipe details and  click \nShow CLI Command\n. The UI will display the \ncreate\n CLI command for the resource.\n\n\nGet help\n\n\nTo get CLI help, you can add help to the end of a command. The following will list help for the CLI at the top-level:\n\n\ncb --help\n\n\n\nor \n\n\ncb --h\n\n\n\nThe following will list help for the create-cluster command, including its command options and global options:\n\n\ncb cluster --help\n\n\n\nor\n\n\ncb cluster --h\n\n\n\n\n\nNext: CLI Reference", 
            "title": "Get started with the CLI"
        }, 
        {
            "location": "/cli-get-started/index.html#getting-started-with-the-cli", 
            "text": "", 
            "title": "Getting started with the CLI"
        }, 
        {
            "location": "/cli-get-started/index.html#get-started-with-the-cli", 
            "text": "After  installing  and  configuring  the CLI, you can use it to perform the same tasks as are available in the Cloudbreak UI: create and manage clusters, credentials, blueprints, and recipes.  Steps    Before you start using the CLI, familiarize yourself with Cloudbreak concepts and the Cloudbreak web UI.     If you haven't already, create at least one Cloudbreak credential by using Cloudbreak UI or the  credential create  command.     To create a cluster, you must first generate a JSON skeleton. Although it is possible to generate it by using the  cluster generate-template  command, it is easiest to obtain it from the Cloudbreak UI, as described in  Obtain cluster JSON template from the UI .    Save the template in the JSON format and edit it if needed.    Once your JSON file is ready, you can use it to create a cluster via the  cluster create  command.    Once your cluster is running, use can use the CLI to manage and monitor your cluster. For a full list of commands, refer to  CLI reference .", 
            "title": "Get started with the CLI"
        }, 
        {
            "location": "/cli-get-started/index.html#obtain-cluster-json-template-from-the-ui", 
            "text": "The simplest way to obtain a valid JSON template for your cluster is to get it from the Cloudbreak UI. You can do this in two ways:  From create cluster  Once you've provided all the cluster parameters, on the last page of the create cluster wizard, click  Show CLI Command  to obtain the JSON template:        Click  Copy the JSON  to copy the content and then use a text editor to edit and save it.   From cluster details  You can obtain the JSON template for a cluster from the cluster details page by selecting  Actions     Show CLI Command . This option is available for all clusters that have been initiated, so the cluster does not need to be in the running state to obtain this information. In fact, this option is useful when troubleshooting cluster failures.         Click  Copy the JSON  to copy the content and then use a text editor to edit and save it.", 
            "title": "Obtain cluster JSON template from the UI"
        }, 
        {
            "location": "/cli-get-started/index.html#obtain-cli-command-from-the-ui", 
            "text": "Cloudbreak web UI includes an option in the UI which allows you to generate the   create  command for resources such as credentials, blueprints, clusters, and recipes. This option is available when creating a resource and for existing resources, from the resource details page.     From Create Resource  When creating a resource (credential, blueprint, cluster, or recipe), provide all information and then click  Show CLI Command . The UI will display the  create  CLI command for the resource.  From Resource Details  Navigate to credential, blueprint, cluster, or recipe details and  click  Show CLI Command . The UI will display the  create  CLI command for the resource.", 
            "title": "Obtain CLI command from the UI"
        }, 
        {
            "location": "/cli-get-started/index.html#get-help", 
            "text": "To get CLI help, you can add help to the end of a command. The following will list help for the CLI at the top-level:  cb --help  or   cb --h  The following will list help for the create-cluster command, including its command options and global options:  cb cluster --help  or  cb cluster --h   Next: CLI Reference", 
            "title": "Get help"
        }, 
        {
            "location": "/cli-reference/index.html", 
            "text": "Cloudbreak CLI reference\n\n\nThis section will help you get started with the Cloudbreak CLI after you have \ninstalled and configured it\n.\n\n\nCommand structure\n\n\nThe CLI command can contain multiple parts. The first part is a set of global options. The next part is the command. The next part is a set of command options and arguments which could include sub-commands.\n\n\ncb [global options] command [command options] [arguments...]\n\n\n\nCommand output\n\n\nYou can control the output from the CLI using the --output argument. The possible output formats include:\n\n\n\n\nJSON (\njson\n)\n\n\nYAML (\nyaml\n)\n\n\nFormatted table (\ntable\n)\n\n\n\n\nFor example:\n\n\ncb cluster list --output json\n\n\n\ncb clusters list --output yaml\n\n\n\ncb cluster list --output table\n\n\n\nCommands\n\n\nConfigure CLI:  \n\n\n\n\nconfigure\n  \n\n\n\n\nCloud provider:\n\n\n\n\ncloud availability-zones\n  \n\n\ncloud regions\n       \n\n\ncloud volumes\n   \n\n\ncloud instances\n   \n\n\n\n\nCredential:  \n\n\n\n\ncredential create\n   \n\n\ncredential delete\n        \n\n\ncredential describe\n        \n\n\ncredential list\n         \n\n\ncredential modify\n   \n\n\n\n\nBlueprint:   \n\n\n\n\nblueprint create\n  \n\n\nblueprint delete\n           \n\n\nblueprint describe\n         \n\n\nblueprint list\n           \n\n\n\n\nCluster: \n\n\n\n\ncluster change-ambari-password\n   \n\n\ncluster create\n \n\n\ncluster delete\n              \n\n\ncluster describe\n    \n\n\ncluster generate-template\n   \n\n\ncluster generate-reinstall-template\n  \n\n\ncluster list\n    \n\n\ncluster repair\n    \n\n\ncluster retry\n         \n\n\ncluster scale\n      \n\n\ncluster start\n        \n\n\ncluster stop\n              \n\n\ncluster sync\n \n\n\n\n\nDatabase:            \n\n\n\n\ndatabase create\n  \n\n\ndatabase delete\n  \n\n\ndatabase list\n  \n\n\ndatabase test\n  \n\n\n\n\nImage catalog\n\n\n\n\nimagecatalog create\n     \n\n\nimagecatalog delete\n \n\n\nimagecatalog images\n                      \n\n\nimagecatalog list\n  \n\n\nimagecatalog set-default\n            \n\n\n\n\nLDAP:\n\n\n\n\nldap create\n  \n\n\nldap delete\n  \n\n\nldap list\n\n\n\n\nMpack\n\n\n\n\nmpack create\n  \n\n\nmpack delete\n  \n\n\nmpack list\n\n\n\n\nProxy:\n\n\n\n\nproxy create\n  \n\n\nproxy delete\n  \n\n\nproxy list\n\n\n\n\nRecipe:  \n\n\n\n\nrecipe create\n     \n\n\nrecipe delete\n            \n\n\nrecipe describe\n            \n\n\nrecipe list\n              \n\n\n\n\n\n\nblueprint create\n\n\nAdds a new blueprint from a file or from a URL.\n\n\nSub-commands\n\n\nfrom-url\n Creates a blueprint by downloading it from a URL location\n\n\nfrom-file\n Creates a blueprint by reading it from a local file\n\n\nRequired options\n\n\nfrom-url\n \n\n\n--name \nvalue\n Name for the blueprint\n\n\n--url \nvalue\n URL location of the Ambari blueprint JSON file\n\n\nfrom-file\n \n\n\n--name \nvalue\n Name for the blueprint\n\n\n--file \nvalue\n Location of the Ambari blueprint JSON file on the local machine\n\n\nOptions\n\n\n--description \nvalue\n  Description of the resource\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\n--public\n   Public in account  \n\n\nExamples\n\n\nAdds a blueprint from a URL:\n\n\ncb blueprint create from-url --url https://someurl.com/test.bp --name test1\n\n\n\nAdds a blueprint from a local file:\n\n\ncb blueprint create from-file --file /Users/test/Documents/blueprints/test.bp --name test2\n\n\n\nRelated links\n\n\nUsing Custom Blueprints\n\n\n\n\nblueprint delete\n\n\nDeletes an existing blueprint.\n\n\nRequired options\n\n\n--name \nvalue\n Blueprint name \n\n\nOptions\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE] \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\ncb blueprint delete --name \"testbp\"\n\n\n\n\n\nblueprint describe\n\n\nDescribes an existing blueprint.\n\n\nRequired options\n\n\n--name \nvalue\n Blueprint name \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\ncb blueprint describe --name \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\"\n{\n  \"Name\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n  \"Description\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n  \"HDPVersion\": \"2.6\",\n  \"HostgroupCount\": \"3\",\n  \"Tags\": \"DEFAULT\"\n}\n\n\n\n\n\nblueprint list\n\n\nLists available blueprints.\n\n\nRequired options\n\n\nNome\n\n\nOptions\n\n\n--output \nvalue\n Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT] \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\ncb blueprint list\n[\n  {\n    \"Name\": \"EDW-Analytics: Apache Hive 2 LLAP, Apache Zeppelin\",\n    \"Description\": \"Useful for EDW analytics using Hive LLAP\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"Data Science: Apache Spark 2, Apache Zeppelin\",\n    \"Description\": \"Useful for data science with Spark and Zeppelin\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"EDW-ETL: Apache Hive, Apache Spark 2\",\n    \"Description\": \"Useful for ETL data processing with Hive and Spark\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"Flow Management: Apache NiFi\",\n    \"Description\": \"Useful for data-flow management with Apache NiFi\",\n    \"HDPVersion\": \"3.1\",\n    \"HostgroupCount\": \"2\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"my-hdf-test\",\n    \"Description\": \"\",\n    \"HDPVersion\": \"3.1\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"USER_MANAGED\"\n }\n]\n\n\n\n\n\ncloud availability-zones\n\n\nLists all availability zones available in the specified cloud provider region. \n\n\nRequired options\n\n\n--credential \nvalue\n  Name of the credential\n\n\n--region \nvalue\n   Name of the region  \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD] \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE] \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\nLists availability zones in the us-west-2 (Oregon) region on the AWS account identified by the credential called \"aws-cred\":\n\n\ncb cloud availability-zones --credential aws-cred --region us-west-2\n[\n  {\n    \"Name\": \"us-west-2a\"\n  },\n  {\n    \"Name\": \"us-west-2b\"\n  },\n  {\n    \"Name\": \"us-west-2c\"\n  }\n]\n\n\n\n\n\ncloud regions\n\n\nLists the available cloud provider regions. \n\n\nRequired options\n\n\n--credential \nvalue\n  Name of the credential  \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT] \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE] \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\nLists regions available on the AWS account identified by the credential called \"aws-cred\":\n\n\ncb cloud regions --credential aws-cred\n[\n  {\n    \"Name\": \"ap-northeast-1\",\n    \"Description\": \"Asia Pacific (Tokyo)\"\n  },\n  {\n    \"Name\": \"ap-northeast-2\",\n    \"Description\": \"Asia Pacific (Seoul)\"\n  },\n  ...\n\n\n\n\n\ncloud volumes\n\n\nLists the available cloud provider volume types. \n\n\nSub-commands\n\n\naws\n     Lists the available aws volume types\n\n\nazure\n   Lists the available azure volume types\n\n\ngcp\n     Lists the available gcp volume types  \n\n\nRequired options\n\n\nNone\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE] \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\nLists volumes available on AWS:\n\n\ncb cloud volumes aws\n[\n  {\n    \"Name\": \"ephemeral\",\n    \"Description\": \"Ephemeral\"\n  },\n  {\n    \"Name\": \"gp2\",\n    \"Description\": \"General Purpose (SSD)\"\n  },\n  {\n    \"Name\": \"st1\",\n    \"Description\": \"Throughput Optimized HDD\"\n  },\n  {\n    \"Name\": \"standard\",\n    \"Description\": \"Magnetic\"\n  }\n]\n\n\n\n\n\ncloud instances\n\n\nLists the available cloud provider instance types.\n\n\nRequired options\n\n\n--credential \nvalue\n  Name of the credential\n\n\n--region \nvalue\n   Name of the region  \n\n\nOptions\n\n\n--availability-zone \nvalue\n  Name of the availability zone \n\n\n--output \nvalue\n      Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n     Selects a config profile to use [$CB_PROFILE] \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    \n\n\nExamples\n\n\nLists instance types available in the us-west-2 (Oregon) region on the AWS account identified by the credential called \"aws-cred\":\n\n\ncb cloud instances --credential aws-cred --region us-west-2\n\n  {\n    \"Name\": \"c3.2xlarge\",\n    \"Cpu\": \"8\",\n    \"Memory\": \"15.0\",\n    \"AvailabilityZone\": \"us-west-2b\"\n  },\n  {\n    \"Name\": \"c3.4xlarge\",\n    \"Cpu\": \"16\",\n    \"Memory\": \"30.0\",\n    \"AvailabilityZone\": \"us-west-2b\"\n  },\n  ...\n\n\n\n\n\ncluster change-ambari-password\n\n\nChanges Ambari password.\n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\n--old-password \nvalue\n Old Ambari password \n\n\n--new-password \nvalue\n  New Ambari password  \n\n\n--ambari-user \nvalue\n  Ambari user   \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    \n\n\nExamples\n\n\nChanges password for Ambari user called \"admin\" for a cluster called \"test1234\":\n\n\ncb cluster change-ambari-password --name test1234 --old-password 123456 --new-password Ambari123456 --ambari-user admin\n\n\n\n\n\ncluster create\n\n\nCreates a new cluster based on a JSON template. \n\n\nRequired options\n\n\n--cli-input-json \nvalue\n  User provided file in JSON format  \n\n\nOptions\n\n\n--name \nvalue\n  Name for the cluster\n\n\n--description \nvalue\n  Description of resource \n\n\n--input-json-param-password \nvalue\n  Password for the cluster and Ambari \n\n\n--wait\n  Wait for the operation to finish. No argument is required \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD] \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]   \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\n--public\n   Public in account  \n\n\nExamples\n\n\nCreates a cluster called \"testcluster\" based on a local JSON file called \"mytemplate.json\" located in the /Users/test/Documents directory:   \n\n\ncb cluster create --name testcluster --cli-input-json /Users/test/Documents/mytemplate.json\n\n\n\nRelated links\n\n\nObtain Cluster JSON Template from the UI\n   \n\n\n\n\ncluster delete\n\n\nDeletes an existing cluster. \n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--force\n  Force the operation\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE] \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\ncb cluster delete --name test1234\n\n\n\n\n\ncluster describe\n\n\nDescribes an existing cluster.\n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT] \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\nReturns a JSON file describing an existing cluster called \"test1234\":\n\n\n./cb cluster describe --name test1234\n\n\n\nThe command returns JSON output which due to space limitation was not captured in the example.\n\n\n\n\ncluster generate-template\n\n\nGenerates a provider-specific cluster template in JSON format.\n\n\nSub-commands\n\n\naws new-network\n Generates an AWS cluster JSON template with new network\n\n\naws existing-network\n Generates an AWS cluster JSON template with existing network\n\n\naws existing-subnet\n Generates an AWS cluster JSON template with existing network and subnet  \n\n\nazure new-network\n Generates an Azure cluster JSON template with new network\n\n\nazure existing-subnet\n Generates an Azure cluster JSON template with existing network and subnet  \n\n\ngcp new-network\n Generates an GCP cluster JSON template with new network\n\n\ngcp existing-network\n Generates an GCP cluster JSON template with existing network \n\n\ngcp existing-subnet\n Generates an GCP cluster JSON template with existing network and subnet\n\n\ngcp legacy-network\n Generates an GCP cluster JSON template with legacy network without subnets    \n\n\nopenstack new-network\n Generates an OS cluster JSON template with new network \n\n\nopenstack existing-network\n Generates an OS cluster JSON template with existing network \n\n\nopenstack existing-subnet\n Generates an OS cluster JSON template with existing network and subnet   \n\n\nExamples\n\n\ncb cluster generate-template aws new-network\n\n\n\nRelated Commands\n\n\nRelated Links\n\n\nObtain Cluster JSON Template from the UI\n   \n\n\n\n\ncluster generate-reinstall-template\n\n\nGenerates a cluster template that you can use to reinstall the cluster if installation went fail. \n\n\nRequired options\n\n\n--blueprint-name \nvalue\n  Name of the blueprint \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\ncb cluster generate-reinstall-template --blueprint-name \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 2.1\"\n\n\n\n\n\ncluster list\n\n\nLists all clusters which are currently associated with the Cloudbreak instance.\n\n\nRequired options\n\n\nNone\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nLists available clusters: \n\n\ncb cluster list\n[\n  {\n    \"Name\": \"test1234\",\n    \"Description\": \"\",\n    \"CloudPlatform\": \"AZURE\",\n    \"StackStatus\": \"UPDATE_IN_PROGRESS\",\n    \"ClusterStatus\": \"REQUESTED\"\n  }\n]\n\n\n\nLists available clusters, with output in a table format:\n\n\ncb cluster list --output table\n+----------+-------------+---------------+--------------------+---------------+\n|   NAME   | DESCRIPTION | CLOUDPLATFORM |    STACKSTATUS     | CLUSTERSTATUS |\n+----------+-------------+---------------+--------------------+---------------+\n| test1234 |             | AZURE         | UPDATE_IN_PROGRESS | REQUESTED     |\n+----------+-------------+---------------+--------------------+---------------+\n\n\n\n\n\ncluster repair\n\n\nRepairs a cluster if cluster installation failed by removing, or removing and replacing failed nodes. You must specify the cluster name and the host group with the failed nodes.  \n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\n--host-groups \nvalue\n Comma separated list of host groups where the failed nodes should be repaired  \n\n\nOptions\n\n\n--remove-only\n The failed nodes will be removed (rather than \"repaired\" by removing and replacing) \n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT] \n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\ncb cluster repair --name test1234 --host-groups master,worker\n\n\n\n\n\ncluster retry\n\n\nRetries the process if cluster or stack provisioning failed.\n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT] \n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\ncb cluster retry --name test1234\n\n\n\n\n\ncluster scale\n\n\nScales a cluster by adding or removing nodes.\n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\n--group-name \nvalue\n  Name of the group to scale\n\n\n--desired-node-count \nvalue\n  Desired number of nodes  \n\n\nOptions\n\n\n--wait\n  Wait for the operation to finish. No argument is required \n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\ncb cluster scale --name test1234 --group-name worker --desired node-count 3\n\n\n\n\n\ncluster start\n\n\nStarts a cluster which has previously been stopped.\n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   \n\n\nExamples\n\n\ncb cluster start --name test1234\n\n\n\n\n\ncluster stop\n\n\nStops a cluster.\n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--wait\n  Wait for the operation to finish. No argument is required\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\ncb cluster stop --name test1234\n\n\n\n\n\ncluster sync\n\n\nSynchronizes a cluster with the cloud provider.\n\n\nRequired options\n\n\n--name \nvalue\n  Cluster name\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\ncb cluster sync --name test1234\n\n\n\n\n\nconfigure\n\n\nConfigures the Cloudbreak server address and credentials used to communicate with this server.\n\n\nRequired options\n\n\n--server \nvalue\n  Server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n     User name (e-mail address) [$CB_USER_NAME]  \n\n\nOptions\n\n\n--password \nvalue\n  Password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Select a config profile to use [$CB_PROFILE] \n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nThis example configures the server address with username and password:\n\n\ncb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com --password MySecurePassword123\n\n\n\nThis example configures the server address with username but without a password:\n\n\ncb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com\n\n\n\nRelated links\n\n\nConfigure CLI\n\n\n\n\ncredential create\n\n\nCreates a new Cloudbreak credential.\n\n\nSub-commands\n\n\naws role-based\n  Creates a new AWS credential\n\n\naws key-based\n  Creates a new AWS credential\n\n\nazure app-based\n Creates a new app-based Azure credential\n\n\ngcp\n Creates a new gcp credential\n\n\nopenstack keystone-v2\n Creates a new OpenStack credential\n\n\nopenstack keystone-v3\n Creates a new OpenStack credential \n\n\nRequired options\n\n\naws role-based\n \n\n\n--name \nvalue\n  Name for the credential \n\n\n--role-arn \nvalue\n IAM Role ARN of the role used for Cloudbreak credential  \n\n\naws key-based\n \n\n\n--name \nvalue\n  Name for the credential  \n\n\n--access-key \nvalue\n  AWS Access Key\n\n\n--secret-key \nvalue\n  AWS Secret Key  \n\n\nazure app-based\n \n\n\n--name \nvalue\n  Name for the credential  \n\n\n--subscription-id \nvalue\n  Subscription ID from your Azure Subscriptions\n\n\n--tenant-id \nvalue\n  Directory ID from your Azure Active Directory \n Properties    \n\n\n--app-id \nvalue\n  Application ID of your app from your Azure Active Directory \n App Registrations          \n\n\n--app-password \nvalue\n  Your application key from app registration's Settings \n Keys  \n\n\ngcp\n \n\n\n--name \nvalue\n  Name for the credential  \n\n\n--project-id \nvalue\n  Project ID from your GCP account                      \n\n\n--service-account-id \nvalue\n  Your GCP Service account ID from IAM \n Admin \n Service accounts               \n\n\n--service-account-private-key-file \nvalue\n  P12 key from your GCP service account  \n\n\nopenstack keystone-v2\n  \n\n\n--name \nvalue\n  Name for the credential  \n\n\n--tenant-user \nvalue\n  OpenStack user name   \n\n\n--tenant-password \nvalue\n  OpenStack password\n\n\n--tenant-name \nvalue\n  OpenStack tenant name    \n\n\n--endpoint \nvalue\n   OpenStack endpoint   \n\n\nopenstack keystone-v3\n \n\n\n--name \nvalue\n  Name for the credential \n\n\n--tenant-user \nvalue\n  OpenStack user name   \n\n\n--tenant-password \nvalue\n  OpenStack password\n\n\n--user-domain \nvalue\n  OpenStack user domain    \n\n\n--endpoint \nvalue\n   OpenStack endpoint  \n\n\nOptions\n\n\n--description \nvalue\n  Description of the resource\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\n--public\n   Public in account  \n\n\nAdditionally, the following option is available for OpenStack Keystone2 and Keystone3:\n\n\n--facing \nvalue\n API facing. One of: public, admin, internal\n\n\nAdditionally, the following options are available for OpenStack Keystone3:\n\n\n--project-domain-name \nvalue\n  OpenStack project domain name  \n\n\n--project-name \nvalue\n  OpenStack project name         \n\n\n--domain-name \nvalue\n  OpenStack domain name  \n\n\n--keystone-scope \nvalue\n OpenStack keystone scope. One of: default, domain, project   \n\n\nExamples\n\n\nCreates a role-based credential on AWS:\n\n\ncb credential create aws role-based --name my-credential1 --role-arn arn:aws:iam::517127065441:role/CredentialRole\n\n\n\nCreates a key-based credential on AWS:\n\n\ncb credential create aws key-based --name my-credential2 --access-key ABDVIRDFV3K4HLJ45SKA --secret-key D89L5pOPM+426Rtj3curKzJEJL3lYoNcP8GvguBV\n\n\n\nCreates an app-based credential on Azure:\n\n\ncb credential create azure app-based --name my-credential3 --subscription-id b8e7379e-568g-55d3-na82-45b8d421e998 --tenant-id  c79n5399-3231-65ba-8dgg-2g4e2a40085e --app-id 6d147d89-48d2-5de2-eef8-b89775bbfcg1 --app-password 4a8hBgfI52s/C8R5Sea2YHGnBFrD3fRONfdG8w7F2Ua=\n\n\n\nCreates a credential on Google Cloud:\n\n\ncb credential create gcp --name my-credential4 --project-id test-proj --service-account-id test@test-proj.iam.gserviceaccount.com --service-account-private-key-file /Users/test/3fff57a6f68e.p12\n\n\n\nCreates a role-based credential on OpenStack with Keystone-v2:\n\n\ncb credential create openstack keystone-v2 --name my-credential5 --tenant-user test --tenant-password MySecurePass123 --tenant-name test --endpoint http://openstack.test.organization.com:5000/v2.0\n\n\n\nRelated links\n  \n\n\nCreate Credential on AWS\n\n\nCreate Credential on Azure\n\n\nCreate Credential on GCP\n\n\nCreate Credential on OpenStack\n  \n\n\n\n\ncredential delete\n\n\nDeletes an existing Cloudbreak credential.\n\n\nRequired options\n\n\n--name \nvalue\n  Credential name \n\n\nOptions\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\ncb credential delete --name test-cred\n\n\n\n\n\ncredential describe\n\n\nDescribes an existing credential.\n\n\nRequired options\n\n\n--name \nvalue\n  Credential name \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\ncb credential describe --name testcred\n{\n  \"Name\": \"testcred\",\n  \"Description\": \"\",\n  \"CloudPlatform\": \"AZURE\"\n}\n\n\n\n\n\ncredential list\n\n\nLists existing Cloudbreak credentials.\n\n\nRequired options\n\n\nNone\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nLists credentials:\n\n\ncb credential list \n[\n  {\n    \"Name\": \"testcred\",\n    \"Description\": \"\",\n    \"CloudPlatform\": \"AZURE\"\n  }\n]\n\n\n\n\nLists credentials, with output formatted in a table format:\n\n\ncb credential list --output table\n+---------+-------------+---------------+\n|  NAME   | DESCRIPTION | CLOUDPLATFORM |\n+---------+-------------+---------------+\n| armcred |             | AZURE         |\n+---------+-------------+---------------+\n\n\n\n\n\ncredential modify\n\n\nModifies an existing Cloudbreak credential. \n\n\n\n\nThe \n--name\n parameter is used to identify the credential that is being modified, and therefore its value cannot be modified.    \n\n\n\n\nSub-commands\n\n\naws role-based\n  Modifies an AWS role-based credential\n\n\naws key-based\n  Modifies an AWS key-based credential\n\n\nazure app-based\n Modifies an app-based Azure credential\n\n\ngcp\n Modifies a Google Cloud credential\n\n\nopenstack keystone-v2\n Modifies an OpenStack v2 credential\n\n\nopenstack keystone-v3\n Modifies an  OpenStack v3 credential \n\n\nRequired options\n\n\naws role-based\n \n\n\n--name \nvalue\n  Credential name\n\n\n--role-arn \nvalue\n IAM Role ARN of the role used for Cloudbreak credential  \n\n\naws key-based\n \n\n\n--name \nvalue\n  Credential name\n\n\n--access-key \nvalue\n  AWS Access Key\n\n\n--secret-key \nvalue\n  AWS Secret Key  \n\n\nazure app-based\n \n\n\n--name \nvalue\n  Credential name\n\n\n--subscription-id \nvalue\n  Subscription ID from your Azure Subscriptions\n\n\n--tenant-id \nvalue\n  Directory ID from your Azure Active Directory \n Properties    \n\n\n--app-id \nvalue\n  Application ID of your app from your Azure Active Directory \n App Registrations          \n\n\n--app-password \nvalue\n  Your application key from app registration's Settings \n Keys  \n\n\ngcp\n \n\n\n--name \nvalue\n  Credential name \n\n\n--project-id \nvalue\n  Project ID from your GCP account                      \n\n\n--service-account-id \nvalue\n  Your GCP Service account ID from IAM \n Admin \n Service accounts               \n\n\n--service-account-private-key-file \nvalue\n  P12 key from your GCP service account  \n\n\nopenstack keystone-v2\n  \n\n\n--name \nvalue\n  Credential name\n\n\n--tenant-user \nvalue\n  OpenStack user name   \n\n\n--tenant-password \nvalue\n  OpenStack password\n\n\n--tenant-name \nvalue\n  OpenStack tenant name    \n\n\n--endpoint \nvalue\n   OpenStack endpoint   \n\n\nopenstack keystone-v3\n \n\n\n--name \nvalue\n  Credential name\n\n\n--tenant-user \nvalue\n  OpenStack user name   \n\n\n--tenant-password \nvalue\n  OpenStack password\n\n\n--user-domain \nvalue\n  OpenStack user domain    \n\n\n--endpoint \nvalue\n   OpenStack endpoint  \n\n\nOptions\n\n\n--description \nvalue\n  Description of the resource\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    \n\n\nAdditionally, the following option is available for OpenStack Keystone2 and Keystone3:\n\n\n--facing \nvalue\n API facing. One of: public, admin, internal\n\n\nAdditionally, the following options are available for OpenStack Keystone3:\n\n\n--project-domain-name \nvalue\n  OpenStack project domain name  \n\n\n--project-name \nvalue\n  OpenStack project name         \n\n\n--domain-name \nvalue\n  OpenStack domain name  \n\n\n--keystone-scope \nvalue\n OpenStack keystone scope. One of: default, domain, project  \n\n\nExamples\n\n\nModifies a role-based AWS credential:\n\n\ncb credential modify aws role-based --name my-credential1 --role-arn arn:aws:iam::517127065441:role/CredentialRole\n\n\n\nModifies a key-based AWS credential:\n\n\ncb credential modify aws key-based --name my-credential2 --access-key ABDVIRDFV3K4HLJ45SKA --secret-key D89L5pOPM+426Rtj3curKzJEJL3lYoNcP8GvguBV\n\n\n\nModifies an app-based Azure credential:\n\n\ncb credential modify azure app-based --name my-credential3 --subscription-id b8e7379e-568g-55d3-na82-45b8d421e998 --tenant-id  c79n5399-3231-65ba-8dgg-2g4e2a40085e --app-id 6d147d89-48d2-5de2-eef8-b89775bbfcg1 --app-password 4a8hBgfI52s/C8R5Sea2YHGnBFrD3fRONfdG8w7F2Ua=\n\n\n\nModifies a Google Cloud credential:\n\n\ncb credential modify gcp --name my-credential4 --project-id test-proj --service-account-id test@test-proj.iam.gserviceaccount.com --service-account-private-key-file /Users/test/3fff57a6f68e.p12\n\n\n\nModifies a role-based OpenStack credential which uses Keystone-v2:\n\n\ncb credential modify openstack keystone-v2 --name my-credential5 --tenant-user test --tenant-password MySecurePass123 --tenant-name test --endpoint http://openstack.test.organization.com:5000/v2.0\n\n\n\n\n\ndatabase create\n\n\nRegisters an existing external database with Cloudbreak.\n\n\nSub-commands\n  \n\n\nmysql\n  Registers a MySQL database configuration\n\n\noracle11\n  Registers an Oracle 11 database configuration\n\n\noracle12\n  Registers an Oracle 12 database configuration\n\n\npostgres\n  Registers a Postgres database configuration  \n\n\nRequired options\n\n\n--name \nvalue\n   Name for the database   \n\n\n--db-username \nvalue\n  Username for the JDBC connection\n\n\n--db-password \nvalue\n  Password for the JDBC connection\n\n\n--url \nvalue\n  JDBC connection URL in the form of jdbc:db-type://address:port/db\n\n\n--type \nvalue\n   Name if the service that will use the database (AMBARI, DRUID, HIVE, OOZIE, RANGER, SUPERSET, or other custom type)    \n\n\nIf using MySQL and Oracle, the \n--connector-jar-url value \nvalue\n parameter is required in all cases except the following: If you are using a custom image and you already placed the JAR file on the machine, then this parameter is not required.\n\n\nOptions\n\n\n--description \nvalue\n  Description for the database     \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD] \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE] \n\n\n--public\n   Public in account  \n\n\nExamples\n\n\nRegisters an existing Postgres database called \"test-postgres\" with Cloudbreak:\n\n\ncb database create postgres --name testpostgres  --type HIVE --url jdbc:postgresql://test-db.cic6nusrpqec.us-west-2.rds.amazonaws.com:5432/testdb --db-username testuser --db-password MySecurePassword123\n\n\n\nThe connection URL includes three components db-type://address:port/db: \n\n\n\n\nDatabase type \"jdbc:postgresql\"  \n\n\nEndpoint \"test-db.cic6nusrpqec.us-west-2.rds.amazonaws.com:5432\"  \n\n\nPort \"5432\"  \n\n\nDatabase name \"testdb\"  \n\n\n\n\nRegisters an existing MySQL database called \"testmysql\" with Cloudbreak:  \n\n\ncb database create mysql --name testmysql --type OOZIE --url jdbc:mysql://test-db.cic6nusrpqec.us-west-2.rds.amazonaws.com:5432/testdb  --db-username test --db-password test --connector-jar-url http://example-page/driver-file.JAR\n\n\n\n\n\ndatabase delete\n\n\nUnregisters a previously registered database with Cloudbreak. It does not delete the database instance. \n\n\nRequired options\n\n\n--name \nvalue\n  Database registration name     \n\n\nOptions\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\nExamples\n\n\ncb database delete --name testdatabase\n\n\n\n\n\ndatabase list\n\n\nLists all available database registrations.\n\n\nRequired options\n\n\nNone\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\nExamples\n\n\nLists existing database registrations:\n\n\ncb database list\n\n\n\nLists existing database registrations, with output presented in a table format:\n\n\ncb database list --output table\n\n\n\n\n\ndatabase test\n\n\nTest database connection.\n\n\nSub-commands\n  \n\n\nby-name\n    Tests a stored database configuration identified by its name\n\n\nby-params\n  Tests database connection parameters   \n\n\nRequired options\n  \n\n\nby-name\n  \n\n\n--name \nvalue\n  Database registration name    \n\n\nby-params\n  \n\n\n--db-username \nvalue\n  Username to use for the JDBC connection\n\n\n--db-password \nvalue\n  Password to use for the JDBC connection\n\n\n--url \nvalue\n  JDBC connection URL in the form of jdbc:db-type://address:port/db\n\n\n--type \nvalue\n  Type of database (the service name that will use the database  \n\n\nOptions\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nTests connection to a previously registered database called \"testpostgres\":\n\n\ndatabase test --name testpostgres\n\n\n\nTests connection to a database based on connection parameters provided: \n\n\ncb database test by-params --type HIVE --url jdbc:postgresql://test-db.cic6nusrpqec.us-west-2.rds.amazonaws.com:5432/testdb --db-username testuser --db-password MySecurePassword123\n\n\n\n\n\nimagecatalog create\n\n\nRegisters a new custom image catalog based on the URL provided.  \n\n\nRequired options\n  \n\n\n--name \nvalue\n  Name for the image catalog\n\n\n--url \nvalue\n   URL location of the image catalog JSON file  \n\n\nOptions\n\n\n--description \nvalue\n  Description for the recipe \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\n--public\n   Public in account  \n\n\nExamples\n\n\nRegisters an image catalog called \"mycustomcatalog\" which is available at https://example.com/myimagecatalog.json: \n\n\ncb imagecatalog create --name mycustomcatalog --url https://example.com/myimagecatalog.json\n\n\n\nRelated links\n  \n\n\nCustom Images\n   \n\n\n\n\nimagecatalog delete\n\n\nDeletes a previously registered custom image catalog. It does not delete any cloud provider resources that you created as a prerequisite for creating the Cloudbreak credential.    \n\n\nRequired options\n  \n\n\n--name \nvalue\n  Image catalog name    \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nDeletes an image catalog called \"mycustomcatalog\":\n\n\ncb imagecatalog delete --name mycustomcatalog\n\n\n\nRelated links\n  \n\n\nCustom Images\n  \n\n\n\n\nimagecatalog images\n\n\nLists images from the specified image catalog available for the specified cloud provider.   \n\n\nSub-commands\n  \n\n\naws\n         Lists available aws images   \n\n\nazure\n       Lists available azure images   \n\n\ngcp\n         Lists available gcp images  \n\n\nopenstack\n   Lists available openstack images    \n\n\nRequired options\n\n\n--imagecatalog \nvalue\n  Name of the imagecatalog     \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]   \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE] \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n  \n\n\nReturns date, description, Ambari version, and image ID for all AWS images from an image catalog called \"myimagecatalog\":\n\n\n./cb imagecatalog images aws --imagecatalog cloudbreak-default\n[\n  {\n    \"Date\": \"2017-10-13\",\n    \"Description\": \"Cloudbreak official base image\",\n    \"Version\": \"2.6.0.0\",\n    \"ImageID\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n  },\n  {\n    \"Date\": \"2017-11-16\",\n    \"Description\": \"Official Cloudbreak image\",\n    \"Version\": \"2.6.0.0\",\n    \"ImageID\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n  }\n]\n\n\n\nRelated links\n  \n\n\nCustom Images\n  \n\n\n\n\nimagecatalog list\n\n\nLists default and custom image catalogs registered with Cloudbreak instance.   \n\n\nRequired options\n  \n\n\nNone  \n\n\nOptions\n  \n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT] \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nLists existing image catalogs:  \n\n\ncb  imagecatalog list \n[\n  {\n    \"Name\": \"mycustomcatalog\",\n    \"Default\": false,\n    \"URL\": \"https://example.com/imagecatalog.json\"\n  },\n  {\n    \"Name\": \"cloudbreak-default\",\n    \"Default\": true,\n    \"URL\": \"https://s3-eu-west-1.amazonaws.com/cloudbreak-info/v2-dev-cb-image-catalog.json\"\n  }\n]\n\n\n\nRelated links\n  \n\n\nCustom Images\n  \n\n\n\n\nimagecatalog set-default\n\n\nSets the specified image catalog as default.  \n\n\nRequired options\n   \n\n\n--name \nvalue\n  Image catalog name     \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]  \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nSets \"mycustomcatalog\" as default:  \n\n\nimagecatalog set-default --name mycustomcatalog\n\n\n\nRelated links\n  \n\n\nCustom Images\n  \n\n\n\n\nldap create\n\n\nRegisters an existing LDAP with Cloudbreak.\n\n\nRequired options\n\n\n--name \nvalue\n  Name for the LDAP     \n\n\n--ldap-server \nvalue\n  Address of the LDAP server (e.g. ldap://10.0.0.1:384)\n\n\n--ldap-domain \nvalue\n   LDAP domain (e.g. ad.cb.com)\n\n\n--ldap-bind-dn \nvalue\n  LDAP bind DN (e.g. CN=Administrator,CN=Users,DC=ad,DC=cb,DC=com)\n\n\n--ldap-bind-password \nvalue\n  LDAP bind password\n\n\n--ldap-directory-type \nvalue\n   LDAP directory type (LDAP or ACTIVE_DIRECTORY) \n\n\n--ldap-user-search-base \nvalue\n   LDAP user search base (e.g. CN=Users,DC=ad,DC=cb,DC=com)\n\n\n--ldap-user-name-attribute \nvalue\n   LDAP user name attribute\n\n\n--ldap-user-object-class \nvalue\n   LDAP user object class\n\n\n--ldap-group-member-attribute \nvalue\n LDAP group member attribute\n\n\n--ldap-group-name-attribute \nvalue\n  LDAP group name attribute\n\n\n--ldap-group-object-class \nvalue\n  LDAP group object class\n\n\n--ldap-group-search-base \nvalue\n   LDAP group search base (e.g. OU=scopes,DC=ad,DC=cb,DC=com)  \n\n\nOptions\n\n\n--ldap-admin-group \nvalue\n  LDAP group of administrators\n\n\n--description \nvalue\n  Description for the LDAP    \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD] \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\n--public\n   Public in account  \n\n\n\n\nldap delete\n\n\nDeletes selected LDAP registration from Cloudbreak. It does not delete the LDAP. \n\n\nRequired options\n\n\n--name \nvalue\n  LDAP name      \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\nExamples\n\n\ncb ldap delete --name testldap\n\n\n\n\n\nldap list\n\n\nLists all available LDAPs.\n\n\nRequired options\n\n\nNone\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\nExamples\n\n\nLists existing LDAPs:\n\n\ncb ldap list\n\n\n\nLists existing LDAPs, with output presented in a table format:\n\n\ncb ldap list --output table\n\n\n\n\n\nmpack create\n\n\nRegisters an existing management pack with Cloudbreak.\n\n\nRequired options\n\n\n--name \nvalue\n  Name for the mpack      \n\n\n--url \nvalue\n  URL that points to the location of the mpack tarball   \n\n\nOptions\n\n\n--purge\n  Purge existing resources specified in purge-list \n\n\n--purge-list \nvalue\n  Comma-separated list of resources to purge (stack-definitions,service-definitions,mpacks). By default (stack-definitions,mpacks) will be purged\n\n\n--force\n  Force install management pack  \n\n\n--description \nvalue\n  Description for the LDAP    \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD] \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\n--public\n   Public in account  \n\n\nExamples\n\n\nRegisters a new mpack without purging:\n\n\ncb mpack create --name test-hdp-search --url http://public-repo-1.hortonworks.com/HDP-SOLR/hdp-solr-ambari-mp/solr-service-mpack-3.0.0.tar.gz\n\n\n\n\n\nmpack delete\n\n\nDeletes selected management registration from Cloudbreak. It does not delete the management pack. \n\n\nRequired options\n\n\n--name \nvalue\n  Management pack name      \n\n\nOptions\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\nExamples\n\n\ncb mpack delete --name testmpack\n\n\n\n\n\nmpack list\n\n\nLists all available management packs.\n\n\nRequired options\n\n\nNone\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\nExamples\n\n\nLists all currently registered management packs and provides information about each:\n\n\ncb mpack list\n[\n  {\n    \"Name\": \"hdp-search-3\",\n    \"Description\": \"\",\n    \"URL\": \"http://public-repo-1.hortonworks.com/HDP-SOLR/hdp-solr-ambari-mp/solr-service-mpack-3.0.0.tar.gz\",\n    \"Purge\": \"false\",\n    \"PurgeList\": \"\",\n    \"Force\": \"false\"\n  }\n]\n\n\n\n\n\nproxy create\n\n\nRegisters an existing proxy with Cloudbreak.\n\n\nRequired options\n\n\n--name \nvalue\n  Name for the proxy   \n\n\n--proxy-host \nvalue\n  Hostname or IP address of the proxy\n\n\n--proxy-port \nvalue\n  Port of the proxy  \n\n\nOptions\n\n\n--proxy-protocol \nvalue\n  Protocol for the proxy (http or https) (default: \"http\")\n\n\n--proxy-user \nvalue\n   User for the proxy if basic auth is required\n\n\n--proxy-password \nvalue\n  Password for the proxy if basic auth is required\n\n\n--description \nvalue\n  Description for the proxy    \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD] \n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\n--public\n   Public in account  \n\n\n\n\nproxy delete\n\n\nUnregisters a previously registered proxy with Cloudbreak. It does not delete the proxy. \n\n\nRequired options\n\n\n--name \nvalue\n  Proxy registration name     \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\nExamples\n\n\ncb proxy delete --name testproxy\n\n\n\n\n\nproxy list\n\n\nLists all proxies that were previously registered with Cloudbreak.\n\n\nRequired options\n\n\nNone\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\nExamples\n\n\nLists existing proxy registrations:\n\n\ncb proxy list\n\n\n\nLists existing proxy registrations, with output presented in a table format:\n\n\ncb proxy list --output table\n\n\n\n\n\nrecipe create\n\n\nAdds a new recipe from a file or from a URL.\n\n\nSub-commands\n\n\nfrom-url\n  Creates a recipe by downloading it from a URL location\n\n\nfrom-file\n  Creates a recipe by reading it from a local file  \n\n\nRequired options\n\n\nfrom-url\n  \n\n\n--name \nvalue\n  Name for the recipe \n\n\n--execution-type \nvalue\n  Type of execution [pre-ambari-start, pre-termination, post-ambari-start, post-cluster-install]  \n\n\n--url \nvalue\n  URL location of the Ambari blueprint JSON file  \n\n\nfrom-file\n \n\n\n--name \nvalue\n  Name for the recipe\n\n\n--execution-type \nvalue\n  Type of execution [pre-ambari-start, pre-termination, post-ambari-start, post-cluster-install] \n\n\n--file \nvalue\n  Location of the Ambari blueprint JSON file  \n\n\nOptions\n\n\n--description \nvalue\n  Description for the recipe \n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]\n\n\n--public\n   Public in account  \n\n\nExamples\n\n\nAdds a new recipe called \"test1\" from a URL:\n\n\ncb recipe create from-url --name \"test1\" --execution-type post-ambari-start --url http://some-site.com/test.sh\n\n\n\nAdds a new recipe called \"test2\" from a file:\n\n\ncb recipe create from-url --name \"test2\" --execution-type post-ambari-start --file /Users/test/Documents/test.sh\n\n\n\nRelated links\n\n\nRecipes\n\n\n\n\nrecipe delete\n\n\nDeletes an existing recipe.\n\n\nRequired options\n\n\n--name \nvalue\n  Recipe name  \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE] \n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\ncb recipe delete --name test\n\n\n\n\n\nrecipe describe\n\n\nDescribes an existing recipe.\n\n\nRequired options\n\n\n--name \nvalue\n  Recipe name     \n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nDescribes a recipe called \"test\":\n\n\ncb recipe describe --name test\n{\n  \"Name\": \"test\",\n  \"Description\": \"\",\n  \"ExecutionType\": \"POST\"\n}\n\n\n\nDescribes a recipe called \"test\", with output presented in a table format:\n\n\ncb describe-recipe --name test --output table\n+------+-------------+----------------+\n| NAME | DESCRIPTION | EXECUTION TYPE |\n+------+-------------+----------------+\n| test |             | POST           |\n+------+-------------+----------------+\n\n\n\n\n\nrecipe list\n\n\nLists all available recipes.\n\n\nRequired options\n\n\nNone\n\n\nOptions\n\n\n--output \nvalue\n  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]\n\n\n--server \nvalue\n  Cloudbreak server address [$CB_SERVER_ADDRESS]\n\n\n--username \nvalue\n  Cloudbreak user name (e-mail address) [$CB_USER_NAME]\n\n\n--password \nvalue\n  Cloudbreak password [$CB_PASSWORD]\n\n\n--profile \nvalue\n  Selects a config profile to use [$CB_PROFILE]\n\n\n--auth-type \nvalue\n  Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  \n\n\nExamples\n\n\nLists existing recipes:\n\n\ncb recipe list\n[\n  {\n    \"Name\": \"test\",\n    \"Description\": \"\",\n    \"ExecutionType\": \"POST\"\n  }\n]\n\n\n\nLists existing recipes, with output presented in a table format:\n\n\ncb recipe list --output table\n+------+-------------+-------------------+\n| NAME | DESCRIPTION | EXECUTION TYPE    |\n+------+-------------+-------------------+\n| test |             | POST-AMBARI-START |\n+------+-------------+-------------------+\n\n\n\n\n\n\nDebugging\n\n\nTo use debugging mode, pass the \n--debug\n option. \n\n\nChecking CLI Version\n\n\nTo check CLI version, use \ncb --version\n.", 
            "title": "CLI Reference"
        }, 
        {
            "location": "/cli-reference/index.html#cloudbreak-cli-reference", 
            "text": "This section will help you get started with the Cloudbreak CLI after you have  installed and configured it .", 
            "title": "Cloudbreak CLI reference"
        }, 
        {
            "location": "/cli-reference/index.html#command-structure", 
            "text": "The CLI command can contain multiple parts. The first part is a set of global options. The next part is the command. The next part is a set of command options and arguments which could include sub-commands.  cb [global options] command [command options] [arguments...]", 
            "title": "Command structure"
        }, 
        {
            "location": "/cli-reference/index.html#command-output", 
            "text": "You can control the output from the CLI using the --output argument. The possible output formats include:   JSON ( json )  YAML ( yaml )  Formatted table ( table )   For example:  cb cluster list --output json  cb clusters list --output yaml  cb cluster list --output table", 
            "title": "Command output"
        }, 
        {
            "location": "/cli-reference/index.html#commands", 
            "text": "Configure CLI:     configure      Cloud provider:   cloud availability-zones     cloud regions          cloud volumes      cloud instances       Credential:     credential create      credential delete           credential describe           credential list            credential modify       Blueprint:      blueprint create     blueprint delete              blueprint describe            blueprint list               Cluster:    cluster change-ambari-password      cluster create    cluster delete                 cluster describe       cluster generate-template      cluster generate-reinstall-template     cluster list       cluster repair       cluster retry            cluster scale         cluster start           cluster stop                 cluster sync     Database:               database create     database delete     database list     database test      Image catalog   imagecatalog create        imagecatalog delete    imagecatalog images                         imagecatalog list     imagecatalog set-default                LDAP:   ldap create     ldap delete     ldap list   Mpack   mpack create     mpack delete     mpack list   Proxy:   proxy create     proxy delete     proxy list   Recipe:     recipe create        recipe delete               recipe describe               recipe list", 
            "title": "Commands"
        }, 
        {
            "location": "/cli-reference/index.html#blueprint-create", 
            "text": "Adds a new blueprint from a file or from a URL.  Sub-commands  from-url  Creates a blueprint by downloading it from a URL location  from-file  Creates a blueprint by reading it from a local file  Required options  from-url    --name  value  Name for the blueprint  --url  value  URL location of the Ambari blueprint JSON file  from-file    --name  value  Name for the blueprint  --file  value  Location of the Ambari blueprint JSON file on the local machine  Options  --description  value   Description of the resource  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  --public    Public in account    Examples  Adds a blueprint from a URL:  cb blueprint create from-url --url https://someurl.com/test.bp --name test1  Adds a blueprint from a local file:  cb blueprint create from-file --file /Users/test/Documents/blueprints/test.bp --name test2  Related links  Using Custom Blueprints", 
            "title": "blueprint create"
        }, 
        {
            "location": "/cli-reference/index.html#blueprint-delete", 
            "text": "Deletes an existing blueprint.  Required options  --name  value  Blueprint name   Options  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]   --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  cb blueprint delete --name \"testbp\"", 
            "title": "blueprint delete"
        }, 
        {
            "location": "/cli-reference/index.html#blueprint-describe", 
            "text": "Describes an existing blueprint.  Required options  --name  value  Blueprint name   Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  cb blueprint describe --name \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\"\n{\n  \"Name\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n  \"Description\": \"Data Science: Apache Spark 2.1, Apache Zeppelin 0.7.0\",\n  \"HDPVersion\": \"2.6\",\n  \"HostgroupCount\": \"3\",\n  \"Tags\": \"DEFAULT\"\n}", 
            "title": "blueprint describe"
        }, 
        {
            "location": "/cli-reference/index.html#blueprint-list", 
            "text": "Lists available blueprints.  Required options  Nome  Options  --output  value  Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]   --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value  Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  cb blueprint list\n[\n  {\n    \"Name\": \"EDW-Analytics: Apache Hive 2 LLAP, Apache Zeppelin\",\n    \"Description\": \"Useful for EDW analytics using Hive LLAP\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"Data Science: Apache Spark 2, Apache Zeppelin\",\n    \"Description\": \"Useful for data science with Spark and Zeppelin\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"EDW-ETL: Apache Hive, Apache Spark 2\",\n    \"Description\": \"Useful for ETL data processing with Hive and Spark\",\n    \"HDPVersion\": \"2.6\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"Flow Management: Apache NiFi\",\n    \"Description\": \"Useful for data-flow management with Apache NiFi\",\n    \"HDPVersion\": \"3.1\",\n    \"HostgroupCount\": \"2\",\n    \"Tags\": \"DEFAULT\"\n  },\n  {\n    \"Name\": \"my-hdf-test\",\n    \"Description\": \"\",\n    \"HDPVersion\": \"3.1\",\n    \"HostgroupCount\": \"3\",\n    \"Tags\": \"USER_MANAGED\"\n }\n]", 
            "title": "blueprint list"
        }, 
        {
            "location": "/cli-reference/index.html#cloud-availability-zones", 
            "text": "Lists all availability zones available in the specified cloud provider region.   Required options  --credential  value   Name of the credential  --region  value    Name of the region    Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]   --profile  value   Selects a config profile to use [$CB_PROFILE]   --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  Lists availability zones in the us-west-2 (Oregon) region on the AWS account identified by the credential called \"aws-cred\":  cb cloud availability-zones --credential aws-cred --region us-west-2\n[\n  {\n    \"Name\": \"us-west-2a\"\n  },\n  {\n    \"Name\": \"us-west-2b\"\n  },\n  {\n    \"Name\": \"us-west-2c\"\n  }\n]", 
            "title": "cloud availability-zones"
        }, 
        {
            "location": "/cli-reference/index.html#cloud-regions", 
            "text": "Lists the available cloud provider regions.   Required options  --credential  value   Name of the credential    Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]   --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]   --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  Lists regions available on the AWS account identified by the credential called \"aws-cred\":  cb cloud regions --credential aws-cred\n[\n  {\n    \"Name\": \"ap-northeast-1\",\n    \"Description\": \"Asia Pacific (Tokyo)\"\n  },\n  {\n    \"Name\": \"ap-northeast-2\",\n    \"Description\": \"Asia Pacific (Seoul)\"\n  },\n  ...", 
            "title": "cloud regions"
        }, 
        {
            "location": "/cli-reference/index.html#cloud-volumes", 
            "text": "Lists the available cloud provider volume types.   Sub-commands  aws      Lists the available aws volume types  azure    Lists the available azure volume types  gcp      Lists the available gcp volume types    Required options  None  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]   --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  Lists volumes available on AWS:  cb cloud volumes aws\n[\n  {\n    \"Name\": \"ephemeral\",\n    \"Description\": \"Ephemeral\"\n  },\n  {\n    \"Name\": \"gp2\",\n    \"Description\": \"General Purpose (SSD)\"\n  },\n  {\n    \"Name\": \"st1\",\n    \"Description\": \"Throughput Optimized HDD\"\n  },\n  {\n    \"Name\": \"standard\",\n    \"Description\": \"Magnetic\"\n  }\n]", 
            "title": "cloud volumes"
        }, 
        {
            "location": "/cli-reference/index.html#cloud-instances", 
            "text": "Lists the available cloud provider instance types.  Required options  --credential  value   Name of the credential  --region  value    Name of the region    Options  --availability-zone  value   Name of the availability zone   --output  value       Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value      Selects a config profile to use [$CB_PROFILE]   --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]      Examples  Lists instance types available in the us-west-2 (Oregon) region on the AWS account identified by the credential called \"aws-cred\":  cb cloud instances --credential aws-cred --region us-west-2\n\n  {\n    \"Name\": \"c3.2xlarge\",\n    \"Cpu\": \"8\",\n    \"Memory\": \"15.0\",\n    \"AvailabilityZone\": \"us-west-2b\"\n  },\n  {\n    \"Name\": \"c3.4xlarge\",\n    \"Cpu\": \"16\",\n    \"Memory\": \"30.0\",\n    \"AvailabilityZone\": \"us-west-2b\"\n  },\n  ...", 
            "title": "cloud instances"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-change-ambari-password", 
            "text": "Changes Ambari password.  Required options  --name  value   Cluster name  --old-password  value  Old Ambari password   --new-password  value   New Ambari password    --ambari-user  value   Ambari user     Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]      Examples  Changes password for Ambari user called \"admin\" for a cluster called \"test1234\":  cb cluster change-ambari-password --name test1234 --old-password 123456 --new-password Ambari123456 --ambari-user admin", 
            "title": "cluster change-ambari-password"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-create", 
            "text": "Creates a new cluster based on a JSON template.   Required options  --cli-input-json  value   User provided file in JSON format    Options  --name  value   Name for the cluster  --description  value   Description of resource   --input-json-param-password  value   Password for the cluster and Ambari   --wait   Wait for the operation to finish. No argument is required   --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]   --profile  value   Selects a config profile to use [$CB_PROFILE]     --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    --public    Public in account    Examples  Creates a cluster called \"testcluster\" based on a local JSON file called \"mytemplate.json\" located in the /Users/test/Documents directory:     cb cluster create --name testcluster --cli-input-json /Users/test/Documents/mytemplate.json  Related links  Obtain Cluster JSON Template from the UI", 
            "title": "cluster create"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-delete", 
            "text": "Deletes an existing cluster.   Required options  --name  value   Cluster name  Options  --force   Force the operation  --wait   Wait for the operation to finish. No argument is required  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]   --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  cb cluster delete --name test1234", 
            "title": "cluster delete"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-describe", 
            "text": "Describes an existing cluster.  Required options  --name  value   Cluster name  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]   --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  Returns a JSON file describing an existing cluster called \"test1234\":  ./cb cluster describe --name test1234  The command returns JSON output which due to space limitation was not captured in the example.", 
            "title": "cluster describe"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-generate-template", 
            "text": "Generates a provider-specific cluster template in JSON format.  Sub-commands  aws new-network  Generates an AWS cluster JSON template with new network  aws existing-network  Generates an AWS cluster JSON template with existing network  aws existing-subnet  Generates an AWS cluster JSON template with existing network and subnet    azure new-network  Generates an Azure cluster JSON template with new network  azure existing-subnet  Generates an Azure cluster JSON template with existing network and subnet    gcp new-network  Generates an GCP cluster JSON template with new network  gcp existing-network  Generates an GCP cluster JSON template with existing network   gcp existing-subnet  Generates an GCP cluster JSON template with existing network and subnet  gcp legacy-network  Generates an GCP cluster JSON template with legacy network without subnets      openstack new-network  Generates an OS cluster JSON template with new network   openstack existing-network  Generates an OS cluster JSON template with existing network   openstack existing-subnet  Generates an OS cluster JSON template with existing network and subnet     Examples  cb cluster generate-template aws new-network  Related Commands  Related Links  Obtain Cluster JSON Template from the UI", 
            "title": "cluster generate-template"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-generate-reinstall-template", 
            "text": "Generates a cluster template that you can use to reinstall the cluster if installation went fail.   Required options  --blueprint-name  value   Name of the blueprint   Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  cb cluster generate-reinstall-template --blueprint-name \"EDW-ETL: Apache Hive 1.2.1, Apache Spark 2.1\"", 
            "title": "cluster generate-reinstall-template"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-list", 
            "text": "Lists all clusters which are currently associated with the Cloudbreak instance.  Required options  None  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  Lists available clusters:   cb cluster list\n[\n  {\n    \"Name\": \"test1234\",\n    \"Description\": \"\",\n    \"CloudPlatform\": \"AZURE\",\n    \"StackStatus\": \"UPDATE_IN_PROGRESS\",\n    \"ClusterStatus\": \"REQUESTED\"\n  }\n]  Lists available clusters, with output in a table format:  cb cluster list --output table\n+----------+-------------+---------------+--------------------+---------------+\n|   NAME   | DESCRIPTION | CLOUDPLATFORM |    STACKSTATUS     | CLUSTERSTATUS |\n+----------+-------------+---------------+--------------------+---------------+\n| test1234 |             | AZURE         | UPDATE_IN_PROGRESS | REQUESTED     |\n+----------+-------------+---------------+--------------------+---------------+", 
            "title": "cluster list"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-repair", 
            "text": "Repairs a cluster if cluster installation failed by removing, or removing and replacing failed nodes. You must specify the cluster name and the host group with the failed nodes.    Required options  --name  value   Cluster name  --host-groups  value  Comma separated list of host groups where the failed nodes should be repaired    Options  --remove-only  The failed nodes will be removed (rather than \"repaired\" by removing and replacing)   --wait   Wait for the operation to finish. No argument is required  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  cb cluster repair --name test1234 --host-groups master,worker", 
            "title": "cluster repair"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-retry", 
            "text": "Retries the process if cluster or stack provisioning failed.  Required options  --name  value   Cluster name  Options  --wait   Wait for the operation to finish. No argument is required  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  cb cluster retry --name test1234", 
            "title": "cluster retry"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-scale", 
            "text": "Scales a cluster by adding or removing nodes.  Required options  --name  value   Cluster name  --group-name  value   Name of the group to scale  --desired-node-count  value   Desired number of nodes    Options  --wait   Wait for the operation to finish. No argument is required   --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  cb cluster scale --name test1234 --group-name worker --desired node-count 3", 
            "title": "cluster scale"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-start", 
            "text": "Starts a cluster which has previously been stopped.  Required options  --name  value   Cluster name  Options  --wait   Wait for the operation to finish. No argument is required  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]     Examples  cb cluster start --name test1234", 
            "title": "cluster start"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-stop", 
            "text": "Stops a cluster.  Required options  --name  value   Cluster name  Options  --wait   Wait for the operation to finish. No argument is required  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  cb cluster stop --name test1234", 
            "title": "cluster stop"
        }, 
        {
            "location": "/cli-reference/index.html#cluster-sync", 
            "text": "Synchronizes a cluster with the cloud provider.  Required options  --name  value   Cluster name  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  cb cluster sync --name test1234", 
            "title": "cluster sync"
        }, 
        {
            "location": "/cli-reference/index.html#configure", 
            "text": "Configures the Cloudbreak server address and credentials used to communicate with this server.  Required options  --server  value   Server address [$CB_SERVER_ADDRESS]  --username  value      User name (e-mail address) [$CB_USER_NAME]    Options  --password  value   Password [$CB_PASSWORD]  --profile  value   Select a config profile to use [$CB_PROFILE]   --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  This example configures the server address with username and password:  cb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com --password MySecurePassword123  This example configures the server address with username but without a password:  cb configure --server https://ec2-11-111-111-11.compute-1.amazonaws.com --username admin@hortonworks.com  Related links  Configure CLI", 
            "title": "configure"
        }, 
        {
            "location": "/cli-reference/index.html#credential-create", 
            "text": "Creates a new Cloudbreak credential.  Sub-commands  aws role-based   Creates a new AWS credential  aws key-based   Creates a new AWS credential  azure app-based  Creates a new app-based Azure credential  gcp  Creates a new gcp credential  openstack keystone-v2  Creates a new OpenStack credential  openstack keystone-v3  Creates a new OpenStack credential   Required options  aws role-based    --name  value   Name for the credential   --role-arn  value  IAM Role ARN of the role used for Cloudbreak credential    aws key-based    --name  value   Name for the credential    --access-key  value   AWS Access Key  --secret-key  value   AWS Secret Key    azure app-based    --name  value   Name for the credential    --subscription-id  value   Subscription ID from your Azure Subscriptions  --tenant-id  value   Directory ID from your Azure Active Directory   Properties      --app-id  value   Application ID of your app from your Azure Active Directory   App Registrations            --app-password  value   Your application key from app registration's Settings   Keys    gcp    --name  value   Name for the credential    --project-id  value   Project ID from your GCP account                        --service-account-id  value   Your GCP Service account ID from IAM   Admin   Service accounts                 --service-account-private-key-file  value   P12 key from your GCP service account    openstack keystone-v2     --name  value   Name for the credential    --tenant-user  value   OpenStack user name     --tenant-password  value   OpenStack password  --tenant-name  value   OpenStack tenant name      --endpoint  value    OpenStack endpoint     openstack keystone-v3    --name  value   Name for the credential   --tenant-user  value   OpenStack user name     --tenant-password  value   OpenStack password  --user-domain  value   OpenStack user domain      --endpoint  value    OpenStack endpoint    Options  --description  value   Description of the resource  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value  Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  --public    Public in account    Additionally, the following option is available for OpenStack Keystone2 and Keystone3:  --facing  value  API facing. One of: public, admin, internal  Additionally, the following options are available for OpenStack Keystone3:  --project-domain-name  value   OpenStack project domain name    --project-name  value   OpenStack project name           --domain-name  value   OpenStack domain name    --keystone-scope  value  OpenStack keystone scope. One of: default, domain, project     Examples  Creates a role-based credential on AWS:  cb credential create aws role-based --name my-credential1 --role-arn arn:aws:iam::517127065441:role/CredentialRole  Creates a key-based credential on AWS:  cb credential create aws key-based --name my-credential2 --access-key ABDVIRDFV3K4HLJ45SKA --secret-key D89L5pOPM+426Rtj3curKzJEJL3lYoNcP8GvguBV  Creates an app-based credential on Azure:  cb credential create azure app-based --name my-credential3 --subscription-id b8e7379e-568g-55d3-na82-45b8d421e998 --tenant-id  c79n5399-3231-65ba-8dgg-2g4e2a40085e --app-id 6d147d89-48d2-5de2-eef8-b89775bbfcg1 --app-password 4a8hBgfI52s/C8R5Sea2YHGnBFrD3fRONfdG8w7F2Ua=  Creates a credential on Google Cloud:  cb credential create gcp --name my-credential4 --project-id test-proj --service-account-id test@test-proj.iam.gserviceaccount.com --service-account-private-key-file /Users/test/3fff57a6f68e.p12  Creates a role-based credential on OpenStack with Keystone-v2:  cb credential create openstack keystone-v2 --name my-credential5 --tenant-user test --tenant-password MySecurePass123 --tenant-name test --endpoint http://openstack.test.organization.com:5000/v2.0  Related links     Create Credential on AWS  Create Credential on Azure  Create Credential on GCP  Create Credential on OpenStack", 
            "title": "credential create"
        }, 
        {
            "location": "/cli-reference/index.html#credential-delete", 
            "text": "Deletes an existing Cloudbreak credential.  Required options  --name  value   Credential name   Options  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value  Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  cb credential delete --name test-cred", 
            "title": "credential delete"
        }, 
        {
            "location": "/cli-reference/index.html#credential-describe", 
            "text": "Describes an existing credential.  Required options  --name  value   Credential name   Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]    --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value  Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  cb credential describe --name testcred\n{\n  \"Name\": \"testcred\",\n  \"Description\": \"\",\n  \"CloudPlatform\": \"AZURE\"\n}", 
            "title": "credential describe"
        }, 
        {
            "location": "/cli-reference/index.html#credential-list", 
            "text": "Lists existing Cloudbreak credentials.  Required options  None  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value  Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  Lists credentials:  cb credential list \n[\n  {\n    \"Name\": \"testcred\",\n    \"Description\": \"\",\n    \"CloudPlatform\": \"AZURE\"\n  }\n]  Lists credentials, with output formatted in a table format:  cb credential list --output table\n+---------+-------------+---------------+\n|  NAME   | DESCRIPTION | CLOUDPLATFORM |\n+---------+-------------+---------------+\n| armcred |             | AZURE         |\n+---------+-------------+---------------+", 
            "title": "credential list"
        }, 
        {
            "location": "/cli-reference/index.html#credential-modify", 
            "text": "Modifies an existing Cloudbreak credential.    The  --name  parameter is used to identify the credential that is being modified, and therefore its value cannot be modified.       Sub-commands  aws role-based   Modifies an AWS role-based credential  aws key-based   Modifies an AWS key-based credential  azure app-based  Modifies an app-based Azure credential  gcp  Modifies a Google Cloud credential  openstack keystone-v2  Modifies an OpenStack v2 credential  openstack keystone-v3  Modifies an  OpenStack v3 credential   Required options  aws role-based    --name  value   Credential name  --role-arn  value  IAM Role ARN of the role used for Cloudbreak credential    aws key-based    --name  value   Credential name  --access-key  value   AWS Access Key  --secret-key  value   AWS Secret Key    azure app-based    --name  value   Credential name  --subscription-id  value   Subscription ID from your Azure Subscriptions  --tenant-id  value   Directory ID from your Azure Active Directory   Properties      --app-id  value   Application ID of your app from your Azure Active Directory   App Registrations            --app-password  value   Your application key from app registration's Settings   Keys    gcp    --name  value   Credential name   --project-id  value   Project ID from your GCP account                        --service-account-id  value   Your GCP Service account ID from IAM   Admin   Service accounts                 --service-account-private-key-file  value   P12 key from your GCP service account    openstack keystone-v2     --name  value   Credential name  --tenant-user  value   OpenStack user name     --tenant-password  value   OpenStack password  --tenant-name  value   OpenStack tenant name      --endpoint  value    OpenStack endpoint     openstack keystone-v3    --name  value   Credential name  --tenant-user  value   OpenStack user name     --tenant-password  value   OpenStack password  --user-domain  value   OpenStack user domain      --endpoint  value    OpenStack endpoint    Options  --description  value   Description of the resource  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value  Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]      Additionally, the following option is available for OpenStack Keystone2 and Keystone3:  --facing  value  API facing. One of: public, admin, internal  Additionally, the following options are available for OpenStack Keystone3:  --project-domain-name  value   OpenStack project domain name    --project-name  value   OpenStack project name           --domain-name  value   OpenStack domain name    --keystone-scope  value  OpenStack keystone scope. One of: default, domain, project    Examples  Modifies a role-based AWS credential:  cb credential modify aws role-based --name my-credential1 --role-arn arn:aws:iam::517127065441:role/CredentialRole  Modifies a key-based AWS credential:  cb credential modify aws key-based --name my-credential2 --access-key ABDVIRDFV3K4HLJ45SKA --secret-key D89L5pOPM+426Rtj3curKzJEJL3lYoNcP8GvguBV  Modifies an app-based Azure credential:  cb credential modify azure app-based --name my-credential3 --subscription-id b8e7379e-568g-55d3-na82-45b8d421e998 --tenant-id  c79n5399-3231-65ba-8dgg-2g4e2a40085e --app-id 6d147d89-48d2-5de2-eef8-b89775bbfcg1 --app-password 4a8hBgfI52s/C8R5Sea2YHGnBFrD3fRONfdG8w7F2Ua=  Modifies a Google Cloud credential:  cb credential modify gcp --name my-credential4 --project-id test-proj --service-account-id test@test-proj.iam.gserviceaccount.com --service-account-private-key-file /Users/test/3fff57a6f68e.p12  Modifies a role-based OpenStack credential which uses Keystone-v2:  cb credential modify openstack keystone-v2 --name my-credential5 --tenant-user test --tenant-password MySecurePass123 --tenant-name test --endpoint http://openstack.test.organization.com:5000/v2.0", 
            "title": "credential modify"
        }, 
        {
            "location": "/cli-reference/index.html#database-create", 
            "text": "Registers an existing external database with Cloudbreak.  Sub-commands     mysql   Registers a MySQL database configuration  oracle11   Registers an Oracle 11 database configuration  oracle12   Registers an Oracle 12 database configuration  postgres   Registers a Postgres database configuration    Required options  --name  value    Name for the database     --db-username  value   Username for the JDBC connection  --db-password  value   Password for the JDBC connection  --url  value   JDBC connection URL in the form of jdbc:db-type://address:port/db  --type  value    Name if the service that will use the database (AMBARI, DRUID, HIVE, OOZIE, RANGER, SUPERSET, or other custom type)      If using MySQL and Oracle, the  --connector-jar-url value  value  parameter is required in all cases except the following: If you are using a custom image and you already placed the JAR file on the machine, then this parameter is not required.  Options  --description  value   Description for the database       --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]   --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]   --public    Public in account    Examples  Registers an existing Postgres database called \"test-postgres\" with Cloudbreak:  cb database create postgres --name testpostgres  --type HIVE --url jdbc:postgresql://test-db.cic6nusrpqec.us-west-2.rds.amazonaws.com:5432/testdb --db-username testuser --db-password MySecurePassword123  The connection URL includes three components db-type://address:port/db:    Database type \"jdbc:postgresql\"    Endpoint \"test-db.cic6nusrpqec.us-west-2.rds.amazonaws.com:5432\"    Port \"5432\"    Database name \"testdb\"     Registers an existing MySQL database called \"testmysql\" with Cloudbreak:    cb database create mysql --name testmysql --type OOZIE --url jdbc:mysql://test-db.cic6nusrpqec.us-west-2.rds.amazonaws.com:5432/testdb  --db-username test --db-password test --connector-jar-url http://example-page/driver-file.JAR", 
            "title": "database create"
        }, 
        {
            "location": "/cli-reference/index.html#database-delete", 
            "text": "Unregisters a previously registered database with Cloudbreak. It does not delete the database instance.   Required options  --name  value   Database registration name       Options  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  Examples  cb database delete --name testdatabase", 
            "title": "database delete"
        }, 
        {
            "location": "/cli-reference/index.html#database-list", 
            "text": "Lists all available database registrations.  Required options  None  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  Examples  Lists existing database registrations:  cb database list  Lists existing database registrations, with output presented in a table format:  cb database list --output table", 
            "title": "database list"
        }, 
        {
            "location": "/cli-reference/index.html#database-test", 
            "text": "Test database connection.  Sub-commands     by-name     Tests a stored database configuration identified by its name  by-params   Tests database connection parameters     Required options     by-name     --name  value   Database registration name      by-params     --db-username  value   Username to use for the JDBC connection  --db-password  value   Password to use for the JDBC connection  --url  value   JDBC connection URL in the form of jdbc:db-type://address:port/db  --type  value   Type of database (the service name that will use the database    Options  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  Tests connection to a previously registered database called \"testpostgres\":  database test --name testpostgres  Tests connection to a database based on connection parameters provided:   cb database test by-params --type HIVE --url jdbc:postgresql://test-db.cic6nusrpqec.us-west-2.rds.amazonaws.com:5432/testdb --db-username testuser --db-password MySecurePassword123", 
            "title": "database test"
        }, 
        {
            "location": "/cli-reference/index.html#imagecatalog-create", 
            "text": "Registers a new custom image catalog based on the URL provided.    Required options     --name  value   Name for the image catalog  --url  value    URL location of the image catalog JSON file    Options  --description  value   Description for the recipe   --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  --public    Public in account    Examples  Registers an image catalog called \"mycustomcatalog\" which is available at https://example.com/myimagecatalog.json:   cb imagecatalog create --name mycustomcatalog --url https://example.com/myimagecatalog.json  Related links     Custom Images", 
            "title": "imagecatalog create"
        }, 
        {
            "location": "/cli-reference/index.html#imagecatalog-delete", 
            "text": "Deletes a previously registered custom image catalog. It does not delete any cloud provider resources that you created as a prerequisite for creating the Cloudbreak credential.      Required options     --name  value   Image catalog name      Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  Deletes an image catalog called \"mycustomcatalog\":  cb imagecatalog delete --name mycustomcatalog  Related links     Custom Images", 
            "title": "imagecatalog delete"
        }, 
        {
            "location": "/cli-reference/index.html#imagecatalog-images", 
            "text": "Lists images from the specified image catalog available for the specified cloud provider.     Sub-commands     aws          Lists available aws images     azure        Lists available azure images     gcp          Lists available gcp images    openstack    Lists available openstack images      Required options  --imagecatalog  value   Name of the imagecatalog       Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]    --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]     --profile  value   Selects a config profile to use [$CB_PROFILE]   --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples     Returns date, description, Ambari version, and image ID for all AWS images from an image catalog called \"myimagecatalog\":  ./cb imagecatalog images aws --imagecatalog cloudbreak-default\n[\n  {\n    \"Date\": \"2017-10-13\",\n    \"Description\": \"Cloudbreak official base image\",\n    \"Version\": \"2.6.0.0\",\n    \"ImageID\": \"44b140a4-bd0b-457d-b174-e988bee3ca47\"\n  },\n  {\n    \"Date\": \"2017-11-16\",\n    \"Description\": \"Official Cloudbreak image\",\n    \"Version\": \"2.6.0.0\",\n    \"ImageID\": \"3c7598a4-ebd6-4a02-5638-882f5c7f7add\"\n  }\n]  Related links     Custom Images", 
            "title": "imagecatalog images"
        }, 
        {
            "location": "/cli-reference/index.html#imagecatalog-list", 
            "text": "Lists default and custom image catalogs registered with Cloudbreak instance.     Required options     None    Options     --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]   --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  Lists existing image catalogs:    cb  imagecatalog list \n[\n  {\n    \"Name\": \"mycustomcatalog\",\n    \"Default\": false,\n    \"URL\": \"https://example.com/imagecatalog.json\"\n  },\n  {\n    \"Name\": \"cloudbreak-default\",\n    \"Default\": true,\n    \"URL\": \"https://s3-eu-west-1.amazonaws.com/cloudbreak-info/v2-dev-cb-image-catalog.json\"\n  }\n]  Related links     Custom Images", 
            "title": "imagecatalog list"
        }, 
        {
            "location": "/cli-reference/index.html#imagecatalog-set-default", 
            "text": "Sets the specified image catalog as default.    Required options      --name  value   Image catalog name       Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]    --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  Sets \"mycustomcatalog\" as default:    imagecatalog set-default --name mycustomcatalog  Related links     Custom Images", 
            "title": "imagecatalog set-default"
        }, 
        {
            "location": "/cli-reference/index.html#ldap-create", 
            "text": "Registers an existing LDAP with Cloudbreak.  Required options  --name  value   Name for the LDAP       --ldap-server  value   Address of the LDAP server (e.g. ldap://10.0.0.1:384)  --ldap-domain  value    LDAP domain (e.g. ad.cb.com)  --ldap-bind-dn  value   LDAP bind DN (e.g. CN=Administrator,CN=Users,DC=ad,DC=cb,DC=com)  --ldap-bind-password  value   LDAP bind password  --ldap-directory-type  value    LDAP directory type (LDAP or ACTIVE_DIRECTORY)   --ldap-user-search-base  value    LDAP user search base (e.g. CN=Users,DC=ad,DC=cb,DC=com)  --ldap-user-name-attribute  value    LDAP user name attribute  --ldap-user-object-class  value    LDAP user object class  --ldap-group-member-attribute  value  LDAP group member attribute  --ldap-group-name-attribute  value   LDAP group name attribute  --ldap-group-object-class  value   LDAP group object class  --ldap-group-search-base  value    LDAP group search base (e.g. OU=scopes,DC=ad,DC=cb,DC=com)    Options  --ldap-admin-group  value   LDAP group of administrators  --description  value   Description for the LDAP      --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]   --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  --public    Public in account", 
            "title": "ldap create"
        }, 
        {
            "location": "/cli-reference/index.html#ldap-delete", 
            "text": "Deletes selected LDAP registration from Cloudbreak. It does not delete the LDAP.   Required options  --name  value   LDAP name        Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  Examples  cb ldap delete --name testldap", 
            "title": "ldap delete"
        }, 
        {
            "location": "/cli-reference/index.html#ldap-list", 
            "text": "Lists all available LDAPs.  Required options  None  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  Examples  Lists existing LDAPs:  cb ldap list  Lists existing LDAPs, with output presented in a table format:  cb ldap list --output table", 
            "title": "ldap list"
        }, 
        {
            "location": "/cli-reference/index.html#mpack-create", 
            "text": "Registers an existing management pack with Cloudbreak.  Required options  --name  value   Name for the mpack        --url  value   URL that points to the location of the mpack tarball     Options  --purge   Purge existing resources specified in purge-list   --purge-list  value   Comma-separated list of resources to purge (stack-definitions,service-definitions,mpacks). By default (stack-definitions,mpacks) will be purged  --force   Force install management pack    --description  value   Description for the LDAP      --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]   --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  --public    Public in account    Examples  Registers a new mpack without purging:  cb mpack create --name test-hdp-search --url http://public-repo-1.hortonworks.com/HDP-SOLR/hdp-solr-ambari-mp/solr-service-mpack-3.0.0.tar.gz", 
            "title": "mpack create"
        }, 
        {
            "location": "/cli-reference/index.html#mpack-delete", 
            "text": "Deletes selected management registration from Cloudbreak. It does not delete the management pack.   Required options  --name  value   Management pack name        Options  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  Examples  cb mpack delete --name testmpack", 
            "title": "mpack delete"
        }, 
        {
            "location": "/cli-reference/index.html#mpack-list", 
            "text": "Lists all available management packs.  Required options  None  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  Examples  Lists all currently registered management packs and provides information about each:  cb mpack list\n[\n  {\n    \"Name\": \"hdp-search-3\",\n    \"Description\": \"\",\n    \"URL\": \"http://public-repo-1.hortonworks.com/HDP-SOLR/hdp-solr-ambari-mp/solr-service-mpack-3.0.0.tar.gz\",\n    \"Purge\": \"false\",\n    \"PurgeList\": \"\",\n    \"Force\": \"false\"\n  }\n]", 
            "title": "mpack list"
        }, 
        {
            "location": "/cli-reference/index.html#proxy-create", 
            "text": "Registers an existing proxy with Cloudbreak.  Required options  --name  value   Name for the proxy     --proxy-host  value   Hostname or IP address of the proxy  --proxy-port  value   Port of the proxy    Options  --proxy-protocol  value   Protocol for the proxy (http or https) (default: \"http\")  --proxy-user  value    User for the proxy if basic auth is required  --proxy-password  value   Password for the proxy if basic auth is required  --description  value   Description for the proxy      --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]   --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  --public    Public in account", 
            "title": "proxy create"
        }, 
        {
            "location": "/cli-reference/index.html#proxy-delete", 
            "text": "Unregisters a previously registered proxy with Cloudbreak. It does not delete the proxy.   Required options  --name  value   Proxy registration name       Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  Examples  cb proxy delete --name testproxy", 
            "title": "proxy delete"
        }, 
        {
            "location": "/cli-reference/index.html#proxy-list", 
            "text": "Lists all proxies that were previously registered with Cloudbreak.  Required options  None  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  Examples  Lists existing proxy registrations:  cb proxy list  Lists existing proxy registrations, with output presented in a table format:  cb proxy list --output table", 
            "title": "proxy list"
        }, 
        {
            "location": "/cli-reference/index.html#recipe-create", 
            "text": "Adds a new recipe from a file or from a URL.  Sub-commands  from-url   Creates a recipe by downloading it from a URL location  from-file   Creates a recipe by reading it from a local file    Required options  from-url     --name  value   Name for the recipe   --execution-type  value   Type of execution [pre-ambari-start, pre-termination, post-ambari-start, post-cluster-install]    --url  value   URL location of the Ambari blueprint JSON file    from-file    --name  value   Name for the recipe  --execution-type  value   Type of execution [pre-ambari-start, pre-termination, post-ambari-start, post-cluster-install]   --file  value   Location of the Ambari blueprint JSON file    Options  --description  value   Description for the recipe   --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]  --public    Public in account    Examples  Adds a new recipe called \"test1\" from a URL:  cb recipe create from-url --name \"test1\" --execution-type post-ambari-start --url http://some-site.com/test.sh  Adds a new recipe called \"test2\" from a file:  cb recipe create from-url --name \"test2\" --execution-type post-ambari-start --file /Users/test/Documents/test.sh  Related links  Recipes", 
            "title": "recipe create"
        }, 
        {
            "location": "/cli-reference/index.html#recipe-delete", 
            "text": "Deletes an existing recipe.  Required options  --name  value   Recipe name    Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]   --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  cb recipe delete --name test", 
            "title": "recipe delete"
        }, 
        {
            "location": "/cli-reference/index.html#recipe-describe", 
            "text": "Describes an existing recipe.  Required options  --name  value   Recipe name       Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  Describes a recipe called \"test\":  cb recipe describe --name test\n{\n  \"Name\": \"test\",\n  \"Description\": \"\",\n  \"ExecutionType\": \"POST\"\n}  Describes a recipe called \"test\", with output presented in a table format:  cb describe-recipe --name test --output table\n+------+-------------+----------------+\n| NAME | DESCRIPTION | EXECUTION TYPE |\n+------+-------------+----------------+\n| test |             | POST           |\n+------+-------------+----------------+", 
            "title": "recipe describe"
        }, 
        {
            "location": "/cli-reference/index.html#recipe-list", 
            "text": "Lists all available recipes.  Required options  None  Options  --output  value   Supported formats: json, yaml, table (default: \"json\") [$CB_OUT_FORMAT]  --server  value   Cloudbreak server address [$CB_SERVER_ADDRESS]  --username  value   Cloudbreak user name (e-mail address) [$CB_USER_NAME]  --password  value   Cloudbreak password [$CB_PASSWORD]  --profile  value   Selects a config profile to use [$CB_PROFILE]  --auth-type  value   Authentication method to use. Values: oauth2, basic [$CB_AUTH_TYPE]    Examples  Lists existing recipes:  cb recipe list\n[\n  {\n    \"Name\": \"test\",\n    \"Description\": \"\",\n    \"ExecutionType\": \"POST\"\n  }\n]  Lists existing recipes, with output presented in a table format:  cb recipe list --output table\n+------+-------------+-------------------+\n| NAME | DESCRIPTION | EXECUTION TYPE    |\n+------+-------------+-------------------+\n| test |             | POST-AMBARI-START |\n+------+-------------+-------------------+", 
            "title": "recipe list"
        }, 
        {
            "location": "/cli-reference/index.html#debugging", 
            "text": "To use debugging mode, pass the  --debug  option.", 
            "title": "Debugging"
        }, 
        {
            "location": "/cli-reference/index.html#checking-cli-version", 
            "text": "To check CLI version, use  cb --version .", 
            "title": "Checking CLI Version"
        }, 
        {
            "location": "/trouble-cb-logs/index.html", 
            "text": "Checking Cloudbreak logs\n\n\nWhen troubleshooting, you can access the following Cloudbreak logs.\n\n\nCloudbreak logs\n\n\nWhen installing Cloudbreak using a pre-built cloud image, the  Cloudbreak deployer location and the cbd root folder is \n/var/lib/cloudbreak-deployment\n. You must execute all cbd actions from the cbd root folder as a cloudbreak user. \n\n\n\n\nYour cbd root directory may be different if you installed Cloudbreak on your own VM. \n\n\n\n\nAggregated logs\n\n\nCloudbreak consists of multiple microservices deployed into Docker containers. \n\n\nTo check aggregated service logs, use the following commands:\n\n\ncbd logs\n shows all service logs.\n\n\ncbd logs | tee cloudbreak.log\n allows you to redirect the input into a file for sharing these logs.\n\n\nIndividual service logs\n\n\nTo check individual service logs, use the following commands:\n\n\ncbd logs cloudbreak\n shows Cloudbreak logs. This service is the backend service that handles all deployments.\n\n\ncbd logs uluwatu\n shows Cloudbreak UI logs. Uluwatu is the UI component of Cloudbreak.\n\n\ncbd logs identity\n shows Identity logs. Identity is responsible for authentication and authorization.\n\n\ncbd logs periscope\n shows Periscope logs. Periscope is responsible for triggering autoscaling rules.\n\n\nDocker logs\n\n\nThe same logs can be accessed via Docker commands:\n\n\ndocker logs cbreak_cloudbreak_1\n shows the same logs as \ncbd logs cloudbreak\n.\n\n\nCloudbreak logs are rotated and can be accessed later from the Cloudbreak deployment folder. Each time you restart the application via cbd restart a new log file is created with a timestamp in the name (for example, cbreak-20170821-105900.log). \n\n\n\n\nThere is a symlink called \ncbreak.log\n which points to the latest log file. Sharing this symlink does not share the log itself.\n\n\n\n\nSaltstack logs\n\n\nCloudbreak uses Saltstack to install Ambari and the necessary packages for the HDP/HDF provisioning. Salt Master always runs alongside the Ambari Server node. Each instance in the cluster runs a Salt Minion, which connects to the Salt Master. There can be multiple Salt Masters if the cluster is configured to run in HA (High Availability) mode and in this case each Salt Minion connects to each Salt Master.\n\n\nCloudbreak also uses SaltStack to execute user-provided customization scripts called \"recipes\". \n\n\nSalt Master and Salt Minion logs can be found at the following location: \n/var/log/salt\n\n\nAmbari logs\n\n\nCloudbreak uses Ambari to orchestrate the installation of the different HDP/HDF components. Each instance in the cluster runs an Ambari agent which connects to the Ambari server. Ambari server is declared by the user during the cluster installation wizard. \n\n\nAmbari server logs\n\n\nAmbari server logs can be found on the nodes where Ambari server is installed in the following locations:\n\n\n/var/log/ambari-server/ambari-server.log\n\n\n/var/log/ambari-server/ambari-server.out\n\n\nBoth files contain important information about the root cause of a certain issue so it is advised to check both.\n\n\nAmbari agent logs\n\n\nAmbari agent logs can be found on the nodes where Ambari agent is installed in the following locations:\n\n\n/var/log/ambari-agent/ambari-agent.log\n\n\nRecipe logs\n\n\nCloudbreak supports \"recipes\" - user-provided customization scripts that can be run prior to or after cluster installation. It is the user\u2019s responsibility to provide an idempotent well tested script. If the execution fails, the recipe logs can be found at \n/var/log/recipes\n on the nodes on which the recipes were executed.\n\n\nIt is advised, but not required to have an advanced logging mechanism in the script, as Cloudbreak always logs every script that are run. Recipes are often the sources of installation failures as users might try to remove necessary packages or reconfigure services.", 
            "title": "Cloudbreak logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#checking-cloudbreak-logs", 
            "text": "When troubleshooting, you can access the following Cloudbreak logs.", 
            "title": "Checking Cloudbreak logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#cloudbreak-logs", 
            "text": "When installing Cloudbreak using a pre-built cloud image, the  Cloudbreak deployer location and the cbd root folder is  /var/lib/cloudbreak-deployment . You must execute all cbd actions from the cbd root folder as a cloudbreak user.    Your cbd root directory may be different if you installed Cloudbreak on your own VM.", 
            "title": "Cloudbreak logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#aggregated-logs", 
            "text": "Cloudbreak consists of multiple microservices deployed into Docker containers.   To check aggregated service logs, use the following commands:  cbd logs  shows all service logs.  cbd logs | tee cloudbreak.log  allows you to redirect the input into a file for sharing these logs.", 
            "title": "Aggregated logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#individual-service-logs", 
            "text": "To check individual service logs, use the following commands:  cbd logs cloudbreak  shows Cloudbreak logs. This service is the backend service that handles all deployments.  cbd logs uluwatu  shows Cloudbreak UI logs. Uluwatu is the UI component of Cloudbreak.  cbd logs identity  shows Identity logs. Identity is responsible for authentication and authorization.  cbd logs periscope  shows Periscope logs. Periscope is responsible for triggering autoscaling rules.", 
            "title": "Individual service logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#docker-logs", 
            "text": "The same logs can be accessed via Docker commands:  docker logs cbreak_cloudbreak_1  shows the same logs as  cbd logs cloudbreak .  Cloudbreak logs are rotated and can be accessed later from the Cloudbreak deployment folder. Each time you restart the application via cbd restart a new log file is created with a timestamp in the name (for example, cbreak-20170821-105900.log).    There is a symlink called  cbreak.log  which points to the latest log file. Sharing this symlink does not share the log itself.", 
            "title": "Docker logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#saltstack-logs", 
            "text": "Cloudbreak uses Saltstack to install Ambari and the necessary packages for the HDP/HDF provisioning. Salt Master always runs alongside the Ambari Server node. Each instance in the cluster runs a Salt Minion, which connects to the Salt Master. There can be multiple Salt Masters if the cluster is configured to run in HA (High Availability) mode and in this case each Salt Minion connects to each Salt Master.  Cloudbreak also uses SaltStack to execute user-provided customization scripts called \"recipes\".   Salt Master and Salt Minion logs can be found at the following location:  /var/log/salt", 
            "title": "Saltstack logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#ambari-logs", 
            "text": "Cloudbreak uses Ambari to orchestrate the installation of the different HDP/HDF components. Each instance in the cluster runs an Ambari agent which connects to the Ambari server. Ambari server is declared by the user during the cluster installation wizard.", 
            "title": "Ambari logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#ambari-server-logs", 
            "text": "Ambari server logs can be found on the nodes where Ambari server is installed in the following locations:  /var/log/ambari-server/ambari-server.log  /var/log/ambari-server/ambari-server.out  Both files contain important information about the root cause of a certain issue so it is advised to check both.", 
            "title": "Ambari server logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#ambari-agent-logs", 
            "text": "Ambari agent logs can be found on the nodes where Ambari agent is installed in the following locations:  /var/log/ambari-agent/ambari-agent.log", 
            "title": "Ambari agent logs"
        }, 
        {
            "location": "/trouble-cb-logs/index.html#recipe-logs", 
            "text": "Cloudbreak supports \"recipes\" - user-provided customization scripts that can be run prior to or after cluster installation. It is the user\u2019s responsibility to provide an idempotent well tested script. If the execution fails, the recipe logs can be found at  /var/log/recipes  on the nodes on which the recipes were executed.  It is advised, but not required to have an advanced logging mechanism in the script, as Cloudbreak always logs every script that are run. Recipes are often the sources of installation failures as users might try to remove necessary packages or reconfigure services.", 
            "title": "Recipe logs"
        }, 
        {
            "location": "/trouble-cb/index.html", 
            "text": "Troubleshooting Cloudbreak\n\n\nThis section includes common errors and steps to resolve them. \n\n\nInvalid PUBLIC_IP in CBD Profile\n\n\nThe \nPUBLIC_IP\n property must be set in the cbd Profile file or else you won\u2019t be able to log in on the Cloudbreak UI. \n\n\nIf you are migrating your instance, make sure that after the start the IP remains valid. If you need to edit the \nPUBLIC_IP\n property in Profile, make sure to restart Cloudbreak using \ncbd restart\n.\n\n\nCbd cannot get VM's public IP\n\n\nBy default the \ncbd\n tool tries to get the VM's public IP to bind Cloudbreak UI to it. But if \ncbd\n cannot get the IP address during the initialization, you must set it manually. Check your \nProfile\n and if \nPUBLIC_IP\n is not set, add the \nPUBLIC_IP\n variable and set it to the public IP of the VM. For example: \n\n\nexport PUBLIC_IP=192.134.23.10\n\n\n\nPermission or connection problems\n\n\nIf you face permission or connection issues, disable SELinux:\n\n\n\n\n\n\nDisable SELINUX:\n\n\nsetenforce 0\nsed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\n\n\n\n\n\n\nEnsure the SELinux is not turned on afterwards:\n\n\nsestatus | grep -i mode\nCurrent mode:                   permissive\nMode from config file:          permissive\n\n\n\n\n\n\nCreating cbreak_sultans_1 ... Error\n\n\ncbd start\n returns the following error:\n\n\nCreating cbreak_sultans_1 ... error\n\nERROR: for cbreak_sultans_1  Cannot create container for service sultans: unknown log opt 'max-size' for journald log driver\nCreating cbreak_consul_1\nCreating cbreak_logrotate_1 ... error\nCreating cbreak_periscope_1 ... error\nCreating cbreak_mail_1 ... error\nCreating cbreak_haveged_1 ... error\n\nERROR: for cbreak_mail_1  Cannot create container for service mail: unknown log opt 'max-size' for journald log driver\n\nCreating cbreak_uluwatu_1 ... error\n\nCreating cbreak_smartsense_1 ... error\n\nCreating cbreak_consul_1 ... error\nCreating cbreak_identity_1 ... error\n\nERROR: for cbreak_identity_1  Cannot create container for service identity: unknown log opt 'max-file' for journald log driver\nCreating cbreak_logsink_1 ... error\nCreating cbreak_commondb_1 ... error\n\nERROR: for cbreak_commondb_1  Cannot create container for service commondb: unknown log opt 'max-size' for journald log driver\n\nERROR: for haveged  Cannot create container for service haveged: unknown log opt 'max-size' for journald log driver\n\nERROR: for uluwatu  Cannot create container for service uluwatu: unknown log opt 'max-size' for journald log driver\n\nERROR: for consul  Cannot create container for service consul: unknown log opt 'max-size' for journald log driver\n\nERROR: for commondb  Cannot create container for service commondb: unknown log opt 'max-size' for journald log driver\n\nERROR: for logrotate  Cannot create container for service logrotate: unknown log opt 'max-size' for journald log driver\n\nERROR: for periscope  Cannot create container for service periscope: unknown log opt 'max-size' for journald log driver\n\nERROR: for sultans  Cannot create container for service sultans: unknown log opt 'max-size' for journald log driver\n\nERROR: for mail  Cannot create container for service mail: unknown log opt 'max-size' for journald log driver\n\nERROR: for logsink  Cannot create container for service logsink: unknown log opt 'max-size' for journald log driver\n\nERROR: for smartsense  Cannot create container for service smartsense: unknown log opt 'max-size' for journald log driver\n\nERROR: for identity  Cannot create container for service identity: unknown log opt 'max-file' for journald log driver\nEncountered errors while bringing up the project.\n\n\n\nThis means that your \nDocker logging drivers\n are not configured correctly.\n\n\nTo resolve the issue, on your Cloudbreak VM:\n\n\n\n\n\n\nCheck the Docker Logging Driver configuration:\n\n\ndocker info | grep \"Logging Driver\"\n\n\nIf it is set to \"Logging Driver: journald\", you must set it to \"json-file\". \n\n\n\n\n\n\nOpen the \ndocker\n file for editing:\n\n\nvi /etc/sysconfig/docker\n\n\n\n\n\n\nEdit the following part of the file so that it looks like below (showing \nlog-driver=json-file\n):\n\n\n# Modify these options if you want to change the way the docker daemon runs\nOPTIONS='--selinux-enabled --log-driver=json-file --signature-verification=false'\n     \n\n\n\n\n\n\nRestart Docker:\n\n\nsystemctl restart docker\nsystemctl status docker", 
            "title": "Troubleshooting Cloudbreak"
        }, 
        {
            "location": "/trouble-cb/index.html#troubleshooting-cloudbreak", 
            "text": "This section includes common errors and steps to resolve them.", 
            "title": "Troubleshooting Cloudbreak"
        }, 
        {
            "location": "/trouble-cb/index.html#invalid-public_ip-in-cbd-profile", 
            "text": "The  PUBLIC_IP  property must be set in the cbd Profile file or else you won\u2019t be able to log in on the Cloudbreak UI.   If you are migrating your instance, make sure that after the start the IP remains valid. If you need to edit the  PUBLIC_IP  property in Profile, make sure to restart Cloudbreak using  cbd restart .", 
            "title": "Invalid PUBLIC_IP in CBD Profile"
        }, 
        {
            "location": "/trouble-cb/index.html#cbd-cannot-get-vms-public-ip", 
            "text": "By default the  cbd  tool tries to get the VM's public IP to bind Cloudbreak UI to it. But if  cbd  cannot get the IP address during the initialization, you must set it manually. Check your  Profile  and if  PUBLIC_IP  is not set, add the  PUBLIC_IP  variable and set it to the public IP of the VM. For example:   export PUBLIC_IP=192.134.23.10", 
            "title": "Cbd cannot get VM's public IP"
        }, 
        {
            "location": "/trouble-cb/index.html#permission-or-connection-problems", 
            "text": "If you face permission or connection issues, disable SELinux:    Disable SELINUX:  setenforce 0\nsed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config    Ensure the SELinux is not turned on afterwards:  sestatus | grep -i mode\nCurrent mode:                   permissive\nMode from config file:          permissive", 
            "title": "Permission or connection problems"
        }, 
        {
            "location": "/trouble-cb/index.html#creating-cbreak_sultans_1-error", 
            "text": "cbd start  returns the following error:  Creating cbreak_sultans_1 ... error\n\nERROR: for cbreak_sultans_1  Cannot create container for service sultans: unknown log opt 'max-size' for journald log driver\nCreating cbreak_consul_1\nCreating cbreak_logrotate_1 ... error\nCreating cbreak_periscope_1 ... error\nCreating cbreak_mail_1 ... error\nCreating cbreak_haveged_1 ... error\n\nERROR: for cbreak_mail_1  Cannot create container for service mail: unknown log opt 'max-size' for journald log driver\n\nCreating cbreak_uluwatu_1 ... error\n\nCreating cbreak_smartsense_1 ... error\n\nCreating cbreak_consul_1 ... error\nCreating cbreak_identity_1 ... error\n\nERROR: for cbreak_identity_1  Cannot create container for service identity: unknown log opt 'max-file' for journald log driver\nCreating cbreak_logsink_1 ... error\nCreating cbreak_commondb_1 ... error\n\nERROR: for cbreak_commondb_1  Cannot create container for service commondb: unknown log opt 'max-size' for journald log driver\n\nERROR: for haveged  Cannot create container for service haveged: unknown log opt 'max-size' for journald log driver\n\nERROR: for uluwatu  Cannot create container for service uluwatu: unknown log opt 'max-size' for journald log driver\n\nERROR: for consul  Cannot create container for service consul: unknown log opt 'max-size' for journald log driver\n\nERROR: for commondb  Cannot create container for service commondb: unknown log opt 'max-size' for journald log driver\n\nERROR: for logrotate  Cannot create container for service logrotate: unknown log opt 'max-size' for journald log driver\n\nERROR: for periscope  Cannot create container for service periscope: unknown log opt 'max-size' for journald log driver\n\nERROR: for sultans  Cannot create container for service sultans: unknown log opt 'max-size' for journald log driver\n\nERROR: for mail  Cannot create container for service mail: unknown log opt 'max-size' for journald log driver\n\nERROR: for logsink  Cannot create container for service logsink: unknown log opt 'max-size' for journald log driver\n\nERROR: for smartsense  Cannot create container for service smartsense: unknown log opt 'max-size' for journald log driver\n\nERROR: for identity  Cannot create container for service identity: unknown log opt 'max-file' for journald log driver\nEncountered errors while bringing up the project.  This means that your  Docker logging drivers  are not configured correctly.  To resolve the issue, on your Cloudbreak VM:    Check the Docker Logging Driver configuration:  docker info | grep \"Logging Driver\"  If it is set to \"Logging Driver: journald\", you must set it to \"json-file\".     Open the  docker  file for editing:  vi /etc/sysconfig/docker    Edit the following part of the file so that it looks like below (showing  log-driver=json-file ):  # Modify these options if you want to change the way the docker daemon runs\nOPTIONS='--selinux-enabled --log-driver=json-file --signature-verification=false'          Restart Docker:  systemctl restart docker\nsystemctl status docker", 
            "title": "Creating cbreak_sultans_1 ... Error"
        }, 
        {
            "location": "/trouble-cluster/index.html", 
            "text": "Troubleshooting cluster creation\n\n\nConfigure communication via private IPs on AWS\n\n\nCloudbreak uses public IP addresses when communicating with cluster nodes. On AWS, you can configure it to use private IPs instead of public IPs by setting the CB_AWS_VPC variable in the Profile. \n\n\n\n\nThis configuration is available for AWS only. Do not use it for other cloud providers. \n\n\n\n\n\n\n\n\nNavigate to the Cloudbreak deployment directory and edit Profile. For example:\n\n\ncd /var/lib/cloudbreak-deployment/\nvi Profile\n\n\n\n\n\n\nAdd the following entry, setting it to the AWS VPC identifier where you have deployed Cloudbreak:\n\n\nexport CB_AWS_VPC=your-VPC-ID\n\n\nFor example:\n\n\nexport CB_AWS_VPC=vpc-e261a185\n\n\n\n\n\n\nRestart Cloudbreak by using \ncbd restart\n.      \n\n\n\n\n\n\nCannot access Oozie web UI\n\n\nExt JS is GPL licensed software and is no longer included in builds of HDP 2.6. Because of this, the Oozie WAR file is not built to include the Ext JS-based user interface unless Ext JS is manually installed on the Oozie server. If you add Oozie using Ambari 2.6.1.0 to an HDP 2.6.4 or greater stack, no Oozie UI will be available by default. Therefore, if you plan to use Oozie web UI with Ambari 2.6.1.0 and HDP 2.6.4 or greater, you you must manually install Ext JS on the Oozie server host.\n\n\nYou can install Ext JS by adding the following PRE-AMBARI-START recipe:\n\n\nexport EXT_JS_VERSION=2.2-1\n     export OS_NAME=centos6\n     wget http://public-repo-1.hortonworks.com/HDP-UTILS-GPL-1.1.0.22/repos/$OS_NAME/extjs/extjs-$EXT_JS_VERSION.noarch.rpm\n     rpm -ivh extjs-$EXT_JS_VERSION.noarch.rpm\n\n\n\nMake the following changes to the script:\n\n\n\n\nChange the EXT_JS_VERSION to the specific ExtJS version that you want to use.  \n\n\nChange the OS_NAME to the name of the operating system. Supported values are: centos6, centos7, centos7-ppc.\n\n\n\n\nThe general steps are:\n\n\n\n\nBe sure to review and agree to the Ext JS license prior to using this recipe.  \n\n\nCreate a PRE-AMBARI-START recipe. For instructions on how to create a recipe, refer to \nAdd recipes\n.   \n\n\nWhen creating a cluster, choose this recipe to be executed on all host groups of the cluster. \n\n\n\n\nRelated links\n\n\nAdd recipes\n\n\nUsing custom scripts (recipes)\n  \n\n\nQuota limitations\n\n\nEach cloud provider has quota limitations on various cloud resources, and these quotas can usually be increased on request. If there is an error message in Cloudbreak stating that there are no more available EIPs (Elastic IP Address) or VPCs, you need to request more of these resources. \n\n\nTo see the limitations visit the cloud provider\u2019s site:\n\n\n\n\nAWS service limits\n \n\n\nAzure subscription and service limits, quotas, and constraints\n\n\nGCP resource quotas\n \n\n\n\n\nConnection timeout when ports are not open\n\n\nIn the cluster installation wizard, you must specify on which node you want to run the Ambari server. Cloudbreak communicates with this node to orchestrate the installation.\n\n\nA common reason for connection timeout is security group misconfiguration. Cloudbreak allows configuring different security groups for the different instance groups; however, there are certain requirements for the Ambari server node. Specifically, the following ports must be open in order to communicate with that node:\n\n\n\n\n22 (SSH)  \n\n\n9443 (two-way-ssl through nginx) \n\n\n\n\nBlueprint errors\n\n\nInvalid services and configurations\n\n\nAmbari blueprints are a declarative definition of a cluster. With a blueprint, you specify a stack, the component layout, and the configurations to materialize a Hadoop cluster instance via a REST API without having to use the Ambari cluster install wizard. \n\n\nCloudbreak supports any type of blueprints, which is a common source of errors. These errors are only visible once the core infrastructure is up and running and Cloudbreak tries to initiate the cluster installation through Ambari. Ambari validates the blueprint and  rejects it if it's invalid. \n\n\nFor example, if there are configurations for a certain service like Hive but Hive as a service is not mapped to any host group, the blueprint is invalid.\n\n\nTo fix these type of issues, edit your blueprint and then reinstall your cluster. Cloudbreak UI has support for this so the infrastructure does not have to be terminated.\n\n\nThere are some cases when Ambari cannot validate your blueprint beforehand. In these cases, the issues are only visible in the Ambari server logs. To troubleshoot, check Ambari server logs.\n\n\nWrong HDP/HDF version\n\n\nIn the blueprint, only the major and minor HDP/HDF version should be defined (for example, \"2.6\"). If wrong version number is provided, the following error can be found in the logs:\n\n\n5/15/2017 12:23:19 PM testcluster26 - create failed: Cannot use the specified Ambari stack: HDPRepo\n{stack='null'; utils='null'}\n. Error: org.apache.ambari.server.controller.spi.NoSuchResourceException: The specified resource doesn't exist: Stack data, Stack HDP 2.6.0.3 is not found in Ambari metainfo\n\n\n\n\nFor correct blueprint layout, refer to the \nAmbari cwiki\n page.\n\n\nRecipe errors\n\n\nRecipe execution times out\n\n\nIf the scripts are taking too much time to execute, the processes will time out, as the threshold for all recipes is set to 15 minutes. To change this threshold, you must override the default value by adding the following to the cbd Profile file:\n\n\nexport CB_JAVA_OPTS=\u201d -Dcb.max.salt.recipe.execution.retry=90\u201d\n\n\n\n\nThis property indicates the number of tries for checking if the scripts have finished with a sleep time (i.e. the wait time between two polling attempts) of 10 seconds. The default value is 90. To increase the threshold, provide a number greater than 90. You must restart Cloudbreak after changing properties in the Profile file.\n\n\nRecipe execution fails\n\n\nIt often happens that a script cannot be executed successfully because there are typos or errors in the script. To verify this you can check the recipe logs at\n\n/var/log/recipes\n. For each script, there will be a separate log file with the name of the script that you provided on the Cloudbreak UI.", 
            "title": "Troubleshooting cluster creation"
        }, 
        {
            "location": "/trouble-cluster/index.html#troubleshooting-cluster-creation", 
            "text": "", 
            "title": "Troubleshooting cluster creation"
        }, 
        {
            "location": "/trouble-cluster/index.html#configure-communication-via-private-ips-on-aws", 
            "text": "Cloudbreak uses public IP addresses when communicating with cluster nodes. On AWS, you can configure it to use private IPs instead of public IPs by setting the CB_AWS_VPC variable in the Profile.    This configuration is available for AWS only. Do not use it for other cloud providers.      Navigate to the Cloudbreak deployment directory and edit Profile. For example:  cd /var/lib/cloudbreak-deployment/\nvi Profile    Add the following entry, setting it to the AWS VPC identifier where you have deployed Cloudbreak:  export CB_AWS_VPC=your-VPC-ID  For example:  export CB_AWS_VPC=vpc-e261a185    Restart Cloudbreak by using  cbd restart .", 
            "title": "Configure communication via private IPs on AWS"
        }, 
        {
            "location": "/trouble-cluster/index.html#cannot-access-oozie-web-ui", 
            "text": "Ext JS is GPL licensed software and is no longer included in builds of HDP 2.6. Because of this, the Oozie WAR file is not built to include the Ext JS-based user interface unless Ext JS is manually installed on the Oozie server. If you add Oozie using Ambari 2.6.1.0 to an HDP 2.6.4 or greater stack, no Oozie UI will be available by default. Therefore, if you plan to use Oozie web UI with Ambari 2.6.1.0 and HDP 2.6.4 or greater, you you must manually install Ext JS on the Oozie server host.  You can install Ext JS by adding the following PRE-AMBARI-START recipe:  export EXT_JS_VERSION=2.2-1\n     export OS_NAME=centos6\n     wget http://public-repo-1.hortonworks.com/HDP-UTILS-GPL-1.1.0.22/repos/$OS_NAME/extjs/extjs-$EXT_JS_VERSION.noarch.rpm\n     rpm -ivh extjs-$EXT_JS_VERSION.noarch.rpm  Make the following changes to the script:   Change the EXT_JS_VERSION to the specific ExtJS version that you want to use.    Change the OS_NAME to the name of the operating system. Supported values are: centos6, centos7, centos7-ppc.   The general steps are:   Be sure to review and agree to the Ext JS license prior to using this recipe.    Create a PRE-AMBARI-START recipe. For instructions on how to create a recipe, refer to  Add recipes .     When creating a cluster, choose this recipe to be executed on all host groups of the cluster.    Related links  Add recipes  Using custom scripts (recipes)", 
            "title": "Cannot access Oozie web UI"
        }, 
        {
            "location": "/trouble-cluster/index.html#quota-limitations", 
            "text": "Each cloud provider has quota limitations on various cloud resources, and these quotas can usually be increased on request. If there is an error message in Cloudbreak stating that there are no more available EIPs (Elastic IP Address) or VPCs, you need to request more of these resources.   To see the limitations visit the cloud provider\u2019s site:   AWS service limits    Azure subscription and service limits, quotas, and constraints  GCP resource quotas", 
            "title": "Quota limitations"
        }, 
        {
            "location": "/trouble-cluster/index.html#connection-timeout-when-ports-are-not-open", 
            "text": "In the cluster installation wizard, you must specify on which node you want to run the Ambari server. Cloudbreak communicates with this node to orchestrate the installation.  A common reason for connection timeout is security group misconfiguration. Cloudbreak allows configuring different security groups for the different instance groups; however, there are certain requirements for the Ambari server node. Specifically, the following ports must be open in order to communicate with that node:   22 (SSH)    9443 (two-way-ssl through nginx)", 
            "title": "Connection timeout when ports are not open"
        }, 
        {
            "location": "/trouble-cluster/index.html#blueprint-errors", 
            "text": "", 
            "title": "Blueprint errors"
        }, 
        {
            "location": "/trouble-cluster/index.html#invalid-services-and-configurations", 
            "text": "Ambari blueprints are a declarative definition of a cluster. With a blueprint, you specify a stack, the component layout, and the configurations to materialize a Hadoop cluster instance via a REST API without having to use the Ambari cluster install wizard.   Cloudbreak supports any type of blueprints, which is a common source of errors. These errors are only visible once the core infrastructure is up and running and Cloudbreak tries to initiate the cluster installation through Ambari. Ambari validates the blueprint and  rejects it if it's invalid.   For example, if there are configurations for a certain service like Hive but Hive as a service is not mapped to any host group, the blueprint is invalid.  To fix these type of issues, edit your blueprint and then reinstall your cluster. Cloudbreak UI has support for this so the infrastructure does not have to be terminated.  There are some cases when Ambari cannot validate your blueprint beforehand. In these cases, the issues are only visible in the Ambari server logs. To troubleshoot, check Ambari server logs.", 
            "title": "Invalid services and configurations"
        }, 
        {
            "location": "/trouble-cluster/index.html#wrong-hdphdf-version", 
            "text": "In the blueprint, only the major and minor HDP/HDF version should be defined (for example, \"2.6\"). If wrong version number is provided, the following error can be found in the logs:  5/15/2017 12:23:19 PM testcluster26 - create failed: Cannot use the specified Ambari stack: HDPRepo\n{stack='null'; utils='null'}\n. Error: org.apache.ambari.server.controller.spi.NoSuchResourceException: The specified resource doesn't exist: Stack data, Stack HDP 2.6.0.3 is not found in Ambari metainfo  For correct blueprint layout, refer to the  Ambari cwiki  page.", 
            "title": "Wrong HDP/HDF version"
        }, 
        {
            "location": "/trouble-cluster/index.html#recipe-errors", 
            "text": "", 
            "title": "Recipe errors"
        }, 
        {
            "location": "/trouble-cluster/index.html#recipe-execution-times-out", 
            "text": "If the scripts are taking too much time to execute, the processes will time out, as the threshold for all recipes is set to 15 minutes. To change this threshold, you must override the default value by adding the following to the cbd Profile file:  export CB_JAVA_OPTS=\u201d -Dcb.max.salt.recipe.execution.retry=90\u201d  This property indicates the number of tries for checking if the scripts have finished with a sleep time (i.e. the wait time between two polling attempts) of 10 seconds. The default value is 90. To increase the threshold, provide a number greater than 90. You must restart Cloudbreak after changing properties in the Profile file.", 
            "title": "Recipe execution times out"
        }, 
        {
            "location": "/trouble-cluster/index.html#recipe-execution-fails", 
            "text": "It often happens that a script cannot be executed successfully because there are typos or errors in the script. To verify this you can check the recipe logs at /var/log/recipes . For each script, there will be a separate log file with the name of the script that you provided on the Cloudbreak UI.", 
            "title": "Recipe execution fails"
        }, 
        {
            "location": "/trouble-aws/index.html", 
            "text": "Troubleshooting Cloudbreak on AWS\n\n\n\n\nCheck out \nHDCloud\n troubleshooting docs.\n\n\n\n\nUnable to create an IAM Role for Cloudbreak\n\n\nMost corporate AWS users are unable to create AWS roles. You may have to contact your AWS admin to create the role(s) for you. \n\n\nCluster fails with a permissions related error\n\n\nMake sure that the policy attached to CredentialRole includes all actions defined in \nCredentialRole\n.", 
            "title": "Troubleshooting AWS"
        }, 
        {
            "location": "/trouble-aws/index.html#troubleshooting-cloudbreak-on-aws", 
            "text": "Check out  HDCloud  troubleshooting docs.", 
            "title": "Troubleshooting Cloudbreak on AWS"
        }, 
        {
            "location": "/trouble-aws/index.html#unable-to-create-an-iam-role-for-cloudbreak", 
            "text": "Most corporate AWS users are unable to create AWS roles. You may have to contact your AWS admin to create the role(s) for you.", 
            "title": "Unable to create an IAM Role for Cloudbreak"
        }, 
        {
            "location": "/trouble-aws/index.html#cluster-fails-with-a-permissions-related-error", 
            "text": "Make sure that the policy attached to CredentialRole includes all actions defined in  CredentialRole .", 
            "title": "Cluster fails with a permissions related error"
        }, 
        {
            "location": "/trouble-azure/index.html", 
            "text": "Troubleshooting Cloudbreak on Azure\n\n\nCloudbreak deployment errors\n\n\nInvalid resource reference\n\n\nExample error message:\n\n\nResource /subscriptions/.../resourceGroups//providers/Microsoft.Network/virtualNetworks/cbdeployerVnet/\n\nsubnets/cbdeployerSubnet referenced by resource /subscriptions/.../resourceGroups/Manulife-ADLS/providers/\n\nMicrosoft.Network/networkInterfaces/cbdeployerNic was not found.\n\nPlease make sure that the referenced resource exists, and that both resources are in the same region.\n\n\nSymptom\n: The most common reason for this error is that you did not provide the Vnet RG Name (last parameter in the template).  \n\n\nSolution\n: When launching Cloudbreak, under \"Vnet RG Name\" provide the name of the resource group in which the selected VNet is located. If using a new VNet, enter the same resource group name as in \"Resource group\". \n\n\nCredential prerequisite errors\n\n\nYou don't have enough permissions to assign roles\n\n\nThis error during the interactive credential creation typically means that you do not have suitable permissions to create an interactive credential. Using an interactive credential currently requires an \"Owner\" role or its equivalent so if you are using a corporate account you are unlikely to have it. Try using the app-based credential. \n\n\nProblems with IAM permissions assignment\n\n\nAfter registering an Azure application you may have to ask your Azure administrator to perform the step of assigning the \"Contributor\" role to it:\n\n\n \n\n\nCredential creation errors\n\n\nRole already exists\n\n\nExample error message: \nRole already exists in Azure with the name: CloudbreakCustom50\n\n\nSymptom\n: You specified that you want to create a new role for Cloudbreak credential, but an existing role with the same name already exists in Azure. \n\n\nSolution\n: You should either rename the role during credential creation or select the \nReuse existing custom role\n option. \n\n\nRole does not exist\n\n\nExample error message: \nRole does not exist in Azure with the name: CloudbreakCustom60\n\n\nSymptom\n: You specified that you want to reuse an existing role for your Cloudbreak credential, but that particular role does not exist in Azure.\n\n\nSolution\n: You should either rename the new role during the credential creation to match the existing role's name or select the \nLet Cloudbreak create a custom role\n option. \n\n\nRole does not have enough privileges\n\n\nExample error message: \nCloudbreakCustom 50 role does not have enough privileges to be used by Cloudbreak!\n\n\n\n\nSymptom\n: You specified that you want to reuse an  existing role for your Cloudbreak credential, but that particular role does not have the necessary privileges for Cloudbreak cluster management.\n\n\nSolution\n: You should either select an existing role with enough privileges or select the \nLet Cloudbreak create a custom role\n option.\n\n\nThe necessary action set for Cloudbreak to be able to manage the clusters includes:\n        \n\"Microsoft.Compute/*\",\n        \"Microsoft.Network/*\",\n        \"Microsoft.Storage/*\",\n        \"Microsoft.Resources/*\"\n\n\nClient does not have authorization\n\n\nExample error message:\n\n\nFailed to verify credential: Status code 403, {\"error\":{\"code\":\"AuthorizationFailed\",\n\n\"message\":\"The client 'X' with object id 'z' does not have authorization to perform action\n\n'Microsoft.Storage/storageAccounts/read' over scope 'subscriptions/...'\"}\n\n\nSymptom\n: Your Azure account does not have sufficient permissions to create a Coudbreak credential. \n\n\nSolution\n: If you get this error during interactive credential creation, please ensure that your Azure account has \nMicrosoft.Authorization/*/Write\n permission. Otherwise contact your Azure administrator to either give your account that permission or create the necessary resources for the app-based credential creation method.  \n\n\nCloud not validate publickey certificate\n\n\nExample error message:\n\n\nCould not validate publickey certificate [certificate: 'fdfdsf'], detailed message: \n\nCorrupt or unknown public key file format\n\n\nSymptom\n: The syntax of your SSH public key is incorrect.\n\n\nSolution\n: You must correct the syntax of your SSH key. For information about the correct syntax, refer to \nthis\n page.", 
            "title": "Troubleshooting Azure"
        }, 
        {
            "location": "/trouble-azure/index.html#troubleshooting-cloudbreak-on-azure", 
            "text": "", 
            "title": "Troubleshooting Cloudbreak on Azure"
        }, 
        {
            "location": "/trouble-azure/index.html#cloudbreak-deployment-errors", 
            "text": "", 
            "title": "Cloudbreak deployment errors"
        }, 
        {
            "location": "/trouble-azure/index.html#invalid-resource-reference", 
            "text": "Example error message:  Resource /subscriptions/.../resourceGroups//providers/Microsoft.Network/virtualNetworks/cbdeployerVnet/ \nsubnets/cbdeployerSubnet referenced by resource /subscriptions/.../resourceGroups/Manulife-ADLS/providers/ \nMicrosoft.Network/networkInterfaces/cbdeployerNic was not found. \nPlease make sure that the referenced resource exists, and that both resources are in the same region.  Symptom : The most common reason for this error is that you did not provide the Vnet RG Name (last parameter in the template).    Solution : When launching Cloudbreak, under \"Vnet RG Name\" provide the name of the resource group in which the selected VNet is located. If using a new VNet, enter the same resource group name as in \"Resource group\".", 
            "title": "Invalid resource reference"
        }, 
        {
            "location": "/trouble-azure/index.html#credential-prerequisite-errors", 
            "text": "", 
            "title": "Credential prerequisite errors"
        }, 
        {
            "location": "/trouble-azure/index.html#you-dont-have-enough-permissions-to-assign-roles", 
            "text": "This error during the interactive credential creation typically means that you do not have suitable permissions to create an interactive credential. Using an interactive credential currently requires an \"Owner\" role or its equivalent so if you are using a corporate account you are unlikely to have it. Try using the app-based credential.", 
            "title": "You don't have enough permissions to assign roles"
        }, 
        {
            "location": "/trouble-azure/index.html#problems-with-iam-permissions-assignment", 
            "text": "After registering an Azure application you may have to ask your Azure administrator to perform the step of assigning the \"Contributor\" role to it:", 
            "title": "Problems with IAM permissions assignment"
        }, 
        {
            "location": "/trouble-azure/index.html#credential-creation-errors", 
            "text": "", 
            "title": "Credential creation errors"
        }, 
        {
            "location": "/trouble-azure/index.html#role-already-exists", 
            "text": "Example error message:  Role already exists in Azure with the name: CloudbreakCustom50  Symptom : You specified that you want to create a new role for Cloudbreak credential, but an existing role with the same name already exists in Azure.   Solution : You should either rename the role during credential creation or select the  Reuse existing custom role  option.", 
            "title": "Role already exists"
        }, 
        {
            "location": "/trouble-azure/index.html#role-does-not-exist", 
            "text": "Example error message:  Role does not exist in Azure with the name: CloudbreakCustom60  Symptom : You specified that you want to reuse an existing role for your Cloudbreak credential, but that particular role does not exist in Azure.  Solution : You should either rename the new role during the credential creation to match the existing role's name or select the  Let Cloudbreak create a custom role  option.", 
            "title": "Role does not exist"
        }, 
        {
            "location": "/trouble-azure/index.html#role-does-not-have-enough-privileges", 
            "text": "Example error message:  CloudbreakCustom 50 role does not have enough privileges to be used by Cloudbreak!   Symptom : You specified that you want to reuse an  existing role for your Cloudbreak credential, but that particular role does not have the necessary privileges for Cloudbreak cluster management.  Solution : You should either select an existing role with enough privileges or select the  Let Cloudbreak create a custom role  option.  The necessary action set for Cloudbreak to be able to manage the clusters includes:\n         \"Microsoft.Compute/*\",\n        \"Microsoft.Network/*\",\n        \"Microsoft.Storage/*\",\n        \"Microsoft.Resources/*\"", 
            "title": "Role does not have enough privileges"
        }, 
        {
            "location": "/trouble-azure/index.html#client-does-not-have-authorization", 
            "text": "Example error message:  Failed to verify credential: Status code 403, {\"error\":{\"code\":\"AuthorizationFailed\", \n\"message\":\"The client 'X' with object id 'z' does not have authorization to perform action \n'Microsoft.Storage/storageAccounts/read' over scope 'subscriptions/...'\"}  Symptom : Your Azure account does not have sufficient permissions to create a Coudbreak credential.   Solution : If you get this error during interactive credential creation, please ensure that your Azure account has  Microsoft.Authorization/*/Write  permission. Otherwise contact your Azure administrator to either give your account that permission or create the necessary resources for the app-based credential creation method.", 
            "title": "Client does not have authorization"
        }, 
        {
            "location": "/trouble-azure/index.html#cloud-not-validate-publickey-certificate", 
            "text": "Example error message:  Could not validate publickey certificate [certificate: 'fdfdsf'], detailed message:  \nCorrupt or unknown public key file format  Symptom : The syntax of your SSH public key is incorrect.  Solution : You must correct the syntax of your SSH key. For information about the correct syntax, refer to  this  page.", 
            "title": "Cloud not validate publickey certificate"
        }, 
        {
            "location": "/trouble-gcp/index.html", 
            "text": "Troubleshooting Cloudbreak on GCP\n\n\nGoogle Cloud create cluster fails with permissions related error\n\n\nIn order to launch clusters on GCP via Cloudbreak, you must have a Service Account that Cloudbreak can use to create resources. In addition, you must also have a P12 key associated with that account.\n\n\nUsually, a user with an \"Owner\" role can assign roles to new and existing service accounts from \nIAM \n Admin \n IAM\n in the Google Cloud console. If you are using your own account, you should be able to perform this step, but if you are using a corporate account, you will likely have to contact your Google Cloud admin.\n\n\nThe roles for the service account are described in \nService account\n.", 
            "title": "Troubleshooting GCP"
        }, 
        {
            "location": "/trouble-gcp/index.html#troubleshooting-cloudbreak-on-gcp", 
            "text": "", 
            "title": "Troubleshooting Cloudbreak on GCP"
        }, 
        {
            "location": "/trouble-gcp/index.html#google-cloud-create-cluster-fails-with-permissions-related-error", 
            "text": "In order to launch clusters on GCP via Cloudbreak, you must have a Service Account that Cloudbreak can use to create resources. In addition, you must also have a P12 key associated with that account.  Usually, a user with an \"Owner\" role can assign roles to new and existing service accounts from  IAM   Admin   IAM  in the Google Cloud console. If you are using your own account, you should be able to perform this step, but if you are using a corporate account, you will likely have to contact your Google Cloud admin.  The roles for the service account are described in  Service account .", 
            "title": "Google Cloud create cluster fails with permissions related error"
        }, 
        {
            "location": "/trouble-cli/index.html", 
            "text": "Troubleshooting Cloudbreak CLI\n\n\nSpecial characters in blueprint name cause an error\n\n\nWhen registering a blueprint via \nblueprint create\n CLI command, if the name of the blueprint includes one or more of the following special characters \n@#$%|:\n*;\n you will get an error similar to:  \n\n\ncb blueprint create from-url --name test@# --url https://myurl.com/myblueprint.bp  \n[1] 7547\n-bash: application.yml: command not found\n-bash: --url: command not found\n ~ \ue0b1 integration-test \ue0b0 1 \ue0b0 time=\"2018-02-01T12:56:44+01:00\" level=\"error\" msg=\"the following parameters are missing: url\\n\"\n\n\n\nSolution:\n\nWhen using special characters in a blueprint name, make sure to use quotes; for example \"test@#\":  \n\n\ncb blueprint create from-url --name \"test@#\" --url https://myurl.com/myblueprint.bp", 
            "title": "Troubleshooting Cloudbreak CLI"
        }, 
        {
            "location": "/trouble-cli/index.html#troubleshooting-cloudbreak-cli", 
            "text": "", 
            "title": "Troubleshooting Cloudbreak CLI"
        }, 
        {
            "location": "/trouble-cli/index.html#special-characters-in-blueprint-name-cause-an-error", 
            "text": "When registering a blueprint via  blueprint create  CLI command, if the name of the blueprint includes one or more of the following special characters  @#$%|: *;  you will get an error similar to:    cb blueprint create from-url --name test@# --url https://myurl.com/myblueprint.bp  \n[1] 7547\n-bash: application.yml: command not found\n-bash: --url: command not found\n ~ \ue0b1 integration-test \ue0b0 1 \ue0b0 time=\"2018-02-01T12:56:44+01:00\" level=\"error\" msg=\"the following parameters are missing: url\\n\"  Solution: \nWhen using special characters in a blueprint name, make sure to use quotes; for example \"test@#\":    cb blueprint create from-url --name \"test@#\" --url https://myurl.com/myblueprint.bp", 
            "title": "Special characters in blueprint name cause an error"
        }, 
        {
            "location": "/cb-upgrade/index.html", 
            "text": "Upgrade Cloudbreak\n\n\nTo upgrade Cloudbreak to the newest version, perform the following steps.\n\n\nWe recommend that you back up Cloudbreak databases before upgrading. Refer to \nBack up Cloudbreak database\n.\n\n\nSteps\n\n\n\n\n\n\nOn the VM where Cloudbreak is running, navigate to the directory where your Profile file is located:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\n\n\n\n\nStop all of the running Cloudbreak components:\n\n\ncbd kill\n\n\n\n\n\n\nUpdate Cloudbreak deployer:\n\n\ncbd update\n\n\n\n\n\n\nUpdate the \ndocker-compose.yml\n file with new Docker containers needed for the cbd:\n\n\ncbd regenerate\n\n\n\n\n\n\nIf there are no other Cloudbreak instances that still use old Cloudbreak versions, remove the obsolete containers:\n\n\ncbd util cleanup\n\n\n\n\n\n\nCheck the health and version of the updated cbd:\n\n\ncbd doctor\n\n\n\n\n\n\nStart the new version of the cbd:\n\n\ncbd start\n\n\nCloudbreak needs to download updated docker images for the new version, so this step may take a while.", 
            "title": "Upgrade Cloudbreak"
        }, 
        {
            "location": "/cb-upgrade/index.html#upgrade-cloudbreak", 
            "text": "To upgrade Cloudbreak to the newest version, perform the following steps.  We recommend that you back up Cloudbreak databases before upgrading. Refer to  Back up Cloudbreak database .  Steps    On the VM where Cloudbreak is running, navigate to the directory where your Profile file is located:  cd /var/lib/cloudbreak-deployment/    Stop all of the running Cloudbreak components:  cbd kill    Update Cloudbreak deployer:  cbd update    Update the  docker-compose.yml  file with new Docker containers needed for the cbd:  cbd regenerate    If there are no other Cloudbreak instances that still use old Cloudbreak versions, remove the obsolete containers:  cbd util cleanup    Check the health and version of the updated cbd:  cbd doctor    Start the new version of the cbd:  cbd start  Cloudbreak needs to download updated docker images for the new version, so this step may take a while.", 
            "title": "Upgrade Cloudbreak"
        }, 
        {
            "location": "/cb-delete/index.html", 
            "text": "Deleting Cloudbreak\n\n\nYou must terminate all clusters associated with a Cloudbreak before you can terminate the Cloudbreak instance. In general, you should delete clusters from the Cloudbreak UI. If needed, you can also delete the cluster resources manually via the cloud provider tools. \n\n\nDeleting clusters\n\n\nThe proper way to delete clusters is to use the the \nTerminate\n option available in the Cloudbreak UI. If the terminate process fails, try the \nTerminate\n \n \nForce terminate\n option.\n\n\nIf the force termination does not delete all cluster resources, delete the resources manually:\n\n\n\n\nTo find the VMs, click on the links available in the cluster details. \n\n\nTo find the network and subnet, see the \nCluster Information\n in the cluster details. \n\n\nOn Azure, you can delete the cluster manually by deleting the whole resource group created when the cluster was deployed. The name of the resource group, under which the cluster-related resources are organized always includes the name of the cluster, so you should be able to find the group by searching for that name in the \nResource groups\n.\n\n\n\n\nUpon cluster termination, Cloudbreak only terminates the resources that it created. It does not terminate any resources (such as networks, subnets, roles, and so on) which existed prior to cluster creation. \n\n\nDelete Cloudbreak on AWS\n\n\nIf you want to delete Cloudbreak deployment, you can do so by deleting the stack in the CloudFormation console.\n\n\nSteps\n\n\n\n\n\n\nLog in to the CloudFormation console.\n\n\n\n\n\n\nSelect the deployment that you want to delete.\n\n\n\n\n\n\nselect \nActions\n \n \nDelete Stack\n.\n\n\n\n\n\n\nClick \nYes, Delete\n to confirm.\n\n\n\n\n\n\nAll resources created as part of this stack (such as the Cloudbreak VM) will be deleted. \n\n\nDelete Cloudbreak on Azure\n\n\nYou can delete Cloudbreak instance from your Azure account by deleting related resources. To delete a Cloudbreak instance:\n\n\n\n\n\n\nIf you deployed Cloudbreak in a new resource group: to delete Cloudbreak, delete the whole related resource group.\n\n\n\n\n\n\nIf you deployed Cloudbreak in an existing resource group: navigate to the group and delete only Cloudbreak related resources such as the VM.\n\n\n\n\n\n\nSteps\n\n\n\n\n\n\nFrom the Microsoft Azure Portal dashboard, select \nResource groups\n.\n\n\n\n\n\n\nFind the resource group that you want to delete.\n\n\n\n\n\n\nIf you deployed Cloudbreak in a new resource group, you can delete the whole resource group. Click on \n...\n and select \nDelete\n:\n\n\n  \n\n\nNext, type the name of the resource group to delete and click \nDelete\n.\n\n\n\n\n\n\nIf you deployed Cloudbreak in an existing resource group, navigate to the details of the resource group and delete only Cloudbreak-related resources such as the VM.    \n\n\n\n\n\n\nDelete Cloudbreak on GCP\n\n\nThere are two ways to delete a previously created Cloudbreak deployment from your Google Cloud account. \n\n\n(Option 1) You can delete the deployment from the Google Cloud console in your browser, from the \nDeployment Manager \n Deployments\n:\n\n\n\n\n(Option 2) You can delete the deployment by using the following gcloud CLI command:\n\n\ngcloud deployment-manager deployments delete deployment-name -q\n\n\n\nFor example: \n\n\ngcloud deployment-manager deployments delete cbd-deployment -q", 
            "title": "Delete Cloudbreak"
        }, 
        {
            "location": "/cb-delete/index.html#deleting-cloudbreak", 
            "text": "You must terminate all clusters associated with a Cloudbreak before you can terminate the Cloudbreak instance. In general, you should delete clusters from the Cloudbreak UI. If needed, you can also delete the cluster resources manually via the cloud provider tools.", 
            "title": "Deleting Cloudbreak"
        }, 
        {
            "location": "/cb-delete/index.html#deleting-clusters", 
            "text": "The proper way to delete clusters is to use the the  Terminate  option available in the Cloudbreak UI. If the terminate process fails, try the  Terminate     Force terminate  option.  If the force termination does not delete all cluster resources, delete the resources manually:   To find the VMs, click on the links available in the cluster details.   To find the network and subnet, see the  Cluster Information  in the cluster details.   On Azure, you can delete the cluster manually by deleting the whole resource group created when the cluster was deployed. The name of the resource group, under which the cluster-related resources are organized always includes the name of the cluster, so you should be able to find the group by searching for that name in the  Resource groups .   Upon cluster termination, Cloudbreak only terminates the resources that it created. It does not terminate any resources (such as networks, subnets, roles, and so on) which existed prior to cluster creation.", 
            "title": "Deleting clusters"
        }, 
        {
            "location": "/cb-delete/index.html#delete-cloudbreak-on-aws", 
            "text": "If you want to delete Cloudbreak deployment, you can do so by deleting the stack in the CloudFormation console.  Steps    Log in to the CloudFormation console.    Select the deployment that you want to delete.    select  Actions     Delete Stack .    Click  Yes, Delete  to confirm.    All resources created as part of this stack (such as the Cloudbreak VM) will be deleted.", 
            "title": "Delete Cloudbreak on AWS"
        }, 
        {
            "location": "/cb-delete/index.html#delete-cloudbreak-on-azure", 
            "text": "You can delete Cloudbreak instance from your Azure account by deleting related resources. To delete a Cloudbreak instance:    If you deployed Cloudbreak in a new resource group: to delete Cloudbreak, delete the whole related resource group.    If you deployed Cloudbreak in an existing resource group: navigate to the group and delete only Cloudbreak related resources such as the VM.    Steps    From the Microsoft Azure Portal dashboard, select  Resource groups .    Find the resource group that you want to delete.    If you deployed Cloudbreak in a new resource group, you can delete the whole resource group. Click on  ...  and select  Delete :      Next, type the name of the resource group to delete and click  Delete .    If you deployed Cloudbreak in an existing resource group, navigate to the details of the resource group and delete only Cloudbreak-related resources such as the VM.", 
            "title": "Delete Cloudbreak on Azure"
        }, 
        {
            "location": "/cb-delete/index.html#delete-cloudbreak-on-gcp", 
            "text": "There are two ways to delete a previously created Cloudbreak deployment from your Google Cloud account.   (Option 1) You can delete the deployment from the Google Cloud console in your browser, from the  Deployment Manager   Deployments :   (Option 2) You can delete the deployment by using the following gcloud CLI command:  gcloud deployment-manager deployments delete deployment-name -q  For example:   gcloud deployment-manager deployments delete cbd-deployment -q", 
            "title": "Delete Cloudbreak on GCP"
        }, 
        {
            "location": "/releasenotes/index.html", 
            "text": "Release notes\n\n\n2.8.0 TP\n\n\nCloudbreak 2.8.0 is a technical preview release, which is not suitable for production deployments.\n\n\n\n\nNew features\n\n\n\n\nTBD \n\n\n\n\nBehavioral changes\n\n\n\n\nTBD\n\n\n\n\nImage catalog updates\n\n\n\n\nDefault versions provided:\n\n\nDefault Ambari version 2.6.2.0\n\nDefault HDP version 2.6.5.0-292\n\nDefault HDF version 3.1.1.0-35    \n\n\n\n\nFixed issues\n\n\n\n\n\n\n\n\n\n\nIssue\n\n\nIssue description\n\n\nCategory\n\n\nFix version\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKnown issues", 
            "title": "Release Notes"
        }, 
        {
            "location": "/releasenotes/index.html#release-notes", 
            "text": "", 
            "title": "Release notes"
        }, 
        {
            "location": "/releasenotes/index.html#280-tp", 
            "text": "Cloudbreak 2.8.0 is a technical preview release, which is not suitable for production deployments.", 
            "title": "2.8.0 TP"
        }, 
        {
            "location": "/releasenotes/index.html#new-features", 
            "text": "TBD", 
            "title": "New features"
        }, 
        {
            "location": "/releasenotes/index.html#behavioral-changes", 
            "text": "TBD", 
            "title": "Behavioral changes"
        }, 
        {
            "location": "/releasenotes/index.html#image-catalog-updates", 
            "text": "Default versions provided:  Default Ambari version 2.6.2.0 \nDefault HDP version 2.6.5.0-292 \nDefault HDF version 3.1.1.0-35", 
            "title": "Image catalog updates"
        }, 
        {
            "location": "/releasenotes/index.html#fixed-issues", 
            "text": "Issue  Issue description  Category  Fix version", 
            "title": "Fixed issues"
        }, 
        {
            "location": "/releasenotes/index.html#known-issues", 
            "text": "", 
            "title": "Known issues"
        }, 
        {
            "location": "/faq/index.html", 
            "text": "FAQs\n\n\nHow to...\n\n\nGenerate SSH key pair\n\n\nAll the instances created by Cloudbreak are configured to allow key-based SSH, so you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak. You can use one of your existing keys or you can generate a new one.\n\n\nTo generate a new SSH key pair, execute:\n\n\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\n\n\n\nYou'll be asked to enter a passphrase, but you can leave it empty:\n\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]\n\n\n\nAfter you enter (or not) a passphrase, the key pair is generated. The output should look similar to:\n\n\n# Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com\n\n\n\nLater you'll need to pass the content of the \n.pub\n file to Cloudbreak and use the private key file to SSH to the instances. \n\n\nRecover public SSH key\n\n\nThe \n-y\n option of \nssh-keygen\n outputs the public key. For example:\n\n\nssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub\n\n\n\nSSH to the hosts\n\n\nTo connect to a running VM through SSH, you need to know its public IP address and have your private key available. \n\n\nThe private key that you must use to access the VM is the counterpart of the public key that you specified when creating a Cloudbreak credential.\n\n\nYou can find the IP addresses of all the running VMs in the Cloudbreak UI, on the cluster details page. Only key-based authentication is supported. \n\n\nCloudbreak creates a cloudbreak user which can be used to ssh into the box. This user has passwordless sudo rights.\n\n\nFor example:\n\n\nssh -i ~/.ssh/your-private-key.pem cloudbreak@\npublic-ip\n\n\n\n\n\nCheck Cloudbreak version\n\n\nTo check Cloudbreak version, navigate to the Cloudbreak home directory and execute the following command:\n\n\ncbd doctor\n\n\n\n\nCheck available environment variables\n\n\nTo see all available environment variables with their default values, use:\n\n\ncbd env show\n\n\n\n\nAccess Cloudbreak logs\n\n\nRefer to \nTroubleshooting\n.", 
            "title": "FAQs"
        }, 
        {
            "location": "/faq/index.html#faqs", 
            "text": "How to...", 
            "title": "FAQs"
        }, 
        {
            "location": "/faq/index.html#generate-ssh-key-pair", 
            "text": "All the instances created by Cloudbreak are configured to allow key-based SSH, so you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak. You can use one of your existing keys or you can generate a new one.  To generate a new SSH key pair, execute:  ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]  You'll be asked to enter a passphrase, but you can leave it empty:  # Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]  After you enter (or not) a passphrase, the key pair is generated. The output should look similar to:  # Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com  Later you'll need to pass the content of the  .pub  file to Cloudbreak and use the private key file to SSH to the instances.", 
            "title": "Generate SSH key pair"
        }, 
        {
            "location": "/faq/index.html#recover-public-ssh-key", 
            "text": "The  -y  option of  ssh-keygen  outputs the public key. For example:  ssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub", 
            "title": "Recover public SSH key"
        }, 
        {
            "location": "/faq/index.html#ssh-to-the-hosts", 
            "text": "To connect to a running VM through SSH, you need to know its public IP address and have your private key available.   The private key that you must use to access the VM is the counterpart of the public key that you specified when creating a Cloudbreak credential.  You can find the IP addresses of all the running VMs in the Cloudbreak UI, on the cluster details page. Only key-based authentication is supported.   Cloudbreak creates a cloudbreak user which can be used to ssh into the box. This user has passwordless sudo rights.  For example:  ssh -i ~/.ssh/your-private-key.pem cloudbreak@ public-ip", 
            "title": "SSH to the hosts"
        }, 
        {
            "location": "/faq/index.html#check-cloudbreak-version", 
            "text": "To check Cloudbreak version, navigate to the Cloudbreak home directory and execute the following command:  cbd doctor", 
            "title": "Check Cloudbreak version"
        }, 
        {
            "location": "/faq/index.html#check-available-environment-variables", 
            "text": "To see all available environment variables with their default values, use:  cbd env show", 
            "title": "Check available environment variables"
        }, 
        {
            "location": "/faq/index.html#access-cloudbreak-logs", 
            "text": "Refer to  Troubleshooting .", 
            "title": "Access Cloudbreak logs"
        }, 
        {
            "location": "/dev/index.html", 
            "text": "Developer documentation links\n\n\nThe following table includes links to Cloudbreak developer documentation: \n\n\n\n\n\n\n\n\nDocumentation link\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSet up local development\n\n\nThis documentation will help you set up your local development environment.\n\n\n\n\n\n\nRetrieve OAuth bearer token via Cloudbreak REST API\n\n\nDescribes how to retrieve OAuth bearer token via Cloudbreak REST API.\n\n\n\n\n\n\nSPI reference\n\n\nThis is Cloudbreak SPI reference documentation.\n\n\n\n\n\n\nAPI reference\n\n\nThis is Cloudbreak API reference documentation. Cloudbreak is a RESTful application development platform whose goal is to help developers deploy HDP clusters in various cloud environments. Once Cloudbreak is deployed in your favorite servlet container, it exposes REST APIs, allowing you to spin up Hadoop clusters of any size with your chosen cloud provider.", 
            "title": "Developer documentation links"
        }, 
        {
            "location": "/dev/index.html#developer-documentation-links", 
            "text": "The following table includes links to Cloudbreak developer documentation:      Documentation link  Description      Set up local development  This documentation will help you set up your local development environment.    Retrieve OAuth bearer token via Cloudbreak REST API  Describes how to retrieve OAuth bearer token via Cloudbreak REST API.    SPI reference  This is Cloudbreak SPI reference documentation.    API reference  This is Cloudbreak API reference documentation. Cloudbreak is a RESTful application development platform whose goal is to help developers deploy HDP clusters in various cloud environments. Once Cloudbreak is deployed in your favorite servlet container, it exposes REST APIs, allowing you to spin up Hadoop clusters of any size with your chosen cloud provider.", 
            "title": "Developer documentation links"
        }, 
        {
            "location": "/get-help/index.html", 
            "text": "Getting help\n\n\nIf you need help with Cloudbreak, you have two options:\n\n\n\n\n\n\n\n\nOption\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nHortonworks Community Connection\n\n\nThis is free optional support via Hortonworks Community Connection (HCC).\n\n\n\n\n\n\nHortonworks Flex Support Subscription\n\n\nThis is paid Hortonworks enterprise support.\n\n\n\n\n\n\n\n\nHCC\n\n\nYou can register for optional free community support at \nHortonworks Community Connection\n where you can browse articles and previously answered questions, and ask questions of your own. When posting questions related to Cloudbreak, make sure to use the \"Cloudbreak\" tag.\n\n\nFlex subscription\n\n\nYou can optionally use your existing Hortonworks \nflex support subscription(s)\n to cover the Cloudbreak node and clusters managed by it. \n\n\n\n\nYou must have an existing SmartSense ID and a Flex subscription. For general information about the Hortonworks Flex Support Subscription, visit the Hortonworks Support page at \nhttps://hortonworks.com/services/support/enterprise/\n.\n\n\n\n\nThe general steps are:\n\n\n\n\nConfigure Smart Sense in your \nProfile\n file.   \n\n\nRegister your Flex subscription in the Cloudbreak web UI. You can register and manage multiple Flex subscriptions. For example, you can choose to use your Flex subscription to cover the Cloudbreak node.   \n\n\nWhen creating a cluster, in the \nGeneral Configuration\n \n \nFlex Subscription\n, you can select the Flex subscription that you want to use for the cluster.  \n\n\n\n\nConfiguring SmartSense\n\n\nTo configure SmartSense in Cloudbreak, enable SmartSense and add your SmartSense ID to the \nProfile\n by adding the following variables:\n\n\nexport CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=YOUR-SMARTSENSE-ID\n\n\n\nFor example:\n\n\nexport CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=A-00000000-C-00000000\n\n\n\nYou can do this in one of the two ways:\n\n\n\n\nWhen initiating Cloudbreak deployer  \n\n\nAfter you've already initiated Cloudbreak deployer. If you choose this option, you must restart Cloudbreak using \ncbd restart\n.\n\n\n\n\nRegister and manage flex subscriptions\n\n\nOnce you log in to the Cloudbreak web UI, you can manage your Flex subscriptions from the \nSettings\n page \n \nFlex Subscriptions\n:\n\n\n  \n\n\nYou can:\n\n\n\n\nRegister a new Flex subscription    \n\n\nSet a default Flex subscription (\"Default\")  \n\n\nSelect a Flex subscription to be used for the Cloudbreak node (\"Use for controller\")  \n\n\nDelete a Flex subscription    \n\n\n\n\nUse flex subscription for a cluster\n\n\nWhen creating a cluster, on the \nGeneral Configuration\n page you can select the Flex subscription that you want to use for the cluster:\n\n\n  \n\n\nUse flex subscription for Cloudbreak node\n\n\nTo use a Flex subscription for Cloudbreak node, on the \nSettings\n page, in the \nFlex Subscriptions\n section, check the \"Use for controller option\" for the selected Flex ID.  \n\n\nMore Cloudbreak resources\n\n\nCheck out the following documentation to learn more:\n\n\n\n\n Resource \nDescription\n\n\nHortonworks documentation \n\n\nDuring cluster create process, Cloudbreak automatically installs Ambari and sets up a cluster for you. After this deployment is complete, refer to the \nAmbari documentation\n and \nHDP documentation\n for help.\n\n\n\n\n\n\nHortonworks tutorials\n\n\n\n\nUse Hortonworks tutorials to get started with Apache Spark, Apache Hive, Apache Zeppelin, and more.\n\n\nApache documentation\n\n\n\n\n In addition to Hortonworks documentation, refer to the Apache Software Foundation documentation to get information on specific Hadoop services. \n\n\n\n\n\nAmbari blueprints\nLearn about Ambari blueprints. Ambari blueprints are a declarative definition of a Hadoop cluster that Ambari can use to create Hadoop clusters.\n\n\nCloudbreak project\nVisit the Hortonworks website to see Cloudbreak-related news and updates.\n\n\nApache Ambari project\nLearn about the Apache Ambari project. Apache Ambari is an operational platform for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari exposes a robust set of REST APIs and a rich web interface for cluster management.", 
            "title": "Getting help"
        }, 
        {
            "location": "/get-help/index.html#getting-help", 
            "text": "If you need help with Cloudbreak, you have two options:     Option  Description      Hortonworks Community Connection  This is free optional support via Hortonworks Community Connection (HCC).    Hortonworks Flex Support Subscription  This is paid Hortonworks enterprise support.", 
            "title": "Getting help"
        }, 
        {
            "location": "/get-help/index.html#hcc", 
            "text": "You can register for optional free community support at  Hortonworks Community Connection  where you can browse articles and previously answered questions, and ask questions of your own. When posting questions related to Cloudbreak, make sure to use the \"Cloudbreak\" tag.", 
            "title": "HCC"
        }, 
        {
            "location": "/get-help/index.html#flex-subscription", 
            "text": "You can optionally use your existing Hortonworks  flex support subscription(s)  to cover the Cloudbreak node and clusters managed by it.    You must have an existing SmartSense ID and a Flex subscription. For general information about the Hortonworks Flex Support Subscription, visit the Hortonworks Support page at  https://hortonworks.com/services/support/enterprise/ .   The general steps are:   Configure Smart Sense in your  Profile  file.     Register your Flex subscription in the Cloudbreak web UI. You can register and manage multiple Flex subscriptions. For example, you can choose to use your Flex subscription to cover the Cloudbreak node.     When creating a cluster, in the  General Configuration     Flex Subscription , you can select the Flex subscription that you want to use for the cluster.", 
            "title": "Flex subscription"
        }, 
        {
            "location": "/get-help/index.html#configuring-smartsense", 
            "text": "To configure SmartSense in Cloudbreak, enable SmartSense and add your SmartSense ID to the  Profile  by adding the following variables:  export CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=YOUR-SMARTSENSE-ID  For example:  export CB_SMARTSENSE_CONFIGURE=true\nexport CB_SMARTSENSE_ID=A-00000000-C-00000000  You can do this in one of the two ways:   When initiating Cloudbreak deployer    After you've already initiated Cloudbreak deployer. If you choose this option, you must restart Cloudbreak using  cbd restart .", 
            "title": "Configuring SmartSense"
        }, 
        {
            "location": "/get-help/index.html#register-and-manage-flex-subscriptions", 
            "text": "Once you log in to the Cloudbreak web UI, you can manage your Flex subscriptions from the  Settings  page    Flex Subscriptions :      You can:   Register a new Flex subscription      Set a default Flex subscription (\"Default\")    Select a Flex subscription to be used for the Cloudbreak node (\"Use for controller\")    Delete a Flex subscription", 
            "title": "Register and manage flex subscriptions"
        }, 
        {
            "location": "/get-help/index.html#use-flex-subscription-for-a-cluster", 
            "text": "When creating a cluster, on the  General Configuration  page you can select the Flex subscription that you want to use for the cluster:", 
            "title": "Use flex subscription for a cluster"
        }, 
        {
            "location": "/get-help/index.html#use-flex-subscription-for-cloudbreak-node", 
            "text": "To use a Flex subscription for Cloudbreak node, on the  Settings  page, in the  Flex Subscriptions  section, check the \"Use for controller option\" for the selected Flex ID.", 
            "title": "Use flex subscription for Cloudbreak node"
        }, 
        {
            "location": "/get-help/index.html#more-cloudbreak-resources", 
            "text": "Check out the following documentation to learn more:    Resource  Description  Hortonworks documentation   During cluster create process, Cloudbreak automatically installs Ambari and sets up a cluster for you. After this deployment is complete, refer to the  Ambari documentation  and  HDP documentation  for help.    Hortonworks tutorials   Use Hortonworks tutorials to get started with Apache Spark, Apache Hive, Apache Zeppelin, and more.  Apache documentation    In addition to Hortonworks documentation, refer to the Apache Software Foundation documentation to get information on specific Hadoop services.    Ambari blueprints Learn about Ambari blueprints. Ambari blueprints are a declarative definition of a Hadoop cluster that Ambari can use to create Hadoop clusters.  Cloudbreak project Visit the Hortonworks website to see Cloudbreak-related news and updates.  Apache Ambari project Learn about the Apache Ambari project. Apache Ambari is an operational platform for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari exposes a robust set of REST APIs and a rich web interface for cluster management.", 
            "title": "More Cloudbreak resources"
        }, 
        {
            "location": "/smartsense/index.html", 
            "text": "SmartSense telemetry\n\n\nHelp us make a better product by opting in to automatically send information to Hortonworks. This includes enabling\nHortonworks SmartSense and sending performance and usage info. As you use the product,\nSmartSense measures and collects information and then sends these information bundles to Hortonworks.\n\n\nDisabling SmartSense telemetry\n\n\nDisable bundle upload for Cloudbreak and new clusters\n\n\n\n    \nImportant\n\n    \n\n    Do not perform these steps when you have clusters currently in the process of being deployed.\n    Wait for all clusters to be deployed.\n\n\n\n\n\n\n\n\n\nSSH into the Cloudbreak host.\n\n\n\n\n\n\nEdit \n/var/lib/cloudbreak-deployment/Profile\n.\n\n\n\n\n\n\nChange \nCB_SMARTSENSE_CONFIGURE\n to \nfalse\n:\n\n    \nexport CB_SMARTSENSE_CONFIGURE=false\n\n\n\n\n\n\nRestart the cloud controller:\n\n    \ncd /var/lib/cloudbreak-deployment\ncbd restart\n\n\n\n\n\n\nDisable bundle upload for an existing cluster\n\n\n\n\n\n\nSSH into the master node for the cluster.\n\n\n\n\n\n\nEdit \n/etc/hst/conf/hst-server.ini\n.\n\n\n\n\n\n\nChange \n[gateway]\n configuration to \nfalse\n:\n\n    \n[gateway]\nenabled=false\n\n\n\n\n\n\nRestart the SmartSense Server:\n    \nhst restart\n\n\n\n\n\n\n(Optional) Disable SmartSense daily bundle capture:\n\n\n\n\nSmartSense is scheduled to capture a telemetry bundle daily. With the bundle upload disabled, the bundle will still\nbe captured but just saved locally (i.e. not uploaded).\n\n\nTo disable the bundle capture, execute the following:\n\nhst capture-schedule -a pause\n\n\n\n\n\n\n\n\nRepeat on all existing clusters.", 
            "title": "SmartSense telemetry"
        }, 
        {
            "location": "/smartsense/index.html#smartsense-telemetry", 
            "text": "Help us make a better product by opting in to automatically send information to Hortonworks. This includes enabling\nHortonworks SmartSense and sending performance and usage info. As you use the product,\nSmartSense measures and collects information and then sends these information bundles to Hortonworks.", 
            "title": "SmartSense telemetry"
        }, 
        {
            "location": "/smartsense/index.html#disabling-smartsense-telemetry", 
            "text": "", 
            "title": "Disabling SmartSense telemetry"
        }, 
        {
            "location": "/smartsense/index.html#disable-bundle-upload-for-cloudbreak-and-new-clusters", 
            "text": "Important \n     \n    Do not perform these steps when you have clusters currently in the process of being deployed.\n    Wait for all clusters to be deployed.     SSH into the Cloudbreak host.    Edit  /var/lib/cloudbreak-deployment/Profile .    Change  CB_SMARTSENSE_CONFIGURE  to  false : \n     export CB_SMARTSENSE_CONFIGURE=false    Restart the cloud controller: \n     cd /var/lib/cloudbreak-deployment\ncbd restart", 
            "title": "Disable bundle upload for Cloudbreak and new clusters"
        }, 
        {
            "location": "/smartsense/index.html#disable-bundle-upload-for-an-existing-cluster", 
            "text": "SSH into the master node for the cluster.    Edit  /etc/hst/conf/hst-server.ini .    Change  [gateway]  configuration to  false : \n     [gateway]\nenabled=false    Restart the SmartSense Server:\n     hst restart    (Optional) Disable SmartSense daily bundle capture:   SmartSense is scheduled to capture a telemetry bundle daily. With the bundle upload disabled, the bundle will still\nbe captured but just saved locally (i.e. not uploaded).  To disable the bundle capture, execute the following: hst capture-schedule -a pause     Repeat on all existing clusters.", 
            "title": "Disable bundle upload for an existing cluster"
        }, 
        {
            "location": "/acknowledge/index.html", 
            "text": "Acknowledgements\n\n\nCopyrights and trademarks\n\n\n\u00a9 2011-2018 Hortonworks Inc. All Rights Reserved. Hortonworks and HDP are registered trademarks\nor trademarks of Hortonworks, Inc. in the United States and other jurisdictions.  All other\ntrademarks and trade names are the property of their respective owners.\n\n\nApache, Hadoop, Falcon, Atlas, Tez, Sqoop, Flume, Kafka, Pig, Hive,\nHBase, Accumulo, Storm, Solr, Spark, Ranger, Knox, Ambari, ZooKeeper,\nOozie, Metron, Druid, and the Hadoop elephant logo are either registered trademarks or\ntrademarks of the \nApache Software Foundation\n in\nthe United States or other countries.\n\n\n\"Amazon Web Services\", \"AWS\", \"Amazon EC2\", \"Amazon S3\", \n\"Amazon VPC\", \"Amazon RDS\", the \"Amazon Web Services\" logo, and other\nAWS service names, graphics, and logos are trademarks of Amazon Technologies, Inc. or its affiliates in the United States and/or other countries.\n\n\n\"Microsoft Azure\", \"Windows Azure\", the \"Azure\" logo, and other\nAzure service names, graphics, and logos are trademarks of Microsoft Corporation or its affiliates in the United States and/or other countries.\n\n\n\"Google Cloud\", the \"Google Cloud\" logo, and other\nGoogle Cloud service names, graphics, and logos are trademarks of Google LLC or its affiliates in the United States and/or other countries.\n\n\n\"OpenStack\", the \"OpenStack\" logo, and other\nOpenStack graphics and logos are trademarks of OpenStack LLC or its affiliates in the United States and/or other countries.\n\n\nDocumentation was built with \nMkDocs\n and the\n\nCinder Theme\n, licensed under\nthe \nMIT license\n.\n\n\nContact information\n\n\nHortonworks, Inc.\n\n\n5470 Great America Parkway\n\n\nSanta Clara, CA 95054\n\n\n\nWebsite:\n \nwww.hortonworks.com\n\n\n\n\nCommunity:\n \ncommunity.hortonworks.com", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/acknowledge/index.html#acknowledgements", 
            "text": "", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/acknowledge/index.html#copyrights-and-trademarks", 
            "text": "\u00a9 2011-2018 Hortonworks Inc. All Rights Reserved. Hortonworks and HDP are registered trademarks\nor trademarks of Hortonworks, Inc. in the United States and other jurisdictions.  All other\ntrademarks and trade names are the property of their respective owners.  Apache, Hadoop, Falcon, Atlas, Tez, Sqoop, Flume, Kafka, Pig, Hive,\nHBase, Accumulo, Storm, Solr, Spark, Ranger, Knox, Ambari, ZooKeeper,\nOozie, Metron, Druid, and the Hadoop elephant logo are either registered trademarks or\ntrademarks of the  Apache Software Foundation  in\nthe United States or other countries.  \"Amazon Web Services\", \"AWS\", \"Amazon EC2\", \"Amazon S3\", \n\"Amazon VPC\", \"Amazon RDS\", the \"Amazon Web Services\" logo, and other\nAWS service names, graphics, and logos are trademarks of Amazon Technologies, Inc. or its affiliates in the United States and/or other countries.  \"Microsoft Azure\", \"Windows Azure\", the \"Azure\" logo, and other\nAzure service names, graphics, and logos are trademarks of Microsoft Corporation or its affiliates in the United States and/or other countries.  \"Google Cloud\", the \"Google Cloud\" logo, and other\nGoogle Cloud service names, graphics, and logos are trademarks of Google LLC or its affiliates in the United States and/or other countries.  \"OpenStack\", the \"OpenStack\" logo, and other\nOpenStack graphics and logos are trademarks of OpenStack LLC or its affiliates in the United States and/or other countries.  Documentation was built with  MkDocs  and the Cinder Theme , licensed under\nthe  MIT license .", 
            "title": "Copyrights and trademarks"
        }, 
        {
            "location": "/acknowledge/index.html#contact-information", 
            "text": "Hortonworks, Inc. \n5470 Great America Parkway \nSanta Clara, CA 95054  Website:   www.hortonworks.com   Community:   community.hortonworks.com", 
            "title": "Contact information"
        }
    ]
}